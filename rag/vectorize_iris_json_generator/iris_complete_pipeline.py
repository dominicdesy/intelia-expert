#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pipeline complet Vectorize Iris - Version tout-en-un
1. DÃ©bloque les PDFs protÃ©gÃ©s
2. Upload vers Vectorize Iris
3. TÃ©lÃ©charge les JSON/TXT rÃ©sultats

Usage:
    python iris_complete_pipeline.py
"""

import os
import sys
import time
import json
import urllib3
import vectorize_client as v
from pathlib import Path
from dotenv import load_dotenv
from datetime import datetime
import tempfile

# Force UTF-8 encoding for Windows console
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Charger .env
load_dotenv()

# Configuration par dÃ©faut
DEFAULT_INPUT_DIR = r"C:\intelia_gpt\intelia-expert\rag\documents\Sources"
DEFAULT_OUTPUT_DIR = r"C:\intelia_gpt\intelia-expert\rag\documents\Knowledge"

# Limites et exclusions
MAX_FILE_SIZE_MB = 50  # Limite Vectorize Iris
EXCLUDED_FILES = [
    "Manual_of_poultry_diseases_en.pdf"  # Trop gros pour Iris
]


class PDFUnlocker:
    """DÃ©bloqueur de PDFs protÃ©gÃ©s"""

    @staticmethod
    def is_protected(pdf_path: Path) -> bool:
        """VÃ©rifie si le PDF est protÃ©gÃ©"""
        try:
            import fitz
            doc = fitz.open(str(pdf_path))
            is_encrypted = doc.is_encrypted
            needs_pass = doc.needs_pass
            doc.close()
            return is_encrypted or needs_pass
        except:
            return False

    @staticmethod
    def unlock_with_pikepdf(input_path: Path, output_path: Path) -> bool:
        """DÃ©bloquer avec pikepdf"""
        try:
            import pikepdf
            pdf = pikepdf.open(input_path)
            pdf.save(output_path)
            pdf.close()
            return True
        except ImportError:
            return False
        except Exception as e:
            print(f"      pikepdf error: {e}")
            return False

    @staticmethod
    def unlock_with_pymupdf(input_path: Path, output_path: Path) -> bool:
        """DÃ©bloquer avec PyMuPDF"""
        try:
            import fitz
            doc = fitz.open(str(input_path))

            # Essayer mots de passe communs si nÃ©cessaire
            if doc.needs_pass:
                common_passwords = ["", "password", "123456"]
                for pwd in common_passwords:
                    if doc.authenticate(pwd):
                        break
                else:
                    doc.close()
                    return False

            doc.save(str(output_path), encryption=fitz.PDF_ENCRYPT_NONE)
            doc.close()
            return True
        except ImportError:
            return False
        except Exception as e:
            print(f"      pymupdf error: {e}")
            return False

    @classmethod
    def unlock(cls, pdf_path: Path) -> Path:
        """
        DÃ©bloque un PDF protÃ©gÃ© et retourne le chemin du fichier dÃ©bloquÃ©
        Retourne le fichier original si pas de protection
        """
        if not cls.is_protected(pdf_path):
            print(f"   ğŸ”“ PDF dÃ©jÃ  dÃ©bloquÃ©")
            return pdf_path

        print(f"   ğŸ”’ PDF protÃ©gÃ© - dÃ©blocage en cours...")

        # CrÃ©er fichier temporaire pour la version dÃ©bloquÃ©e
        temp_dir = Path(tempfile.gettempdir())
        unlocked_path = temp_dir / f"unlocked_{pdf_path.name}"

        # Essayer pikepdf d'abord (plus puissant)
        if cls.unlock_with_pikepdf(pdf_path, unlocked_path):
            print(f"   âœ… DÃ©bloquÃ© avec pikepdf")
            return unlocked_path

        # Sinon PyMuPDF
        if cls.unlock_with_pymupdf(pdf_path, unlocked_path):
            print(f"   âœ… DÃ©bloquÃ© avec pymupdf")
            return unlocked_path

        # Ã‰chec
        print(f"   âš ï¸  Impossible de dÃ©bloquer - tentative avec fichier original")
        return pdf_path


class IrisExtractor:
    """Extracteur Vectorize Iris"""

    def __init__(self, api_key: str, organization_id: str, output_dir: Path):
        self.organization_id = organization_id
        self.output_dir = output_dir
        self.output_dir.mkdir(exist_ok=True, parents=True)

        # CrÃ©er client API
        cfg = v.Configuration(host="https://api.vectorize.io")
        if hasattr(cfg, "access_token"):
            cfg.access_token = api_key
        else:
            cfg.api_key = {"ApiKeyAuth": api_key}

        self.api_client = v.ApiClient(cfg)
        self.files_api = v.FilesApi(self.api_client)
        self.extraction_api = v.ExtractionApi(self.api_client)

    def upload_pdf(self, pdf_path: Path) -> str:
        """Upload PDF vers Iris"""
        print(f"   ğŸ“¤ Upload vers Iris...")

        upload_request = v.StartFileUploadRequest(
            content_type="application/pdf",
            name=pdf_path.name
        )

        upload_response = self.files_api.start_file_upload(
            self.organization_id,
            start_file_upload_request=upload_request
        )

        # Upload avec retries
        http = urllib3.PoolManager(
            retries=urllib3.util.retry.Retry(
                total=5, backoff_factor=0.5,
                status_forcelist=[429, 500, 502, 503, 504],
                allowed_methods=frozenset({"PUT"}),
                raise_on_status=False,
            ),
            timeout=60.0,
        )

        file_size = pdf_path.stat().st_size

        with open(pdf_path, "rb") as file:
            response = http.request(
                "PUT",
                upload_response.upload_url,
                body=file,
                headers={
                    "Content-Type": "application/pdf",
                    "Content-Length": str(file_size),
                },
            )

        if response.status not in (200, 201, 204):
            raise Exception(f"Upload Ã©chouÃ©: {response.status}")

        print(f"      âœ… Upload rÃ©ussi ({file_size / 1024:.1f} KB)")
        return upload_response.file_id

    def extract(self, file_id: str) -> str:
        """DÃ©marre extraction"""
        print(f"   ğŸ”„ Extraction dÃ©marrÃ©e...")

        extraction_request = v.StartExtractionRequest(file_id=file_id)

        response = self.extraction_api.start_extraction(
            self.organization_id,
            start_extraction_request=extraction_request
        )

        return response.extraction_id

    def wait_for_result(self, extraction_id: str, timeout=600) -> dict:
        """Attend le rÃ©sultat (max 10 min)"""
        print(f"   â³ Attente du rÃ©sultat...", end="", flush=True)
        start_time = time.time()

        dots = 0
        while time.time() - start_time < timeout:
            response = self.extraction_api.get_extraction_result(
                self.organization_id,
                extraction_id
            )

            if response.ready:
                elapsed = time.time() - start_time
                if response.data.success:
                    print(f"\r   âœ… TerminÃ© ({elapsed:.1f}s)      ")
                    return response.data
                else:
                    error = getattr(response.data, 'error', 'Erreur inconnue')
                    raise Exception(f"Extraction Ã©chouÃ©e: {error}")

            # Animation points
            dots = (dots + 1) % 4
            print(f"\r   â³ Attente du rÃ©sultat{'.' * dots}   ", end="", flush=True)
            time.sleep(2)

        raise Exception(f"Timeout aprÃ¨s {timeout}s")

    def save_results(self, extraction_data, original_pdf_name: str) -> tuple:
        """Sauvegarde JSON et TXT"""
        base_name = Path(original_pdf_name).stem

        # JSON
        json_path = self.output_dir / f"{base_name}_extracted.json"
        json_data = {
            "source_file": original_pdf_name,
            "extraction_date": datetime.now().isoformat(),
            "text": extraction_data.text,
            "metadata": getattr(extraction_data, "metadata", {}),
            "chunks": getattr(extraction_data, "chunks", [])
        }

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False, default=str)

        # TXT
        txt_path = self.output_dir / f"{base_name}_extracted.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(extraction_data.text)

        print(f"   ğŸ’¾ RÃ©sultats sauvegardÃ©s:")
        print(f"      ğŸ“„ JSON: {json_path.name}")
        print(f"      ğŸ“„ TXT: {txt_path.name}")

        return json_path, txt_path


class CompletePipeline:
    """Pipeline complet: dÃ©blocage + extraction + sauvegarde"""

    def __init__(self, api_key: str, organization_id: str, output_dir: Path):
        self.unlocker = PDFUnlocker()
        self.extractor = IrisExtractor(api_key, organization_id, output_dir)

    def process_pdf(self, pdf_path: Path) -> dict:
        """Traite un PDF complet"""
        result = {
            "success": False,
            "pdf": pdf_path.name,
            "error": None,
            "json_path": None,
            "txt_path": None
        }

        try:
            print(f"\n{'='*70}")
            print(f"ğŸ“„ {pdf_path.name}")
            print(f"{'='*70}")

            # Ã‰tape 1: DÃ©blocage
            unlocked_pdf = self.unlocker.unlock(pdf_path)

            # Ã‰tape 2: Upload
            file_id = self.extractor.upload_pdf(unlocked_pdf)

            # Ã‰tape 3: Extraction
            extraction_id = self.extractor.extract(file_id)

            # Ã‰tape 4: Attente rÃ©sultat
            extraction_data = self.extractor.wait_for_result(extraction_id)

            # Ã‰tape 5: Sauvegarde
            json_path, txt_path = self.extractor.save_results(
                extraction_data,
                pdf_path.name
            )

            # Statistiques
            text_length = len(extraction_data.text)
            chunks_count = len(getattr(extraction_data, 'chunks', []))

            print(f"\n   ğŸ“Š Statistiques:")
            print(f"      Texte: {text_length:,} caractÃ¨res")
            print(f"      Chunks: {chunks_count}")
            print(f"\n   âœ… SUCCÃˆS")

            result["success"] = True
            result["json_path"] = str(json_path)
            result["txt_path"] = str(txt_path)
            result["text_length"] = text_length
            result["chunks_count"] = chunks_count

            # Nettoyer fichier temporaire dÃ©bloquÃ© si diffÃ©rent de l'original
            if unlocked_pdf != pdf_path and unlocked_pdf.exists():
                try:
                    unlocked_pdf.unlink()
                except:
                    pass

        except Exception as e:
            print(f"\n   âŒ Ã‰CHEC: {e}")
            result["error"] = str(e)

        return result

    def is_already_extracted(self, pdf_path: Path) -> bool:
        """VÃ©rifie si le PDF a dÃ©jÃ  Ã©tÃ© extrait"""
        base_name = pdf_path.stem
        json_output = self.extractor.output_dir / f"{base_name}_extracted.json"
        return json_output.exists()

    def should_exclude_file(self, pdf_path: Path) -> tuple[bool, str]:
        """VÃ©rifie si le fichier doit Ãªtre exclu (nom ou taille)"""
        # VÃ©rifier exclusions par nom
        if pdf_path.name in EXCLUDED_FILES:
            return True, "Fichier exclu (trop gros pour Iris)"

        # VÃ©rifier taille
        file_size_mb = pdf_path.stat().st_size / (1024 * 1024)
        if file_size_mb > MAX_FILE_SIZE_MB:
            return True, f"Fichier trop gros ({file_size_mb:.1f} MB > {MAX_FILE_SIZE_MB} MB)"

        return False, ""

    def process_directory(self, input_dir: Path, skip_existing: bool = True) -> dict:
        """Traite tous les PDFs d'un rÃ©pertoire"""
        print(f"\nğŸ” Recherche de PDFs dans: {input_dir}")

        # Trouver PDFs rÃ©cursivement
        all_pdf_files = list(input_dir.rglob("*.pdf"))

        if not all_pdf_files:
            print(f"âŒ Aucun PDF trouvÃ©")
            return {"total": 0, "success": 0, "failed": 0, "skipped": 0, "excluded": 0, "results": []}

        print(f"ğŸ“š {len(all_pdf_files)} PDF(s) trouvÃ©(s)")

        # Filtrer les fichiers
        new_files = []
        already_extracted = []
        excluded_files = []

        for pdf_file in all_pdf_files:
            # VÃ©rifier exclusions
            should_exclude, exclude_reason = self.should_exclude_file(pdf_file)
            if should_exclude:
                excluded_files.append((pdf_file, exclude_reason))
                continue

            # VÃ©rifier si dÃ©jÃ  extrait
            if skip_existing and self.is_already_extracted(pdf_file):
                already_extracted.append(pdf_file)
            else:
                new_files.append(pdf_file)

        # Afficher le statut
        print(f"\nğŸ“Š Statut:")
        print(f"   âœ… DÃ©jÃ  extraits: {len(already_extracted)}")
        print(f"   ğŸš« Exclus: {len(excluded_files)}")
        print(f"   ğŸ†• Nouveaux: {len(new_files)}")

        if excluded_files:
            print(f"\nğŸš« Fichiers exclus:")
            for pdf, reason in excluded_files:
                print(f"   â€¢ {pdf.name} - {reason}")

        if already_extracted:
            print(f"\nâ­ï¸  Fichiers dÃ©jÃ  extraits (ignorÃ©s):")
            for pdf in already_extracted[:5]:
                print(f"   â€¢ {pdf.name}")
            if len(already_extracted) > 5:
                print(f"   ... et {len(already_extracted) - 5} autres")

        if not new_files:
            print(f"\nâœ… Aucun nouveau fichier Ã  traiter!")
            return {
                "total": len(all_pdf_files),
                "success": 0,
                "failed": 0,
                "skipped": len(already_extracted),
                "excluded": len(excluded_files),
                "results": []
            }

        # Afficher les fichiers Ã  traiter
        print(f"\nğŸ“‹ Nouveaux fichiers Ã  traiter:")
        for i, pdf in enumerate(new_files, 1):
            print(f"   {i}. {pdf.name}")

        print(f"\nğŸš€ DÃ©marrage de l'extraction de {len(new_files)} fichier(s)...")

        # Traitement
        stats = {
            "total": len(all_pdf_files),
            "success": 0,
            "failed": 0,
            "skipped": len(already_extracted),
            "excluded": len(excluded_files),
            "results": []
        }
        start_time = time.time()

        for i, pdf_file in enumerate(new_files, 1):
            print(f"\n[{i}/{len(new_files)}]", end=" ")

            result = self.process_pdf(pdf_file)
            stats["results"].append(result)

            if result["success"]:
                stats["success"] += 1
            else:
                stats["failed"] += 1

            # Pause entre fichiers
            if i < len(new_files):
                time.sleep(1)

        # Rapport final
        elapsed = time.time() - start_time
        stats["elapsed_minutes"] = elapsed / 60

        print(f"\n{'='*70}")
        print(f"ğŸ“Š RAPPORT FINAL")
        print(f"{'='*70}")
        print(f"Total PDFs trouvÃ©s: {stats['total']}")
        print(f"ğŸš« Exclus (trop gros): {stats['excluded']}")
        print(f"â­ï¸  DÃ©jÃ  extraits: {stats['skipped']}")
        print(f"ğŸ†• Nouveaux traitÃ©s: {stats['success'] + stats['failed']}")
        print(f"   âœ… SuccÃ¨s: {stats['success']}")
        print(f"   âŒ Ã‰checs: {stats['failed']}")
        print(f"â±ï¸  Temps: {elapsed / 60:.1f} minutes")

        if stats['failed'] > 0:
            print(f"\nâŒ Fichiers en Ã©chec:")
            for r in stats['results']:
                if not r['success']:
                    print(f"   â€¢ {r['pdf']}: {r['error']}")

        print(f"{'='*70}")

        return stats


def main():
    """Point d'entrÃ©e principal"""
    print("=" * 70)
    print("ğŸš€ PIPELINE COMPLET VECTORIZE IRIS")
    print("=" * 70)
    print("1. DÃ©bloque les PDFs protÃ©gÃ©s")
    print("2. Upload vers Vectorize Iris")
    print("3. TÃ©lÃ©charge JSON + TXT")
    print("4. Skip les fichiers dÃ©jÃ  extraits")
    print("=" * 70)

    # VÃ©rifier .env
    api_key = os.getenv("VECTORIZE_API_KEY", "").strip()
    organization_id = os.getenv("VECTORIZE_ORGANIZATION_ID", "").strip()

    if not api_key or not organization_id:
        print("\nâŒ Variables d'environnement manquantes")
        print("\nCrÃ©ez un fichier .env avec:")
        print("VECTORIZE_API_KEY=votre_clÃ©")
        print("VECTORIZE_ORGANIZATION_ID=votre_org_id")
        return

    # Configuration avec chemins par dÃ©faut
    input_path = Path(DEFAULT_INPUT_DIR)
    output_path = Path(DEFAULT_OUTPUT_DIR)

    print(f"\nğŸ“‹ Configuration:")
    print(f"   Input: {input_path}")
    print(f"   Output: {output_path}")
    print(f"   Organisation: {organization_id}")

    # VÃ©rifier que les rÃ©pertoires existent
    if not input_path.exists():
        print(f"\nâŒ RÃ©pertoire source non trouvÃ©: {input_path}")
        return

    # CrÃ©er le rÃ©pertoire de sortie s'il n'existe pas
    output_path.mkdir(parents=True, exist_ok=True)

    # Lancer le pipeline
    pipeline = CompletePipeline(api_key, organization_id, output_path)
    stats = pipeline.process_directory(input_path, skip_existing=True)

    # Message final
    if stats["success"] > 0:
        print(f"\nğŸ‰ Extraction terminÃ©e!")
        print(f"ğŸ“ RÃ©sultats dans: {output_path}")
        print(f"\nğŸ’¡ Prochaine Ã©tape:")
        print(f"   cd ../knowledge_extractor")
        print(f"   python knowledge_extractor.py --force")
    elif stats["skipped"] > 0 and stats["success"] == 0 and stats["failed"] == 0:
        print(f"\nâœ… Tous les fichiers sont dÃ©jÃ  extraits!")
        print(f"ğŸ“ RÃ©sultats existants dans: {output_path}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸  Interrompu par l'utilisateur")
    except Exception as e:
        print(f"\nâŒ Erreur fatale: {e}")
        import traceback
        traceback.print_exc()
