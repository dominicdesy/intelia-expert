# -*- coding: utf-8 -*-
"""
api/chat_handlers.py - Logique de traitement des requ√™tes de chat
Version: 1.4.1
Last modified: 2025-10-26
"""
"""
api/chat_handlers.py - Logique de traitement des requ√™tes de chat
Version 5.1.0 - INT√âGRATION ConversationMemory
Le contexte conversationnel est maintenant g√©r√© par QueryRouter + ConversationMemory
"""

import time
import asyncio
import logging
from utils.types import Dict, Any, Optional

from config.config import STREAM_CHUNK_LEN
from utils.utilities import (
    safe_get_attribute,
    safe_dict_get,
    sse_event,
    smart_chunk_text,
    get_aviculture_response,
)

# CORRECTION: Import depuis utils au lieu de endpoints_utils
from .utils import (
    safe_serialize_for_json,
)

logger = logging.getLogger(__name__)


class ChatHandlers:
    """
    Gestionnaires de logique m√©tier pour les endpoints de chat

    VERSION 5.1.0 - INT√âGRATION CONVERSATION MEMORY:
    - Double sauvegarde: ConversationMemory + ancien syst√®me
    - Initialisation lazy de ConversationMemory
    - QueryRouter dans RAGEngine g√®re TOUT le routing
    - Cette classe fait l'interface entre endpoints et RAG
    """

    def __init__(self, services: Dict[str, Any]):
        """
        Initialisation simplifi√©e

        Args:
            services: Services disponibles (health_monitor, etc.)

        Note: context_manager supprim√© - le router g√®re le contexte
        """
        self.services = services
        self._conversation_memory = None  # Lazy initialization

    @property
    def conversation_memory(self):
        """
        Lazy initialization de ConversationMemory

        Returns:
            Instance de ConversationMemory
        """
        if self._conversation_memory is None:
            try:
                from core.memory import ConversationMemory

                # Client Weaviate peut √™tre None pour l'instant
                # ConversationMemory utilisera Redis si disponible
                self._conversation_memory = ConversationMemory(client=None)
                logger.info("‚úÖ ConversationMemory initialis√©")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible d'initialiser ConversationMemory: {e}")
                self._conversation_memory = None
        return self._conversation_memory

    def get_rag_engine(self):
        """Helper pour r√©cup√©rer le RAG Engine"""
        health_monitor = self.services.get("health_monitor")
        if health_monitor:
            return health_monitor.get_service("rag_engine_enhanced")
        return None

    async def generate_rag_response(
        self,
        query: str,
        tenant_id: str,
        conversation_id: Optional[str] = None,  # üÜï ID de session/conversation
        language: str = "fr",
        use_json_search: bool = True,
        genetic_line_filter: Optional[str] = None,
        performance_context: Optional[Dict[str, Any]] = None,
    ):
        """
        G√©n√®re une r√©ponse via le RAG Engine

        VERSION 5.1.0:
        - Appel direct au RAG sans pr√©-traitement
        - Le QueryRouter dans RAGEngine g√®re:
          * Extraction d'entit√©s
          * Contexte conversationnel (via ConversationMemory)
          * Validation
          * Routing

        Args:
            query: Requ√™te utilisateur
            tenant_id: ID utilisateur/organisation (identifie l'utilisateur)
            conversation_id: ID de conversation (isole les sessions m√©moire)
            language: Langue d√©tect√©e
            use_json_search: Activer recherche JSON
            genetic_line_filter: Filtre lign√©e g√©n√©tique
            performance_context: Contexte performance

        Returns:
            RAGResult ou None si erreur
        """
        rag_engine = self.get_rag_engine()

        if not rag_engine or not safe_get_attribute(
            rag_engine, "is_initialized", False
        ):
            logger.error("RAG Engine non disponible ou non initialis√©")
            return None

        try:
            if not hasattr(rag_engine, "generate_response"):
                logger.error("RAG Engine sans m√©thode generate_response")
                return None

            logger.info(
                f"üéØ Appel RAG pour tenant={tenant_id}, "
                f"conversation={conversation_id or 'none'}, lang={language}"
            )

            # Appel RAG - tenant_id identifie l'utilisateur, conversation_id isole la m√©moire
            rag_result = await rag_engine.generate_response(
                query=query,
                tenant_id=tenant_id,
                conversation_id=conversation_id,  # üÜï Passer conversation_id
                language=language,
                use_json_search=use_json_search,
                genetic_line_filter=genetic_line_filter,
                performance_context=performance_context,
                enable_preprocessing=True,
            )

            # Le router a g√©r√© la validation et le contexte
            # V√©rifier si clarification n√©cessaire
            if hasattr(rag_result, "metadata"):
                metadata = rag_result.metadata or {}
                if metadata.get("needs_clarification"):
                    logger.info("‚ö†Ô∏è Clarification n√©cessaire d√©tect√©e par le router")
                    # Le message de clarification est dans rag_result.answer
                    return rag_result

            return rag_result

        except Exception as e:
            logger.error(f"Erreur generate_response: {e}", exc_info=True)
            return None

    def create_fallback_result(
        self,
        message: str,
        language: str,
        fallback_reason: str,
        total_start_time: float,
        use_json_search: bool = True,
        genetic_line_filter: Optional[str] = None,
    ):
        """
        Cr√©e un r√©sultat de fallback avec r√©ponse aviculture g√©n√©rique

        Args:
            message: Message original
            language: Langue
            fallback_reason: Raison du fallback
            total_start_time: Timestamp de d√©but
            use_json_search: Flag JSON search
            genetic_line_filter: Filtre lign√©e

        Returns:
            FallbackResult avec r√©ponse g√©n√©rique
        """
        aviculture_response = get_aviculture_response(message, language)

        class FallbackResult:
            def __init__(self, answer, reason):
                self.answer = answer
                self.source = "aviculture_fallback"
                self.confidence = 0.8
                self.processing_time = time.time() - total_start_time
                self.metadata = {
                    "fallback_used": True,
                    "fallback_reason": reason,
                    "source_type": "integrated_knowledge",
                    "json_system_attempted": use_json_search,
                    "genetic_line_filter": genetic_line_filter,
                    "preprocessing_enabled": True,
                    "router_version": "5.1.0",
                }
                self.context_docs = []

        return FallbackResult(aviculture_response, fallback_reason)

    # üóëÔ∏è DEPRECATED - M√©thode remplac√©e par sauvegarde inline avec follow-up support
    # async def _save_to_memory(
    #     self,
    #     tenant_id: str,
    #     message: str,
    #     answer: str,
    #     metadata: Optional[Dict[str, Any]] = None,
    # ) -> None:
    #     """
    #     Sauvegarde dans ConversationMemory (syst√®me unique)
    #
    #     Args:
    #         tenant_id: ID utilisateur
    #         message: Question de l'utilisateur
    #         answer: R√©ponse g√©n√©r√©e
    #         metadata: M√©tadonn√©es optionnelles
    #     """
    #     # Sauvegarde dans ConversationMemory
    #     if self.conversation_memory:
    #         try:
    #             self.conversation_memory.add_exchange(
    #                 tenant_id=tenant_id, question=message, answer=answer
    #             )
    #             logger.debug(f"‚úÖ Sauvegarde ConversationMemory OK pour {tenant_id}")
    #         except Exception as e:
    #             logger.error(f"‚ùå Erreur sauvegarde ConversationMemory: {e}")
    #     else:
    #         logger.warning(f"‚ö†Ô∏è ConversationMemory non disponible pour {tenant_id}")

    async def generate_streaming_response(
        self,
        rag_result: Any,
        message: str,
        tenant_id: str,
        language: str,
        total_processing_time: float,
        conversation_id: Optional[str] = None,  # üÜï ID de conversation pour m√©moire
    ):
        """
        G√©n√®re un flux de r√©ponse SSE (Server-Sent Events)

        VERSION 5.1.0:
        - Double sauvegarde m√©moire (ConversationMemory + ancien)
        - M√©tadonn√©es enrichies
        - Plus de gestion de clarification ici
        - Le router a d√©j√† g√©r√© tout le contexte

        Args:
            rag_result: R√©sultat du RAG Engine
            message: Message original
            tenant_id: ID utilisateur
            language: Langue
            total_processing_time: Temps total de traitement
            conversation_id: ID de conversation (pour isolation m√©moire)

        Yields:
            Events SSE format√©s
        """
        try:
            # Extraction m√©tadonn√©es
            metadata = safe_get_attribute(rag_result, "metadata", {}) or {}
            source = safe_get_attribute(rag_result, "source", "unknown")
            confidence = safe_get_attribute(rag_result, "confidence", 0.5)
            processing_time = safe_get_attribute(rag_result, "processing_time", 0)

            # Normaliser source (peut √™tre un enum)
            if hasattr(source, "value"):
                source = source.value
            else:
                source = str(source)

            # Event START
            logger.info(
                f"üì§ Sending START event with source='{source}', confidence={confidence}"
            )
            start_data = {
                "type": "start",
                "source": source,
                "confidence": float(confidence),
                "processing_time": float(processing_time),
                "fallback_used": safe_dict_get(metadata, "fallback_used", False),
                "architecture": "query-router-v5.1",
                "serialization_version": "optimized_cached",
                "preprocessing_enabled": True,
                "router_managed": True,
                "memory_enabled": self.conversation_memory is not None,
                "needs_clarification": metadata.get("needs_clarification", False),
                "missing_fields": metadata.get("missing_fields", []),
                "json_system_used": metadata.get("json_system", {}).get("used", False),
                "json_results_count": metadata.get("json_system", {}).get(
                    "results_count", 0
                ),
                "genetic_line_detected": metadata.get("json_system", {}).get(
                    "genetic_line_filter"
                ),
            }

            yield sse_event(safe_serialize_for_json(start_data))

            # Extraction de la r√©ponse (avec fallbacks)
            answer = safe_get_attribute(rag_result, "answer", "")
            if not answer:
                answer = safe_get_attribute(rag_result, "response", "")
                if not answer:
                    answer = safe_get_attribute(rag_result, "text", "")
                    if not answer:
                        # Dernier fallback: r√©ponse aviculture
                        answer = get_aviculture_response(message, language)

            # Streaming de la r√©ponse par chunks
            if answer:
                chunks = smart_chunk_text(str(answer), STREAM_CHUNK_LEN)
                for i, chunk in enumerate(chunks):
                    yield sse_event(
                        {"type": "chunk", "content": chunk, "chunk_index": i}
                    )
                    await asyncio.sleep(0.01)  # Petit d√©lai pour fluidit√©

            # Envoyer follow-up proactif si disponible (message s√©par√©)
            proactive_followup = metadata.get("proactive_followup")
            followup_to_save = None  # üÜï Variable pour sauvegarder le follow-up
            if proactive_followup and isinstance(proactive_followup, str):
                logger.info(
                    f"üì§ Envoi follow-up proactif: {proactive_followup[:80]}..."
                )
                followup_to_save = proactive_followup  # üÜï Capturer pour sauvegarde
                yield sse_event(
                    {
                        "type": "proactive_followup",
                        "suggestion": proactive_followup,
                    }
                )
                await asyncio.sleep(0.01)

            # Extraction documents utilis√©s
            context_docs = safe_get_attribute(rag_result, "context_docs", [])
            if not isinstance(context_docs, list):
                context_docs = []

            documents_used = 0
            if hasattr(rag_result, "metadata") and rag_result.metadata:
                documents_used = rag_result.metadata.get("documents_used", 0)

            if documents_used == 0:
                documents_used = len(context_docs)

            # üñºÔ∏è Extraction des images associ√©es
            images = safe_get_attribute(rag_result, "images", [])
            if not isinstance(images, list):
                images = []
            logger.info(f"üñºÔ∏è Retrieved {len(images)} images for response")

            # Event END
            # üîç DEBUG: Extract CoT fields before building end_data
            cot_thinking = safe_get_attribute(rag_result, "cot_thinking", None)
            cot_analysis = safe_get_attribute(rag_result, "cot_analysis", None)
            has_cot_structure = safe_get_attribute(
                rag_result, "has_cot_structure", False
            )

            logger.info(
                f"üß† END event CoT fields - has_cot: {has_cot_structure}, thinking: {len(cot_thinking or '') } chars, analysis: {len(cot_analysis or '')} chars"
            )

            end_data = {
                "type": "end",
                "total_time": total_processing_time,
                "confidence": float(confidence),
                "documents_used": documents_used,
                "source": source,
                "architecture": "query-router-v5.1",
                "preprocessing_enabled": True,
                "router_managed": True,
                "memory_enabled": self.conversation_memory is not None,
                "needs_clarification": metadata.get("needs_clarification", False),
                "is_contextual": metadata.get("is_contextual", False),
                "json_system_used": metadata.get("json_system", {}).get("used", False),
                "json_results_count": metadata.get("json_system", {}).get(
                    "results_count", 0
                ),
                "genetic_lines_detected": metadata.get("json_system", {}).get(
                    "genetic_lines_detected", []
                ),
                "detection_version": "5.1.0_conversation_memory",
                # üß† Chain-of-Thought sections for PostgreSQL storage
                "cot_thinking": cot_thinking,
                "cot_analysis": cot_analysis,
                "has_cot_structure": has_cot_structure,
                # üñºÔ∏è Associated images
                "images": images,
            }

            serialized_data = safe_serialize_for_json(end_data)
            logger.info(f"üîç END event full data: {str(serialized_data)[:500]}")
            yield sse_event(serialized_data)

            # Sauvegarder dans les deux syst√®mes de m√©moire
            # Seulement si c'est une vraie r√©ponse (pas une clarification)
            if answer and source and not metadata.get("needs_clarification"):
                # üÜï Utiliser conversation_id comme cl√© m√©moire (fallback to tenant_id)
                memory_key = conversation_id or tenant_id
                logger.debug(f"üíæ Saving to memory with key: {memory_key}")

                # üÜï Sauvegarder avec le follow-up si pr√©sent
                if self.conversation_memory:
                    try:
                        self.conversation_memory.add_exchange(
                            tenant_id=memory_key,
                            question=message,
                            answer=str(answer),
                            followup=followup_to_save,  # üÜï Inclure le follow-up
                        )
                        logger.debug(
                            f"‚úÖ Sauvegarde ConversationMemory OK pour {memory_key} (with followup: {followup_to_save is not None})"
                        )
                    except Exception as e:
                        logger.error(f"‚ùå Erreur sauvegarde ConversationMemory: {e}")
                else:
                    logger.warning(
                        f"‚ö†Ô∏è ConversationMemory non disponible pour {memory_key}"
                    )

        except Exception as e:
            logger.error(f"Erreur streaming: {e}", exc_info=True)
            yield sse_event({"type": "error", "message": str(e)})

    def get_status(self) -> Dict[str, Any]:
        """
        Status des handlers

        Returns:
            Dict avec informations de status
        """
        rag_engine = self.get_rag_engine()

        return {
            "version": "5.1.0",
            "architecture": "query-router-integrated",
            "rag_engine_available": rag_engine is not None,
            "rag_engine_initialized": (
                safe_get_attribute(rag_engine, "is_initialized", False)
                if rag_engine
                else False
            ),
            "context_management": "router_managed",
            "clarification_management": "router_managed",
            "conversation_memory_enabled": self.conversation_memory is not None,
            "dual_memory_system": True,  # Nouveau + ancien
        }
