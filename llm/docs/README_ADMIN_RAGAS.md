# API Admin - RAGAS Evaluation

Endpoint API pour d√©clencher des √©valuations RAGAS du syst√®me RAG sur Digital Ocean App Platform.

## üöÄ Utilisation

### D√©clencher une √©valuation

```bash
curl -X POST https://expert.intelia.com/llm/admin/evaluate-rag \
  -H "Content-Type: application/json" \
  -H "User-Agent: Mozilla/5.0" \
  -d '{
    "test_cases": 2,
    "llm_model": "gpt-4o-mini",
    "use_simulation": false
  }'
```

**Param√®tres:**
- `test_cases`: Nombre de questions (‚ö†Ô∏è **MAX 2** pour √©viter timeout)
- `llm_model`: Mod√®le d'√©valuation (`gpt-4o-mini` ou `gpt-4o`)
- `use_simulation`: `false` pour RAG r√©el, `true` pour simulation

**R√©ponse (apr√®s ~50s):**
```json
{
  "scores": {
    "context_precision": 0.85,
    "context_recall": 0.78,
    "faithfulness": 0.92,
    "answer_relevancy": 0.88,
    "overall": 0.86
  },
  "detailed_scores": [...],
  "summary": "üìä RAGAS EVALUATION REPORT...",
  "timestamp": "2025-10-07T15:32:53",
  "llm_model": "gpt-4o-mini",
  "num_test_cases": 2,
  "duration_seconds": 51.88
}
```

### V√©rifier les d√©pendances

```bash
curl -s https://expert.intelia.com/llm/admin/debug-imports | python3 -m json.tool
```

**R√©ponse:**
```json
{
  "imports": {
    "ragas": {"available": true},
    "datasets": {"available": true},
    "langchain_openai": {"available": true}
  },
  "ready_for_evaluation": true
}
```

### Info syst√®me

```bash
curl -s https://expert.intelia.com/llm/admin/info | python3 -m json.tool
```

## ‚ö†Ô∏è Limitations

### Timeout 2 minutes
Digital Ocean App Platform a un timeout de **2 minutes** pour les requ√™tes HTTP.

**Dur√©e par question:**
- 1 question: ~25 secondes ‚úÖ
- 2 questions: ~50 secondes ‚úÖ
- 3 questions: ~75 secondes ‚úÖ (mais risqu√©)
- 5 questions: ~2+ minutes ‚ùå **TIMEOUT**

**Recommandation:** Maximum **2 questions** pour production

### Co√ªts estim√©s

| Questions | Dur√©e | Co√ªt (gpt-4o-mini) |
|-----------|-------|-------------------|
| 2         | ~50s  | ~$0.02           |
| 3         | ~75s  | ~$0.03           |
| 5         | ~2min | ~$0.05           |
| 10        | ~4min | ~$0.10 (timeout) |

**Budget recommand√©:** ~$1-2/mois pour √©valuations r√©guli√®res

## üìä Interpr√©tation des scores

### Scores individuels

**Context Precision (0-100%)**
- Pertinence des documents r√©cup√©r√©s
- **< 70%:** Documents non pertinents ‚Üí Am√©liorer reranking/similarit√©
- **70-85%:** Bon
- **> 85%:** Excellent

**Context Recall (0-100%)**
- Couverture du contexte vs ground truth
- **< 70%:** Contexte incomplet ‚Üí Augmenter top_k, enrichir base
- **70-85%:** Bon
- **> 85%:** Excellent

**Faithfulness (0-100%)**
- Fid√©lit√© de la r√©ponse au contexte (pas d'hallucination)
- **< 70%:** Hallucinations ‚Üí Renforcer guardrails, r√©duire temp√©rature
- **70-85%:** Bon
- **> 85%:** Excellent

**Answer Relevancy (0-100%)**
- Pertinence de la r√©ponse √† la question
- **< 70%:** R√©ponse hors-sujet ‚Üí Am√©liorer prompts
- **70-85%:** Bon
- **> 85%:** Excellent ‚úÖ

### Score global (Overall)

```
‚â• 90%  üèÜ Excellent
80-90% ‚úÖ Tr√®s bon
70-80% ‚ö†Ô∏è  Acceptable
< 70%  ‚ùå Probl√®me
```

## üß™ Dataset de test

**28 questions couvrant:**
- Calculs (feed, projections, reverse lookup)
- Diagnostics (sous-performance, FCR, stress)
- Nutrition (sp√©cifications, concepts)
- Environnement (temp√©rature, validation)
- Comparatifs (multi-√¢ges, Ross vs Cobb)
- Multi-langue (FR, EN, TH)
- Conversationnel (follow-up avec m√©moire)
- Validation (√¢ge hors limites, questions incompl√®tes)
- Out-of-domain (doit √™tre rejet√©)

**Fichier:** `llm/evaluation/golden_dataset_intelia.py`

## üîß Troubleshooting

### Erreur 524 (Timeout)
```
error code: 524
```
**Solution:** R√©duire `test_cases` √† 2 maximum

### Scores NaN
```json
{
  "context_precision": 0.0,
  "context_recall": "nan"
}
```
**Cause:** Contexts vides ‚Üí Questions de calcul ne retournent pas de contexts

**Solution:** En cours (correction CalculationHandler)

### D√©pendances manquantes
```json
{
  "ragas": {"available": false}
}
```
**Solution:** V√©rifier `requirements.txt` contient:
```
ragas==0.1.1
datasets>=2.14.0
langchain-core==0.1.52
langsmith>=0.1.0,<0.2.0
langchain-openai>=0.0.5
```

## üìù Workflow recommand√©

### Apr√®s modification du RAG
```bash
# 1. Test rapide (2 questions)
curl -X POST https://expert.intelia.com/llm/admin/evaluate-rag \
  -H "Content-Type: application/json" \
  -d '{"test_cases": 2, "llm_model": "gpt-4o-mini", "use_simulation": false}'

# 2. V√©rifier Answer Relevancy > 80%
# Si OK ‚Üí D√©ployer
```

### Monitoring hebdomadaire
```bash
# Lancer √©valuation tous les lundis
curl -X POST https://expert.intelia.com/llm/admin/evaluate-rag \
  -H "Content-Type: application/json" \
  -d '{"test_cases": 2, "llm_model": "gpt-4o-mini", "use_simulation": false}' \
  > weekly_evaluation_$(date +%Y%m%d).json

# V√©rifier score global
cat weekly_evaluation_*.json | python3 -c "import sys, json; data=json.load(sys.stdin); print(f\"Overall: {data['scores']['overall']:.2%}\")"
```

## üöß Probl√®mes connus

### 1. Context Precision = 0% pour certaines questions
**Sympt√¥me:** Documents retourn√©s non pertinents (ex: question sur maladies ‚Üí contextes sur feed intake)

**Cause:** Probl√®me dans le RAG Engine (embeddings, reranking, ou base vectorielle)

**Statut:** Investigation requise (Option 3)

### 2. Contexts vides pour calculs
**Sympt√¥me:** Questions de calcul retournent `contexts: []`

**Cause:** CalculationHandler ne renseigne pas `context_docs`

**Statut:** ‚úÖ **En cours de correction (Option 2)**

### 3. Timeout pour 3+ questions
**Sympt√¥me:** Erreur 524 apr√®s 2 minutes

**Cause:** Limite Digital Ocean App Platform

**Solution:** Utiliser 2 questions maximum

## üìö Documentation compl√®te

- **Guide d√©taill√©:** `llm/evaluation/README.md`
- **Quick start:** `llm/scripts/EVALUATION_QUICKSTART.md`
- **Dataset:** `llm/evaluation/golden_dataset_intelia.py`
- **Script CLI:** `llm/scripts/run_ragas_evaluation.py`

## üîó Endpoints disponibles

```
GET  /llm/admin/info              # Info syst√®me
GET  /llm/admin/debug-imports     # V√©rifier d√©pendances
POST /llm/admin/evaluate-rag      # D√©clencher √©valuation
GET  /llm/admin/evaluation-results    # R√©sultats (fichiers, non fonctionnel sur App Platform)
GET  /llm/admin/evaluation-history    # Historique (fichiers, non fonctionnel sur App Platform)
```

---

**Version:** 1.0
**Date:** 2025-10-07
**Statut:** ‚úÖ Op√©rationnel (avec limitations)
