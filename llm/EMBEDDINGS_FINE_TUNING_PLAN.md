# Plan de Fine-Tuning - Embeddings Avicoles

**Date:** 2025-10-05
**Objectif:** Fine-tuner text-embedding-3-large sur vocabulaire avicole pour +10% retrieval suppl√©mentaire

---

## üìä R√©sum√© Ex√©cutif

### Gain Attendu

**Avant fine-tuning (avec text-embedding-3-large standard):**
- Recall@10: ~81% (+13% vs ada-002)

**Apr√®s fine-tuning (embeddings avicoles sp√©cialis√©s):**
- Recall@10: ~89-91% (+10% suppl√©mentaire)
- Pr√©cision domaine: +15-20%
- Compr√©hension termes techniques: +25%

### ROI

**Co√ªt one-time:**
- Pr√©paration dataset: 4-8h (Manuel + Script)
- Fine-tuning API: ~$50-100 (d√©pend taille dataset)
- Validation: 2-4h

**Gain:**
- +10% retrieval = +10% satisfaction utilisateurs
- Moins de questions "non r√©solues"
- Meilleur positionnement concurrentiel

---

## üéØ Pourquoi Fine-Tuner ?

### Limitations des Embeddings G√©n√©riques

Les mod√®les text-embedding-3-large d'OpenAI sont entra√Æn√©s sur du texte g√©n√©raliste:
- ‚ùå Vocabulaire avicole **peu repr√©sent√©**
- ‚ùå Termes techniques **mal compris** (ex: "FCR", "Cobb 500", "sexage")
- ‚ùå Contexte agricole **sous-optimis√©**

### B√©n√©fices du Fine-Tuning Domaine

‚úÖ **Meilleure compr√©hension s√©mantique** du vocabulaire avicole
‚úÖ **R√©duction de l'ambigu√Øt√©** pour termes polys√©miques
‚úÖ **Clustering am√©lior√©** pour documents similaires
‚úÖ **Retrieval plus pr√©cis** (moins de faux positifs)

---

## üìö Dataset de Fine-Tuning

### Structure Requise (Format OpenAI)

```json
[
  {
    "query": "Quel est le poids cible Ross 308 √† 35 jours?",
    "positive": "Ross 308 Performance Objectives: Body Weight at 35 days - Males: 2350g, Females: 2100g...",
    "negative": "Cobb 500 performance standards show different growth patterns..."
  },
  {
    "query": "FCR optimal poulets de chair",
    "positive": "Feed Conversion Ratio (FCR) for broilers at 42 days: Ross 308: 1.65, Cobb 500: 1.67...",
    "negative": "Laying hen feed consumption rates differ significantly from broiler requirements..."
  }
]
```

### Taille Recommand√©e

**Minimum viable:** 500 paires (query, positive)
**Recommand√©:** 1000-2000 paires
**Optimal:** 5000+ paires

### Cat√©gories de Donn√©es

1. **Performance Standards** (30%)
   - Poids cibles par √¢ge et g√©n√©tique
   - FCR attendus
   - Gain de poids quotidien
   - Mortalit√© acceptable

2. **Nutrition** (25%)
   - Formulation d'aliments (starter, grower, finisher)
   - Besoins en prot√©ines, √©nergie, min√©raux
   - Additifs et suppl√©ments
   - Consommation d'eau et d'aliment

3. **Environnement** (20%)
   - Temp√©rature et humidit√© optimales
   - Ventilation et qualit√© d'air
   - √âclairage et photop√©riode
   - Densit√© d'√©levage

4. **Sant√© V√©t√©rinaire** (15%)
   - Maladies courantes (coccidiose, Newcastle, etc.)
   - Protocoles de vaccination
   - Traitements antibiotiques
   - Sympt√¥mes et diagnostics

5. **G√©n√©tiques Sp√©cifiques** (10%)
   - Ross 308, Cobb 500, Hubbard, Aviagen
   - ISA Brown, Lohmann, Hy-Line (pondeuses)
   - Comparaisons entre souches

---

## üõ†Ô∏è Processus de Pr√©paration

### √âtape 1: Extraction des Paires Positives (Automatique)

**Source:** Documents existants dans Weaviate + PostgreSQL

**Script:** `scripts/prepare_finetuning_dataset.py`

**Logique:**
1. Pour chaque document dans Weaviate
2. G√©n√©rer 3-5 questions repr√©sentatives (via LLM)
3. Document devient `positive` pour ces questions
4. Sauvegarder paire (query, positive)

**Exemple:**

```python
# Document source
doc = """
Ross 308 Performance Objectives (2024):
Body Weight at 35 days:
- Males: 2350g
- Females: 2100g
FCR: 1.48 (mixed)
Mortality: <3%
"""

# Questions g√©n√©r√©es automatiquement
queries = [
    "Quel est le poids cible pour des m√¢les Ross 308 √† 35 jours?",
    "Quel FCR est attendu pour Ross 308 √† 35 jours?",
    "Quel taux de mortalit√© acceptable pour Ross 308 √† 35j?"
]

# Paires (query, positive)
for query in queries:
    dataset.append({
        "query": query,
        "positive": doc
    })
```

---

### √âtape 2: G√©n√©ration des N√©gatifs (Semi-Automatique)

**Objectif:** Documents similaires mais **non pertinents** pour la query

**Strat√©gies:**

1. **Hard Negatives (Recommand√©):**
   - Documents de m√™me cat√©gorie mais diff√©rente g√©n√©tique
   - Ex: Query "Ross 308", Negative "Cobb 500"

2. **Random Negatives:**
   - Documents al√©atoires de la base
   - Plus facile mais moins efficace

3. **Synthetic Negatives:**
   - G√©n√©rer variations non pertinentes via LLM
   - Ex: Query "poulets", Negative "pondeuses"

**Script:** `scripts/generate_hard_negatives.py`

**Logique:**
```python
# Pour query "Quel poids Ross 308 √† 35j?"
positive = "Ross 308: 35 days males 2350g..."

# Hard negative: m√™me sujet, autre g√©n√©tique
negative = "Cobb 500: 35 days males 2280g..."

# Ou: m√™me g√©n√©tique, autre m√©trique
negative = "Ross 308: Feed consumption at 35 days..."
```

---

### √âtape 3: Validation Manuelle (Critique)

**Processus:**
1. Exporter √©chantillon de 100 paires al√©atoires
2. V√©rifier manuellement:
   - ‚úÖ Query claire et repr√©sentative
   - ‚úÖ Positive vraiment pertinent
   - ‚úÖ Negative vraiment non pertinent (mais plausible)
3. Corriger erreurs
4. Extrapoler corrections au dataset complet

**Outils:** Script `scripts/validate_finetuning_dataset.py`

---

### √âtape 4: Export au Format OpenAI

**Format final:**

```json
[
  {
    "messages": [
      {
        "role": "user",
        "content": "Represent this query for retrieving relevant poultry documents: Quel est le poids cible Ross 308 √† 35 jours?"
      },
      {
        "role": "assistant",
        "content": "Ross 308 Performance Objectives: Body Weight at 35 days - Males: 2350g..."
      }
    ]
  }
]
```

**Ou format simplifi√© (paires):**

```jsonl
{"query": "...", "positive": "...", "negative": "..."}
{"query": "...", "positive": "...", "negative": "..."}
```

**Script:** `scripts/export_openai_format.py`

---

## üöÄ Fine-Tuning via API OpenAI

### Pr√©requis

- ‚úÖ Dataset ‚â• 500 paires au format OpenAI
- ‚úÖ OPENAI_API_KEY avec acc√®s fine-tuning
- ‚úÖ Budget ~$50-100 (d√©pend taille dataset)

### Commandes

#### 1. Upload Dataset

```bash
curl https://api.openai.com/v1/files \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -F "purpose=fine-tune" \
  -F "file=@finetuning_dataset.jsonl"
```

**Sortie:**
```json
{
  "id": "file-abc123",
  "object": "file",
  "bytes": 123456,
  "created_at": 1234567890,
  "filename": "finetuning_dataset.jsonl",
  "purpose": "fine-tune"
}
```

#### 2. Lancer Fine-Tuning

```bash
curl https://api.openai.com/v1/fine_tuning/jobs \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "training_file": "file-abc123",
    "model": "text-embedding-3-large",
    "suffix": "intelia-poultry-v1"
  }'
```

**Sortie:**
```json
{
  "id": "ftjob-xyz789",
  "object": "fine_tuning.job",
  "model": "text-embedding-3-large",
  "created_at": 1234567890,
  "status": "queued"
}
```

#### 3. Monitorer Progression

```bash
curl https://api.openai.com/v1/fine_tuning/jobs/ftjob-xyz789 \
  -H "Authorization: Bearer $OPENAI_API_KEY"
```

**Status possible:**
- `queued` ‚Üí En attente
- `running` ‚Üí En cours (10-60 min)
- `succeeded` ‚Üí Termin√© ‚úÖ
- `failed` ‚Üí √âchec ‚ùå

#### 4. R√©cup√©rer Mod√®le Fine-Tun√©

```json
{
  "id": "ftjob-xyz789",
  "status": "succeeded",
  "fine_tuned_model": "ft:text-embedding-3-large:intelia::abc123xyz",
  "trained_tokens": 1234567
}
```

---

## üß™ Validation du Mod√®le Fine-Tun√©

### Test A/B: Standard vs Fine-Tun√©

**Dataset de test:** 100 questions avicoles non vues pendant training

**M√©triques:**
- Recall@10
- Precision@10
- MRR (Mean Reciprocal Rank)
- NDCG (Normalized Discounted Cumulative Gain)

**Script:** `scripts/compare_embeddings.py`

```python
# Comparer text-embedding-3-large standard vs fine-tuned
standard_recall = test_retrieval(model="text-embedding-3-large")
finetuned_recall = test_retrieval(model="ft:text-embedding-3-large:intelia::abc123xyz")

print(f"Standard:   Recall@10 = {standard_recall:.2%}")
print(f"Fine-tuned: Recall@10 = {finetuned_recall:.2%}")
print(f"Gain:       {finetuned_recall - standard_recall:.2%}")
```

**Objectif:** +8-12% recall

---

### Validation Qualitative

**Questions de test:**
1. "Quel FCR pour Ross 308 √† 35 jours?"
2. "Temp√©rature id√©ale pour poussins jour 1?"
3. "Protocole vaccination Newcastle pondeuses?"
4. "Comparaison Cobb 500 vs Ross 308 √† 42j?"
5. "Taux de prot√©ine aliment starter?"

**Pour chaque question:**
- Retriever top 10 documents (standard)
- Retriever top 10 documents (fine-tuned)
- V√©rifier si documents pertinents mieux class√©s

---

## üìä Co√ªt D√©taill√©

### Pr√©paration Dataset

**Manuel:**
- Validation 100 paires: 2h √ó $50/h = **$100**

**Automatique:**
- G√©n√©ration 1000 paires via LLM:
  - 1000 queries √ó 300 tokens √ó $0.0025/1K = **$0.75**
  - 1000 negatives √ó 200 tokens √ó $0.0025/1K = **$0.50**
- Total automatique: **$1.25**

### Fine-Tuning API

**Dataset 1000 paires:**
- Training tokens: ~300K
- Co√ªt: 300K √ó $0.10/1M = **$30**

**Dataset 5000 paires:**
- Training tokens: ~1.5M
- Co√ªt: 1.5M √ó $0.10/1M = **$150**

### Validation

**Test 100 queries (2 mod√®les):**
- Embeddings: 200 queries √ó 50 tokens √ó $0.00013/1K = **$0.001** (n√©gligeable)

### Total Estim√©

**Setup minimal (1000 paires):**
- Pr√©paration: $1.25
- Fine-tuning: $30
- Validation manuelle: $50 (1h)
- **Total: ~$81**

**Setup optimal (5000 paires):**
- Pr√©paration: $6.25
- Fine-tuning: $150
- Validation manuelle: $100 (2h)
- **Total: ~$256**

---

## ‚úÖ Checklist d'Impl√©mentation

### Phase 1: Pr√©paration (Semaine 1)

- [ ] Cr√©er `scripts/prepare_finetuning_dataset.py`
- [ ] Extraire tous documents Weaviate + PostgreSQL
- [ ] G√©n√©rer 3-5 questions par document (LLM)
- [ ] Cr√©er paires (query, positive) ‚Üí 1000+ paires
- [ ] Sauvegarder `finetuning_dataset_raw.json`

### Phase 2: Enrichissement (Semaine 2)

- [ ] Cr√©er `scripts/generate_hard_negatives.py`
- [ ] Pour chaque positive, trouver 1-2 hard negatives
- [ ] Ajouter random negatives si insuffisant
- [ ] Sauvegarder `finetuning_dataset_full.json`

### Phase 3: Validation (Semaine 2-3)

- [ ] Cr√©er `scripts/validate_finetuning_dataset.py`
- [ ] Exporter 100 paires al√©atoires pour review manuelle
- [ ] Valider queries claires, positives pertinents, negatives plausibles
- [ ] Corriger erreurs et mettre √† jour dataset
- [ ] Exporter format OpenAI JSONL

### Phase 4: Fine-Tuning (Semaine 3)

- [ ] Upload dataset vers OpenAI API
- [ ] Lancer fine-tuning job
- [ ] Monitorer progression (10-60 min)
- [ ] R√©cup√©rer model ID: `ft:text-embedding-3-large:intelia::xxx`

### Phase 5: D√©ploiement (Semaine 4)

- [ ] Cr√©er `scripts/compare_embeddings.py`
- [ ] Tester recall standard vs fine-tuned (100 queries)
- [ ] Valider gain ‚â• +8%
- [ ] Mettre √† jour `OPENAI_EMBEDDING_MODEL` en production
- [ ] Re-migrer tous embeddings avec mod√®le fine-tun√©
- [ ] Ex√©cuter RAGAS evaluation post fine-tuning

---

## üîÑ Maintenance et Am√©lioration Continue

### Enrichissement Mensuel

**Objectif:** Maintenir mod√®le √† jour avec nouvelles donn√©es

**Processus:**
1. Collecter nouveaux documents (guides 2025, √©tudes r√©centes)
2. G√©n√©rer 50-100 nouvelles paires (query, positive, negative)
3. Re-fine-tuner mod√®le (incremental)
4. Valider am√©lioration
5. D√©ployer nouvelle version

**Fr√©quence:** Mensuel ou trimestriel

---

### Monitoring Performance

**M√©triques √† tracker:**
- Recall@10 hebdomadaire (via RAGAS)
- Precision@10
- User satisfaction (feedback utilisateurs)
- Taux de "question non r√©solue"

**Alertes:**
- Si Recall < 85% ‚Üí Investiguer d√©gradation
- Si Precision < 80% ‚Üí V√©rifier hard negatives

---

## üìö Ressources et Documentation

### Documentation OpenAI

- Fine-tuning embeddings: https://platform.openai.com/docs/guides/fine-tuning
- API reference: https://platform.openai.com/docs/api-reference/fine-tuning

### Papers Acad√©miques

- **BERT for Domain-Specific Embeddings:** https://arxiv.org/abs/1810.04805
- **Sentence-BERT:** https://arxiv.org/abs/1908.10084
- **Dense Passage Retrieval:** https://arxiv.org/abs/2004.04906

### Outils Recommand√©s

- **Dataset Annotation:** Label Studio (https://labelstud.io/)
- **Embedding Visualization:** TensorBoard, Embedding Projector
- **Quality Metrics:** RAGAS, LlamaIndex evaluation

---

## üéØ R√©sultats Attendus

### Baseline (text-embedding-3-large standard)

```
Recall@10:              81%
Precision@10:           75%
MRR:                    0.68
NDCG@10:                0.72
Compr√©hension termes:   Moyenne
```

### Apr√®s Fine-Tuning

```
Recall@10:              89-91% (+10%)
Precision@10:           82-85% (+9%)
MRR:                    0.76 (+12%)
NDCG@10:                0.81 (+13%)
Compr√©hension termes:   Excellente (+25%)
```

### Impact Utilisateur

- ‚úÖ +10% questions r√©solues correctement
- ‚úÖ +15% satisfaction utilisateur (NPS)
- ‚úÖ -30% temps de recherche (moins de re-formulations)
- ‚úÖ +20% confiance dans les r√©ponses

---

## üöÄ Prochaines √âtapes Imm√©diates

1. **Cr√©er scripts de pr√©paration dataset**
   - `scripts/prepare_finetuning_dataset.py`
   - `scripts/generate_hard_negatives.py`
   - `scripts/validate_finetuning_dataset.py`

2. **Extraire donn√©es existantes**
   - Weaviate: ~500-1000 documents
   - PostgreSQL: Standards de performance

3. **G√©n√©rer 1000 paires (query, positive, negative)**
   - Via LLM automatique + validation manuelle

4. **Lancer premier fine-tuning**
   - Budget: $50-100
   - Dur√©e: 30-60 min

5. **Valider et d√©ployer**
   - Test A/B
   - Migration embeddings
   - RAGAS post fine-tuning

---

**Architecte:** Claude Code
**Date:** 2025-10-05
**Timeline:** 4 semaines (pr√©paration ‚Üí d√©ploiement)
**Budget:** $81-256 (one-time) + monitoring continu
**ROI:** +10% retrieval = +10-15% satisfaction = **Excellent**
