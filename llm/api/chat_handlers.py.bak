# -*- coding: utf-8 -*-
"""
api/chat_handlers.py - Logique de traitement des requ√™tes de chat
Version 4.5.0 - SIMPLIFICATION - Le router g√®re le contexte
"""

import time
import asyncio
import logging
from typing import Dict, Any, Optional

from config.config import STREAM_CHUNK_LEN
from utils.utilities import (
    safe_get_attribute,
    safe_dict_get,
    sse_event,
    smart_chunk_text,
    get_aviculture_response,
)
from .endpoints_utils import (
    safe_serialize_for_json,
    add_to_conversation_memory,
)
from .conversation_context import ConversationContextManager

logger = logging.getLogger(__name__)


class ChatHandlers:
    """Gestionnaires de logique m√©tier pour les endpoints de chat"""

    def __init__(
        self, context_manager: ConversationContextManager, services: Dict[str, Any]
    ):
        self.context_manager = context_manager
        self.services = services

    def get_rag_engine(self):
        """Helper pour r√©cup√©rer le RAG Engine"""
        health_monitor = self.services.get("health_monitor")
        if health_monitor:
            return health_monitor.get_service("rag_engine_enhanced")
        return None

    async def generate_rag_response(
        self,
        query: str,
        tenant_id: str,
        language: str,
        use_json_search: bool = True,
        genetic_line_filter: Optional[str] = None,
        performance_context: Optional[Dict[str, Any]] = None,
    ):
        """
        G√©n√®re une r√©ponse via le RAG Engine
        Retourne le r√©sultat ou None si erreur

        ‚úÖ VERSION 4.5.0: Simplifi√© - pas de gestion de contexte
        - Le router g√®re maintenant tout le contexte conversationnel
        - Cette m√©thode se concentre uniquement sur l'appel RAG
        """
        rag_engine = self.get_rag_engine()

        if not rag_engine or not safe_get_attribute(
            rag_engine, "is_initialized", False
        ):
            return None

        try:
            if not hasattr(rag_engine, "generate_response"):
                return None

            # Le contexte conversationnel est g√©r√© par le router
            # Ici on fait juste l'appel RAG basique
            logger.info(f"üéØ Appel RAG avec performance_context: {performance_context}")

            rag_result = await rag_engine.generate_response(
                query=query,
                tenant_id=tenant_id,
                language=language,
                use_json_search=use_json_search,
                genetic_line_filter=genetic_line_filter,
                performance_context=performance_context,
                enable_preprocessing=True,
            )

            return rag_result

        except Exception as e:
            logger.error(f"Erreur generate_response: {e}")
            return None

    def create_fallback_result(
        self,
        message: str,
        language: str,
        fallback_reason: str,
        total_start_time: float,
        use_json_search: bool = True,
        genetic_line_filter: Optional[str] = None,
    ):
        """Cr√©e un r√©sultat de fallback avec r√©ponse aviculture"""
        aviculture_response = get_aviculture_response(message, language)

        class FallbackResult:
            def __init__(self, answer, reason):
                self.answer = answer
                self.source = "aviculture_fallback"
                self.confidence = 0.8
                self.processing_time = time.time() - total_start_time
                self.metadata = {
                    "fallback_used": True,
                    "fallback_reason": reason,
                    "source_type": "integrated_knowledge",
                    "json_system_attempted": use_json_search,
                    "genetic_line_filter": genetic_line_filter,
                    "preprocessing_enabled": True,
                }
                self.context_docs = []

        return FallbackResult(aviculture_response, fallback_reason)

    async def generate_streaming_response(
        self,
        rag_result: Any,
        message: str,
        tenant_id: str,
        language: str,
        total_processing_time: float,
    ):
        """
        G√©n√®re un flux de r√©ponse SSE
        
        ‚úÖ VERSION 4.5.0: Simplifi√© - m√©tadonn√©es de base uniquement
        """
        try:
            metadata = safe_get_attribute(rag_result, "metadata", {}) or {}
            source = safe_get_attribute(rag_result, "source", "unknown")
            confidence = safe_get_attribute(rag_result, "confidence", 0.5)
            processing_time = safe_get_attribute(rag_result, "processing_time", 0)

            if hasattr(source, "value"):
                source = source.value
            else:
                source = str(source)

            start_data = {
                "type": "start",
                "source": source,
                "confidence": float(confidence),
                "processing_time": float(processing_time),
                "fallback_used": safe_dict_get(metadata, "fallback_used", False),
                "architecture": "modular-endpoints-simplified",
                "serialization_version": "optimized_cached",
                "preprocessing_enabled": True,
                "json_system_used": metadata.get("json_system", {}).get("used", False),
                "json_results_count": metadata.get("json_system", {}).get(
                    "results_count", 0
                ),
                "genetic_line_detected": metadata.get("json_system", {}).get(
                    "genetic_line_filter"
                ),
            }

            yield sse_event(safe_serialize_for_json(start_data))

            answer = safe_get_attribute(rag_result, "answer", "")
            if not answer:
                answer = safe_get_attribute(rag_result, "response", "")
                if not answer:
                    answer = safe_get_attribute(rag_result, "text", "")
                    if not answer:
                        answer = get_aviculture_response(message, language)

            if answer:
                chunks = smart_chunk_text(str(answer), STREAM_CHUNK_LEN)
                for i, chunk in enumerate(chunks):
                    yield sse_event(
                        {"type": "chunk", "content": chunk, "chunk_index": i}
                    )
                    await asyncio.sleep(0.01)

            context_docs = safe_get_attribute(rag_result, "context_docs", [])
            if not isinstance(context_docs, list):
                context_docs = []

            documents_used = 0
            if hasattr(rag_result, "metadata") and rag_result.metadata:
                documents_used = rag_result.metadata.get("documents_used", 0)

            if documents_used == 0:
                documents_used = len(context_docs)

            end_data = {
                "type": "end",
                "total_time": total_processing_time,
                "confidence": float(confidence),
                "documents_used": documents_used,
                "source": source,
                "architecture": "modular-endpoints-simplified",
                "preprocessing_enabled": True,
                "json_system_used": metadata.get("json_system", {}).get("used", False),
                "json_results_count": metadata.get("json_system", {}).get(
                    "results_count", 0
                ),
                "genetic_lines_detected": metadata.get("json_system", {}).get(
                    "genetic_lines_detected", []
                ),
                "detection_version": "4.5.0_simplified",
            }

            yield sse_event(safe_serialize_for_json(end_data))

            # Sauvegarder dans la m√©moire conversationnelle si pas de clarification
            if answer and source and not metadata.get("needs_clarification"):
                add_to_conversation_memory(
                    tenant_id, message, str(answer), "rag_enhanced_json"
                )

        except Exception as e:
            logger.error(f"Erreur streaming: {e}")
            yield sse_event({"type": "error", "message": str(e)})