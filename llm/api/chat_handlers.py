# -*- coding: utf-8 -*-
"""
api/chat_handlers.py - Logique de traitement des requ√™tes de chat
Version 4.4.0 - M√âMOIRE CONVERSATIONNELLE POUR R√âSOLUTION CONTEXTUELLE
"""

import time
import asyncio
import logging
from typing import Dict, Any, Optional

from config.config import STREAM_CHUNK_LEN
from utils.utilities import (
    safe_get_attribute,
    safe_dict_get,
    sse_event,
    smart_chunk_text,
    get_aviculture_response,
)
from .endpoints_utils import (
    safe_serialize_for_json,
    add_to_conversation_memory,
)
from .conversation_context import ConversationContextManager

logger = logging.getLogger(__name__)


class ChatHandlers:
    """Gestionnaires de logique m√©tier pour les endpoints de chat"""

    # Limite de tentatives de clarification (Test 8)
    MAX_CLARIFICATION_ATTEMPTS = 3

    def __init__(
        self, context_manager: ConversationContextManager, services: Dict[str, Any]
    ):
        self.context_manager = context_manager
        self.services = services

    def get_rag_engine(self):
        """Helper pour r√©cup√©rer le RAG Engine"""
        health_monitor = self.services.get("health_monitor")
        if health_monitor:
            return health_monitor.get_service("rag_engine_enhanced")
        return None

    async def handle_clarification_abandonment(
        self,
        message: str,
        tenant_id: str,
        pending_context: Dict,
        language: str,
        total_start_time: float,
    ):
        """
        G√®re l'abandon d'une clarification par l'utilisateur
        Retourne une r√©ponse g√©n√©rique appropri√©e
        """
        logger.info(f"Abandon clarification pour {tenant_id}")

        # Effacer le contexte en attente
        self.context_manager.clear_pending(tenant_id)

        # Extraire contexte partiel
        partial_entities = pending_context.get("partial_entities", {})
        age = partial_entities.get("age_days")

        # G√©n√©rer r√©ponse g√©n√©rique
        generic_responses = {
            "fr": f"Je comprends. Voici une moyenne g√©n√©rale pour les poulets de chair{' √† ' + str(age) + ' jours' if age else ''}: Le poids moyen se situe entre 300-2500g selon la souche et l'√¢ge. L'indice de conversion (FCR) est g√©n√©ralement de 1.5-1.9.",
            "en": f"I understand. Here's a general average for broilers{' at ' + str(age) + ' days' if age else ''}: Average weight ranges from 300-2500g depending on strain and age. Feed conversion ratio (FCR) is typically 1.5-1.9.",
            "es": f"Entiendo. Aqu√≠ hay un promedio general para pollos de engorde{' a ' + str(age) + ' d√≠as' if age else ''}: El peso promedio var√≠a entre 300-2500g seg√∫n la cepa y edad. El √≠ndice de conversi√≥n (FCR) es t√≠picamente 1.5-1.9.",
        }

        generic_answer = generic_responses.get(language, generic_responses["fr"])

        # Cr√©er r√©sultat g√©n√©rique
        class GenericResult:
            def __init__(self, answer):
                self.answer = answer
                self.source = "generic_fallback"
                self.confidence = 0.6
                self.processing_time = time.time() - total_start_time
                self.metadata = {
                    "clarification_abandoned": True,
                    "fallback_used": True,
                }
                self.context_docs = []

        return GenericResult(generic_answer)

    async def handle_ambiguous_response(
        self,
        message: str,
        tenant_id: str,
        pending_context: Dict,
        language: str,
        total_start_time: float,
    ):
        """
        G√®re une r√©ponse ambigu√´ de l'utilisateur

        ‚úÖ Test 8: Limite de tentatives + r√©ponse g√©n√©rique apr√®s MAX_CLARIFICATION_ATTEMPTS
        """
        logger.warning(f"‚ö†Ô∏è R√©ponse ambigu√´ d√©tect√©e: {message}")

        # Incr√©menter compteur AVANT v√©rification
        self.context_manager.increment_clarification_attempt(tenant_id)

        # ‚úÖ V√âRIFIER LIMITE DE TENTATIVES (Test 8)
        attempts = pending_context.get("clarification_attempts", 0)

        if attempts >= self.MAX_CLARIFICATION_ATTEMPTS:
            logger.warning(
                f"üõë Abandon apr√®s {attempts} tentatives - r√©ponse g√©n√©rique"
            )

            # Nettoyer contexte
            self.context_manager.clear_pending(tenant_id)

            # G√©n√©rer r√©ponse g√©n√©rique avec disclaimer
            fallback_messages = {
                "fr": (
                    "Je comprends que vous n'avez pas l'information exacte sous la main. "
                    "Voici des donn√©es g√©n√©rales qui pourraient vous aider :\n\n"
                    "**Pour les poulets de chair (broilers) :**\n"
                    "- Poids moyen : 300g (J1) √† 2500g (J42) selon la souche\n"
                    "- FCR moyen : 1.5-1.9 selon l'√¢ge et la g√©n√©tique\n"
                    "- Consommation eau : 1.8-2.2x la consommation d'aliment\n\n"
                    "Pour une r√©ponse plus pr√©cise, n'h√©sitez pas √† me donner la race et l'√¢ge exacts."
                ),
                "en": (
                    "I understand you don't have the exact information at hand. "
                    "Here's general data that might help:\n\n"
                    "**For broilers:**\n"
                    "- Average weight: 300g (D1) to 2500g (D42) depending on strain\n"
                    "- Average FCR: 1.5-1.9 depending on age and genetics\n"
                    "- Water consumption: 1.8-2.2x feed consumption\n\n"
                    "For a more precise answer, feel free to provide the exact breed and age."
                ),
                "es": (
                    "Entiendo que no tiene la informaci√≥n exacta a mano. "
                    "Aqu√≠ hay datos generales que podr√≠an ayudar:\n\n"
                    "**Para pollos de engorde:**\n"
                    "- Peso promedio: 300g (D1) a 2500g (D42) seg√∫n la cepa\n"
                    "- FCR promedio: 1.5-1.9 seg√∫n edad y gen√©tica\n"
                    "- Consumo de agua: 1.8-2.2x consumo de alimento\n\n"
                    "Para una respuesta m√°s precisa, proporcione la raza y edad exactas."
                ),
            }

            fallback_message = fallback_messages.get(language, fallback_messages["en"])

            class FallbackResult:
                def __init__(self, answer):
                    self.answer = answer
                    self.source = "clarification_limit_exceeded"
                    self.confidence = 0.7
                    self.processing_time = time.time() - total_start_time
                    self.metadata = {
                        "clarification_abandoned": True,
                        "clarification_attempts": attempts,
                        "fallback_used": True,
                    }
                    self.context_docs = []

            return FallbackResult(fallback_message)

        # Si pas encore √† la limite, g√©n√©rer demande plus pr√©cise
        missing_fields = pending_context.get("missing_fields", [])
        retry_message = self.context_manager.generate_clarification_retry(
            message,
            missing_fields[0] if missing_fields else "breed",
            language,
        )

        class AmbiguityResult:
            def __init__(self, question):
                self.answer = question
                self.source = "needs_clarification"
                self.confidence = 0.8
                self.processing_time = time.time() - total_start_time
                self.metadata = {
                    "needs_clarification": True,
                    "ambiguous_response_detected": True,
                    "clarification_pending": True,
                    "clarification_attempts": attempts,
                }
                self.context_docs = []

        return AmbiguityResult(retry_message)

    async def handle_clarification_context(
        self, message: str, tenant_id: str, language: str, total_start_time: float
    ) -> Optional[Any]:
        """
        G√®re le contexte de clarification et retourne un r√©sultat si n√©cessaire
        Retourne None si le flux normal doit continuer

        ‚úÖ VERSION 4.3.1 - CORRECTION MAJEURE:
        - Fusion des entit√©s AVANT de retourner pour traitement
        - Flag continue_processing pour reprendre le flux RAG
        - Passage des entit√©s fusionn√©es au RAG Engine
        """
        pending_context = self.context_manager.get_pending(tenant_id)

        if not pending_context:
            return None

        # √âTAPE 1: V√©rifier abandon
        if self.context_manager.detect_clarification_abandon(message):
            return await self.handle_clarification_abandonment(
                message, tenant_id, pending_context, language, total_start_time
            )

        # √âTAPE 2: V√©rifier si c'est une r√©ponse √† la clarification
        if self.context_manager.is_clarification_response(message, pending_context):
            logger.info(f"D√©tection r√©ponse clarification pour {tenant_id}")

            # √âTAPE 3: V√©rifier ambigu√Øt√© AVANT accumulation
            if self.context_manager.detect_ambiguous_response(message):
                return await self.handle_ambiguous_response(
                    message, tenant_id, pending_context, language, total_start_time
                )

            # ‚úÖ √âTAPE 4: FUSIONNER LES ENTIT√âS (correction majeure)
            self.context_manager.update_accumulated_query(tenant_id, message)

            # R√©cup√©rer le contexte mis √† jour avec les entit√©s fusionn√©es
            updated_context = self.context_manager.get_pending(tenant_id)
            accumulated_query = updated_context["original_query"]
            partial_entities = updated_context.get("partial_entities", {})

            logger.info(f"üîÑ Reprise avec entit√©s fusionn√©es: {partial_entities}")
            logger.info(f"üìù Requ√™te accumul√©e: {accumulated_query}")

            # Pr√©server la langue originale
            original_language = updated_context.get("original_language")
            if original_language:
                language = original_language
                logger.info(f"Langue restaur√©e depuis contexte: {original_language}")

            # ‚úÖ RETOURNER UN FLAG POUR CONTINUER LE TRAITEMENT
            # Au lieu de retourner directement la requ√™te, on indique qu'il faut
            # continuer le traitement avec les entit√©s fusionn√©es
            return {
                "continue_processing": True,
                "accumulated_query": accumulated_query,
                "merged_entities": partial_entities,
                "language": language,
            }

        return None

    async def generate_rag_response(
        self,
        query: str,
        tenant_id: str,
        language: str,
        use_json_search: bool = True,
        genetic_line_filter: Optional[str] = None,
        performance_context: Optional[Dict[str, Any]] = None,
    ):
        """
        G√©n√®re une r√©ponse via le RAG Engine
        Retourne le r√©sultat ou None si erreur

        ‚úÖ VERSION 4.4.0: Support m√©moire conversationnelle
        - R√©cup√©ration du contexte de la derni√®re requ√™te r√©ussie
        - Transmission au RAG Engine pour r√©solution contextuelle
        """
        rag_engine = self.get_rag_engine()

        if not rag_engine or not safe_get_attribute(
            rag_engine, "is_initialized", False
        ):
            return None

        try:
            if not hasattr(rag_engine, "generate_response"):
                return None

            # ‚úÖ NOUVEAU v4.4: R√©cup√©rer le contexte conversationnel
            conversation_context = self.context_manager.get_last_context(tenant_id)

            if conversation_context:
                logger.info(f"üìñ Utilisation contexte conversationnel pour {tenant_id}")
                logger.debug(
                    f"   Previous: {conversation_context.get('previous_query', 'N/A')[:50]}..."
                )
                logger.debug(
                    f"   Entities: breed={conversation_context.get('breed')}, "
                    f"age={conversation_context.get('age_days')}, "
                    f"sex={conversation_context.get('sex')}"
                )

            logger.info(f"üéØ Appel RAG avec performance_context: {performance_context}")

            rag_result = await rag_engine.generate_response(
                query=query,
                tenant_id=tenant_id,
                language=language,
                use_json_search=use_json_search,
                genetic_line_filter=genetic_line_filter,
                performance_context=performance_context,
                conversation_context=conversation_context,  # ‚úÖ NOUVEAU v4.4
                enable_preprocessing=True,
            )

            return rag_result

        except Exception as e:
            logger.error(f"Erreur generate_response: {e}")
            return None

    async def handle_validation_status(
        self,
        rag_result: Any,
        message: str,
        tenant_id: str,
        language: str,
        total_start_time: float,
    ) -> Optional[Any]:
        """
        V√©rifie le statut de validation et retourne un r√©sultat de clarification si n√©cessaire
        Retourne None si validation OK

        ‚úÖ VERSION 4.4.0 - M√âMOIRE CONVERSATIONNELLE:
        - Stocke le contexte apr√®s validation r√©ussie
        - Permet la r√©solution de r√©f√©rences contextuelles ("at the same age", "for females")

        ‚úÖ VERSION 4.3.1 - CORRECTION CRITIQUE:
        - Stocke les entit√©s extraites lors du premier appel √† mark_pending()
        - Permet la fusion d'entit√©s dans les √©changes suivants
        """
        if not hasattr(rag_result, "metadata"):
            return None

        metadata = rag_result.metadata
        validation_status = metadata.get("validation_status")

        if validation_status != "needs_fallback":
            # Validation r√©ussie: effacer le contexte en attente
            pending_context = self.context_manager.get_pending(tenant_id)
            if pending_context:
                logger.info(f"Requ√™te compl√®te valid√©e pour {tenant_id}")
                self.context_manager.clear_pending(tenant_id)

            # ‚úÖ NOUVEAU v4.4: Stocker le contexte conversationnel apr√®s succ√®s
            enhanced_entities = metadata.get("enhanced_entities", {})

            # Ne stocker que si on a les informations essentielles
            if enhanced_entities and enhanced_entities.get("breed"):
                self.context_manager.store_last_successful_query(
                    tenant_id=tenant_id,
                    query=message,
                    entities=enhanced_entities,
                    language=language,
                )
                logger.info(f"üíæ Contexte conversationnel stock√© pour {tenant_id}")
                logger.debug(
                    f"   Stored: breed={enhanced_entities.get('breed')}, "
                    f"age={enhanced_entities.get('age_days')}, "
                    f"sex={enhanced_entities.get('sex')}, "
                    f"metric={enhanced_entities.get('metric_type')}"
                )

            return None

        # Contexte insuffisant d√©tect√©
        logger.warning(f"Contexte insuffisant d√©tect√© pour {tenant_id}")

        missing_fields = metadata.get("missing_fields", [])
        suggestions = metadata.get("suggestions", {})

        # ‚úÖ CORRECTION CRITIQUE: Extraire les entit√©s d√©tect√©es depuis metadata
        detected_entities = metadata.get("detected_entities", {})

        logger.info(f"üîç Entit√©s d√©tect√©es par le validator: {detected_entities}")

        # Si pas encore en attente, marquer maintenant AVEC les entit√©s
        pending_context = self.context_manager.get_pending(tenant_id)
        if not pending_context:
            # ‚úÖ PASSER LES ENTIT√âS D√âTECT√âES lors de la cr√©ation du contexte
            self.context_manager.mark_pending(
                tenant_id=tenant_id,
                original_query=message,
                missing_fields=missing_fields,
                suggestions=suggestions,
                language=language,
                partial_entities=detected_entities,  # ‚úÖ AJOUT CRITIQUE
            )
            logger.info(
                f"‚úÖ Contexte cr√©√© avec entit√©s partielles: {detected_entities}"
            )
        else:
            # Mettre √† jour les champs manquants
            self.context_manager.pending_clarifications[tenant_id][
                "missing_fields"
            ] = missing_fields
            self.context_manager.pending_clarifications[tenant_id][
                "suggestions"
            ] = suggestions

            # ‚úÖ METTRE √Ä JOUR les entit√©s partielles si de nouvelles sont d√©tect√©es
            existing_entities = self.context_manager.pending_clarifications[
                tenant_id
            ].get("partial_entities", {})
            # Fusionner : nouvelles entit√©s compl√®tent les existantes
            for key, value in detected_entities.items():
                if value is not None and (
                    not existing_entities.get(key) or existing_entities.get(key) is None
                ):
                    existing_entities[key] = value

            self.context_manager.pending_clarifications[tenant_id][
                "partial_entities"
            ] = existing_entities

            logger.info(f"‚úÖ Entit√©s partielles mises √† jour: {existing_entities}")

        # Utiliser le message d√©j√† g√©n√©r√© par le validator
        clarification_msg = rag_result.answer

        # Fallback si pas de message (s√©curit√©)
        if not clarification_msg:
            from .conversation_context import generate_clarification_question

            clarification_msg = generate_clarification_question(
                missing_fields=missing_fields,
                suggestions=suggestions,
                language=language,
            )

        logger.info(f"Question de clarification: {clarification_msg[:100]}...")

        # Cr√©er un r√©sultat sp√©cial pour la clarification
        class ClarificationResult:
            def __init__(self, question, missing):
                self.answer = question
                self.source = "needs_clarification"
                self.confidence = 0.9
                self.processing_time = time.time() - total_start_time
                self.metadata = {
                    "needs_clarification": True,
                    "missing_fields": missing,
                    "original_query": message,
                    "clarification_pending": True,
                }
                self.context_docs = []

        return ClarificationResult(clarification_msg, missing_fields)

    def create_fallback_result(
        self,
        message: str,
        language: str,
        fallback_reason: str,
        total_start_time: float,
        use_json_search: bool = True,
        genetic_line_filter: Optional[str] = None,
    ):
        """Cr√©e un r√©sultat de fallback avec r√©ponse aviculture"""
        aviculture_response = get_aviculture_response(message, language)

        class FallbackResult:
            def __init__(self, answer, reason):
                self.answer = answer
                self.source = "aviculture_fallback"
                self.confidence = 0.8
                self.processing_time = time.time() - total_start_time
                self.metadata = {
                    "fallback_used": True,
                    "fallback_reason": reason,
                    "source_type": "integrated_knowledge",
                    "json_system_attempted": use_json_search,
                    "genetic_line_filter": genetic_line_filter,
                    "preprocessing_enabled": True,
                }
                self.context_docs = []

        return FallbackResult(aviculture_response, fallback_reason)

    async def generate_streaming_response(
        self,
        rag_result: Any,
        message: str,
        tenant_id: str,
        language: str,
        total_processing_time: float,
    ):
        """
        G√©n√®re un flux de r√©ponse SSE
        """
        try:
            metadata = safe_get_attribute(rag_result, "metadata", {}) or {}
            source = safe_get_attribute(rag_result, "source", "unknown")
            confidence = safe_get_attribute(rag_result, "confidence", 0.5)
            processing_time = safe_get_attribute(rag_result, "processing_time", 0)

            if hasattr(source, "value"):
                source = source.value
            else:
                source = str(source)

            # R√©cup√©rer le contexte actuel pour les m√©tadonn√©es
            current_pending = self.context_manager.get_pending(tenant_id)

            start_data = {
                "type": "start",
                "source": source,
                "confidence": float(confidence),
                "processing_time": float(processing_time),
                "fallback_used": safe_dict_get(metadata, "fallback_used", False),
                "architecture": "modular-endpoints-conversational-memory",
                "serialization_version": "optimized_cached",
                "preprocessing_enabled": True,
                "needs_clarification": metadata.get("needs_clarification", False),
                "clarification_count": (
                    current_pending.get("clarification_count", 0)
                    if current_pending
                    else 0
                ),
                "clarification_attempts": (
                    current_pending.get("clarification_attempts", 0)
                    if current_pending
                    else 0
                ),
                "ambiguous_response_detected": metadata.get(
                    "ambiguous_response_detected", False
                ),
                "clarification_abandoned": metadata.get(
                    "clarification_abandoned", False
                ),
                "json_system_used": metadata.get("json_system", {}).get("used", False),
                "json_results_count": metadata.get("json_system", {}).get(
                    "results_count", 0
                ),
                "genetic_line_detected": metadata.get("json_system", {}).get(
                    "genetic_line_filter"
                ),
            }

            yield sse_event(safe_serialize_for_json(start_data))

            answer = safe_get_attribute(rag_result, "answer", "")
            if not answer:
                answer = safe_get_attribute(rag_result, "response", "")
                if not answer:
                    answer = safe_get_attribute(rag_result, "text", "")
                    if not answer:
                        answer = get_aviculture_response(message, language)

            if answer:
                chunks = smart_chunk_text(str(answer), STREAM_CHUNK_LEN)
                for i, chunk in enumerate(chunks):
                    yield sse_event(
                        {"type": "chunk", "content": chunk, "chunk_index": i}
                    )
                    await asyncio.sleep(0.01)

            context_docs = safe_get_attribute(rag_result, "context_docs", [])
            if not isinstance(context_docs, list):
                context_docs = []

            documents_used = 0
            if hasattr(rag_result, "metadata") and rag_result.metadata:
                documents_used = rag_result.metadata.get("documents_used", 0)

            if documents_used == 0:
                documents_used = len(context_docs)

            end_data = {
                "type": "end",
                "total_time": total_processing_time,
                "confidence": float(confidence),
                "documents_used": documents_used,
                "source": source,
                "architecture": "modular-endpoints-conversational-memory",
                "preprocessing_enabled": True,
                "needs_clarification": metadata.get("needs_clarification", False),
                "clarification_pending": metadata.get("clarification_pending", False),
                "clarification_count": (
                    current_pending.get("clarification_count", 0)
                    if current_pending
                    else 0
                ),
                "clarification_attempts": (
                    current_pending.get("clarification_attempts", 0)
                    if current_pending
                    else 0
                ),
                "ambiguous_response_detected": metadata.get(
                    "ambiguous_response_detected", False
                ),
                "clarification_abandoned": metadata.get(
                    "clarification_abandoned", False
                ),
                "json_system_used": metadata.get("json_system", {}).get("used", False),
                "json_results_count": metadata.get("json_system", {}).get(
                    "results_count", 0
                ),
                "genetic_lines_detected": metadata.get("json_system", {}).get(
                    "genetic_lines_detected", []
                ),
                "detection_version": "4.4.0_conversational_memory",
            }

            yield sse_event(safe_serialize_for_json(end_data))

            if answer and source and not metadata.get("needs_clarification"):
                add_to_conversation_memory(
                    tenant_id, message, str(answer), "rag_enhanced_json"
                )

        except Exception as e:
            logger.error(f"Erreur streaming: {e}")
            yield sse_event({"type": "error", "message": str(e)})
