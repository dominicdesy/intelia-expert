#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
migrate_embeddings.py - Migration vers text-embedding-3-large

Re-vectorise tous les documents existants avec le nouveau mod√®le d'embedding.
Supporte les dimensions r√©duites (1536) et compl√®tes (3072).

Usage:
    python scripts/migrate_embeddings.py [--dry-run] [--batch-size 100] [--collection COLLECTION]

Options:
    --dry-run         Simulation sans modification (compte uniquement)
    --batch-size N    Nombre de documents par batch (d√©faut: 100)
    --collection C    Nom de la collection Weaviate (d√©faut: Documents)
    --dimensions D    Dimensions vecteurs (1536 ou 3072, d√©faut: 1536)
    --skip-cache      Ne pas utiliser le cache Redis
"""

import asyncio
import os
import sys
import logging
import argparse
import time
from pathlib import Path

# Ajouter le r√©pertoire parent au path pour imports
sys.path.insert(0, str(Path(__file__).parent.parent))

# Imports apr√®s path setup
from utils.imports_and_dependencies import AsyncOpenAI
from retrieval.embedder import OpenAIEmbedder
from utils.types import List, Dict, Any

# Cr√©er logs directory AVANT logging config
os.makedirs("logs", exist_ok=True)

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("logs/migration_embeddings.log", mode="a"),
    ],
)
logger = logging.getLogger(__name__)


class EmbeddingMigrator:
    """Gestionnaire de migration des embeddings"""

    def __init__(
        self,
        collection_name: str = "Documents",
        batch_size: int = 100,
        dimensions: int = 1536,
        dry_run: bool = False,
        skip_cache: bool = False,
    ):
        self.collection_name = collection_name
        self.batch_size = batch_size
        self.dimensions = dimensions
        self.dry_run = dry_run
        self.skip_cache = skip_cache

        # Clients
        self.openai_client = None
        self.weaviate_client = None
        self.embedder = None

        # Stats
        self.stats = {
            "total_documents": 0,
            "processed": 0,
            "failed": 0,
            "skipped": 0,
            "start_time": None,
            "end_time": None,
        }

    async def initialize(self):
        """Initialise les clients"""
        logger.info("üîß Initialisation des clients...")

        # OpenAI
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY non configur√©e")

        self.openai_client = AsyncOpenAI(api_key=openai_api_key)
        logger.info("‚úÖ Client OpenAI initialis√©")

        # Embedder (sans cache si --skip-cache)
        cache_manager = None
        if not self.skip_cache:
            try:
                from cache.cache_manager import CacheManager

                cache_manager = CacheManager()
                await cache_manager.initialize()
                logger.info("‚úÖ Cache manager activ√©")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Cache non disponible: {e}")

        # V√©rifier mod√®le configur√©
        embedding_model = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-ada-002")
        logger.info(f"üìä Mod√®le d'embedding: {embedding_model}")
        logger.info(f"üìä Dimensions cibles: {self.dimensions}")

        self.embedder = OpenAIEmbedder(
            client=self.openai_client,
            cache_manager=cache_manager,
            model=embedding_model,
        )
        logger.info(f"‚úÖ Embedder initialis√© avec {embedding_model}")

        # Weaviate
        await self._connect_weaviate()

    async def _connect_weaviate(self):
        """Connexion Weaviate"""
        try:
            import weaviate
            import weaviate.classes as wvc_classes

            weaviate_url = os.getenv(
                "WEAVIATE_URL",
                "https://xmlc4jvtu6hfw9zrrmnw.c0.us-east1.gcp.weaviate.cloud",
            )
            weaviate_api_key = os.getenv("WEAVIATE_API_KEY", "")
            openai_api_key = os.getenv("OPENAI_API_KEY", "")

            logger.info(f"üîå Connexion Weaviate: {weaviate_url}")

            # D√©finir OPENAI_APIKEY pour Weaviate
            if openai_api_key and "OPENAI_APIKEY" not in os.environ:
                os.environ["OPENAI_APIKEY"] = openai_api_key

            # Connexion cloud
            if "weaviate.cloud" in weaviate_url:
                headers = {}
                if openai_api_key:
                    headers["X-OpenAI-Api-Key"] = openai_api_key

                self.weaviate_client = weaviate.connect_to_weaviate_cloud(
                    cluster_url=weaviate_url,
                    auth_credentials=wvc_classes.init.Auth.api_key(weaviate_api_key),
                    headers=headers,
                )
            else:
                # Connexion locale
                host = weaviate_url.replace("http://", "").replace("https://", "")
                self.weaviate_client = weaviate.connect_to_local(host=host)

            # Test connexion
            if self.weaviate_client and self.weaviate_client.is_ready():
                logger.info(f"‚úÖ Weaviate connect√©: {weaviate_url}")
            else:
                raise Exception("Weaviate non pr√™t")

        except Exception as e:
            logger.error(f"‚ùå Erreur connexion Weaviate: {e}")
            raise

    async def count_documents(self) -> int:
        """Compte le nombre total de documents"""
        try:
            collection = self.weaviate_client.collections.get(self.collection_name)

            # Compter avec aggregate
            result = collection.aggregate.over_all(total_count=True)
            count = result.total_count

            logger.info(f"üìä Documents trouv√©s: {count}")
            return count

        except Exception as e:
            logger.error(f"‚ùå Erreur comptage documents: {e}")
            return 0

    async def fetch_documents_batch(
        self, offset: int, limit: int
    ) -> List[Dict[str, Any]]:
        """R√©cup√®re un batch de documents"""
        try:
            collection = self.weaviate_client.collections.get(self.collection_name)

            # Fetch avec pagination
            results = collection.query.fetch_objects(
                limit=limit,
                offset=offset,
                include_vector=False,  # On ne r√©cup√®re pas les vecteurs existants
            )

            documents = []
            for obj in results.objects:
                # Extraire propri√©t√©s
                doc = {
                    "id": str(obj.uuid),
                    "content": obj.properties.get("content", ""),
                    "metadata": {
                        k: v for k, v in obj.properties.items() if k != "content"
                    },
                }
                documents.append(doc)

            return documents

        except Exception as e:
            logger.error(f"‚ùå Erreur fetch batch (offset={offset}): {e}")
            return []

    async def update_document_vector(
        self, doc_id: str, new_vector: List[float]
    ) -> bool:
        """Met √† jour le vecteur d'un document"""
        try:
            if self.dry_run:
                return True  # Simulation

            collection = self.weaviate_client.collections.get(self.collection_name)

            # Update uniquement le vecteur
            collection.data.update(uuid=doc_id, vector=new_vector)

            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur update vecteur {doc_id}: {e}")
            return False

    async def migrate_batch(self, documents: List[Dict[str, Any]]) -> Dict[str, int]:
        """Migre un batch de documents"""
        stats = {"success": 0, "failed": 0}

        try:
            # Extraire textes
            texts = [doc.get("content", "") for doc in documents]

            # Filtrer textes vides
            valid_docs = [
                (i, doc) for i, doc in enumerate(documents) if texts[i].strip()
            ]
            if not valid_docs:
                logger.warning("‚ö†Ô∏è Batch vide (tous textes vides)")
                stats["failed"] = len(documents)
                return stats

            valid_texts = [texts[i] for i, _ in valid_docs]

            # G√©n√©rer nouveaux embeddings (batch)
            logger.debug(
                f"   G√©n√©ration embeddings pour {len(valid_texts)} documents..."
            )

            try:
                # Appel direct API OpenAI avec dimensions
                response = await self.openai_client.embeddings.create(
                    model=self.embedder.model,
                    input=valid_texts,
                    encoding_format="float",
                    dimensions=self.dimensions,  # Force dimensions
                )

                if not response or not response.data:
                    logger.error("‚ùå R√©ponse embeddings vide")
                    stats["failed"] = len(documents)
                    return stats

                new_embeddings = [item.embedding for item in response.data]

            except Exception as emb_error:
                logger.error(f"‚ùå Erreur g√©n√©ration embeddings: {emb_error}")
                stats["failed"] = len(documents)
                return stats

            # Mettre √† jour les documents
            for (i, doc), new_vector in zip(valid_docs, new_embeddings):
                doc_id = doc["id"]

                # V√©rifier dimensions
                if len(new_vector) != self.dimensions:
                    logger.warning(
                        f"‚ö†Ô∏è Dimension mismatch {doc_id}: "
                        f"attendu {self.dimensions}, re√ßu {len(new_vector)}"
                    )
                    stats["failed"] += 1
                    continue

                # Update vecteur
                success = await self.update_document_vector(doc_id, new_vector)

                if success:
                    stats["success"] += 1
                else:
                    stats["failed"] += 1

        except Exception as e:
            logger.error(f"‚ùå Erreur migration batch: {e}")
            stats["failed"] = len(documents)

        return stats

    async def migrate_all(self):
        """Migration compl√®te de tous les documents"""
        self.stats["start_time"] = time.time()

        try:
            # Compter documents
            total = await self.count_documents()
            self.stats["total_documents"] = total

            if total == 0:
                logger.warning("‚ö†Ô∏è Aucun document √† migrer")
                return

            if self.dry_run:
                logger.info(f"üîç DRY RUN: {total} documents seraient migr√©s")
                return

            logger.info(f"üöÄ D√©but migration de {total} documents...")
            logger.info(f"   Batch size: {self.batch_size}")
            logger.info(f"   Dimensions: {self.dimensions}")
            logger.info(f"   Mod√®le: {self.embedder.model}")

            # Migration par batches
            offset = 0
            batch_num = 0

            while offset < total:
                batch_num += 1
                batch_size = min(self.batch_size, total - offset)

                logger.info(
                    f"üì¶ Batch {batch_num} (documents {offset+1}-{offset+batch_size}/{total})..."
                )

                # Fetch batch
                documents = await self.fetch_documents_batch(offset, batch_size)

                if not documents:
                    logger.warning(f"‚ö†Ô∏è Batch {batch_num} vide, skip")
                    offset += batch_size
                    continue

                # Migrer batch
                batch_stats = await self.migrate_batch(documents)

                # Stats
                self.stats["processed"] += batch_stats["success"]
                self.stats["failed"] += batch_stats["failed"]

                # Progress
                progress = (offset + batch_size) / total * 100
                elapsed = time.time() - self.stats["start_time"]
                rate = (offset + batch_size) / elapsed if elapsed > 0 else 0
                eta = (total - (offset + batch_size)) / rate if rate > 0 else 0

                logger.info(
                    f"   ‚úÖ Success: {batch_stats['success']}, "
                    f"‚ùå Failed: {batch_stats['failed']}"
                )
                logger.info(
                    f"   üìä Progress: {progress:.1f}% "
                    f"({offset+batch_size}/{total}) - "
                    f"Rate: {rate:.1f} docs/s - "
                    f"ETA: {eta/60:.1f} min"
                )

                offset += batch_size

                # Pause pour √©viter rate limiting
                await asyncio.sleep(0.5)

        except Exception as e:
            logger.error(f"‚ùå Erreur migration: {e}")
            raise

        finally:
            self.stats["end_time"] = time.time()
            await self._print_summary()

    async def _print_summary(self):
        """Affiche le r√©sum√© de migration"""
        duration = (
            self.stats["end_time"] - self.stats["start_time"]
            if self.stats["end_time"] and self.stats["start_time"]
            else 0
        )

        logger.info("\n" + "=" * 70)
        logger.info("üìä R√âSUM√â MIGRATION")
        logger.info("=" * 70)
        logger.info(f"Collection:       {self.collection_name}")
        logger.info(f"Mod√®le:           {self.embedder.model}")
        logger.info(f"Dimensions:       {self.dimensions}")
        logger.info(f"Mode:             {'DRY RUN' if self.dry_run else 'PRODUCTION'}")
        logger.info("")
        logger.info(f"Documents total:  {self.stats['total_documents']}")
        logger.info(f"‚úÖ Trait√©s:        {self.stats['processed']}")
        logger.info(f"‚ùå √âchecs:         {self.stats['failed']}")
        logger.info(f"‚è≠Ô∏è Skipped:         {self.stats['skipped']}")
        logger.info("")
        logger.info(f"Dur√©e:            {duration:.1f}s ({duration/60:.1f} min)")
        logger.info(
            f"Rate:             {self.stats['processed']/duration if duration > 0 else 0:.1f} docs/s"
        )
        logger.info("=" * 70)

        if self.stats["failed"] > 0:
            logger.warning(f"‚ö†Ô∏è {self.stats['failed']} documents ont √©chou√©")
        else:
            logger.info("üéâ Migration termin√©e avec succ√®s!")

    async def close(self):
        """Fermeture propre"""
        if self.weaviate_client:
            try:
                self.weaviate_client.close()
                logger.info("‚úÖ Weaviate ferm√©")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur fermeture Weaviate: {e}")

        if self.openai_client:
            try:
                await self.openai_client.close()
                logger.info("‚úÖ OpenAI ferm√©")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur fermeture OpenAI: {e}")


async def main():
    """Point d'entr√©e principal"""
    # Parser arguments
    parser = argparse.ArgumentParser(
        description="Migrer les embeddings vers text-embedding-3-large"
    )
    parser.add_argument(
        "--dry-run", action="store_true", help="Simulation sans modification"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=100,
        help="Nombre de documents par batch (d√©faut: 100)",
    )
    parser.add_argument(
        "--collection",
        type=str,
        default="Documents",
        help="Nom de la collection Weaviate (d√©faut: Documents)",
    )
    parser.add_argument(
        "--dimensions",
        type=int,
        default=1536,
        choices=[1536, 3072],
        help="Dimensions vecteurs (1536 ou 3072, d√©faut: 1536)",
    )
    parser.add_argument(
        "--skip-cache", action="store_true", help="Ne pas utiliser le cache Redis"
    )

    args = parser.parse_args()

    # Cr√©er logs directory
    os.makedirs("logs", exist_ok=True)

    # V√©rifier variables d'environnement critiques
    required_vars = ["OPENAI_API_KEY", "WEAVIATE_URL", "WEAVIATE_API_KEY"]
    missing_vars = [var for var in required_vars if not os.getenv(var)]

    if missing_vars:
        logger.error(
            f"‚ùå Variables d'environnement manquantes: {', '.join(missing_vars)}"
        )
        sys.exit(1)

    # V√©rifier mod√®le configur√©
    embedding_model = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-ada-002")
    if embedding_model == "text-embedding-ada-002":
        logger.warning(
            "‚ö†Ô∏è OPENAI_EMBEDDING_MODEL est toujours text-embedding-ada-002\n"
            "   Voulez-vous vraiment migrer vers le m√™me mod√®le?\n"
            "   Pour utiliser text-embedding-3-large, configurez:\n"
            "   export OPENAI_EMBEDDING_MODEL=text-embedding-3-large"
        )
        response = input("   Continuer quand m√™me? (y/N): ")
        if response.lower() != "y":
            logger.info("Migration annul√©e")
            sys.exit(0)

    # Cr√©er migrator
    migrator = EmbeddingMigrator(
        collection_name=args.collection,
        batch_size=args.batch_size,
        dimensions=args.dimensions,
        dry_run=args.dry_run,
        skip_cache=args.skip_cache,
    )

    try:
        # Initialiser
        await migrator.initialize()

        # Migrer
        await migrator.migrate_all()

    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è Migration interrompue par l'utilisateur")
        sys.exit(1)

    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)

    finally:
        # Cleanup
        await migrator.close()


if __name__ == "__main__":
    # Compatibilit√© Windows
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    asyncio.run(main())
