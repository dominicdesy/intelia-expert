#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
run_ragas_evaluation.py - Script d'√©valuation RAGAS pour Intelia Expert

√âvalue le syst√®me RAG avec un dataset de questions avicoles.
G√©n√®re un rapport d√©taill√© avec scores RAGAS.

Usage:
    python scripts/run_ragas_evaluation.py [--test-cases N] [--output PATH]

Options:
    --test-cases N    Nombre de cas de test (d√©faut: tous)
    --output PATH     Chemin fichier de sortie (d√©faut: logs/ragas_evaluation_{timestamp}.json)
    --llm MODEL       Mod√®le LLM √©valuateur (d√©faut: gpt-4o)
"""

import asyncio
import os
import sys
import argparse
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

# Fix UTF-8 encoding for Windows console FIRST (before any print)
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Charger .env depuis le r√©pertoire llm/
from dotenv import load_dotenv
env_path = Path(__file__).parent.parent / ".env"
if env_path.exists():
    load_dotenv(env_path)
    print(f"‚úÖ Variables d'environnement charg√©es depuis: {env_path}")
else:
    print(f"‚ö†Ô∏è Fichier .env non trouv√©: {env_path}")

# Ajouter r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

# Imports apr√®s path setup
from evaluation.ragas_evaluator import RAGASEvaluator  # noqa: E402
from evaluation.golden_dataset_intelia import get_intelia_test_dataset  # noqa: E402
from evaluation.golden_dataset_weaviate import get_weaviate_test_dataset  # noqa: E402
from evaluation.golden_dataset_weaviate_v2 import get_weaviate_v2_test_dataset  # noqa: E402

# Import du syst√®me RAG r√©el
try:
    from core.rag_engine import InteliaRAGEngine

    RAG_ENGINE_AVAILABLE = True
except ImportError:
    RAG_ENGINE_AVAILABLE = False
    logging.warning("‚ö†Ô∏è InteliaRAGEngine non disponible, mode simulation")

# Configuration logging
os.makedirs("logs", exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(
            f'logs/ragas_evaluation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        ),
    ],
)
logger = logging.getLogger(__name__)


async def query_rag_system(
    question: str, rag_engine: Optional[object] = None, expected_lang: str = "en"
) -> Dict[str, Any]:
    """
    Interroge le syst√®me RAG et retourne r√©ponse + contextes.

    Args:
        question: Question √† poser
        rag_engine: Instance de InteliaRAGEngine (optionnel)
        expected_lang: Langue attendue de la question (depuis le dataset)

    Returns:
        Dictionnaire avec:
            - answer: R√©ponse g√©n√©r√©e (str)
            - contexts: Liste de contextes r√©cup√©r√©s (List[str])
    """
    if rag_engine and RAG_ENGINE_AVAILABLE:
        try:
            # Utiliser la langue du dataset au lieu de la d√©tection automatique
            detected_lang = expected_lang
            logger.debug(f"   Langue utilis√©e: {detected_lang} (depuis dataset)")

            # Appel r√©el au RAG (m√©thode correcte: generate_response)
            result = await rag_engine.generate_response(
                query=question,
                language=detected_lang,  # Utiliser la langue d√©tect√©e
                conversation_id=f"ragas_eval_{datetime.now().timestamp()}",
            )

            # Extraire answer (RAGResult.answer)
            answer = result.answer or "[PAS DE R√âPONSE]"

            # Extraire contextes des documents (RAGResult.context_docs: List[Union[Document, dict]])
            contexts = []

            logger.info(f"   üìÑ RAG returned {len(result.context_docs)} context documents")

            for i, doc in enumerate(result.context_docs):
                # Extraction selon type (dict ou objet Document)
                if isinstance(doc, dict):
                    content = doc.get("content", "")
                else:
                    # Objet Document
                    content = getattr(doc, "content", "")

                if content and content.strip():
                    contexts.append(content)
                    logger.debug(f"      [{i+1}] Content extracted: {len(content)} chars")
                else:
                    logger.warning(f"      [{i+1}] Empty content in document: {type(doc)}")

            logger.info(f"   ‚úÖ Extracted {len(contexts)} non-empty contexts")

            return {"answer": answer, "contexts": contexts}

        except Exception as e:
            logger.error(f"‚ùå Erreur query RAG: {e}", exc_info=True)
            return {"answer": f"[ERREUR: {str(e)}]", "contexts": []}
    else:
        # Mode simulation (si RAG non disponible)
        logger.warning(f"‚ö†Ô∏è Simulation r√©ponse pour: {question[:50]}...")
        return {
            "answer": f"[SIMULATION] R√©ponse pour: {question}",
            "contexts": [
                f"[SIMULATION] Contexte 1 pour {question}",
                f"[SIMULATION] Contexte 2 pour {question}",
            ],
        }


async def run_evaluation(
    num_test_cases: int = None,
    output_path: str = None,
    llm_model: str = "gpt-4o",
    use_real_rag: bool = True,
    dataset_type: str = "intelia",
):
    """
    Ex√©cute l'√©valuation RAGAS compl√®te.

    Args:
        num_test_cases: Nombre de cas de test (None = tous)
        output_path: Chemin fichier de sortie
        llm_model: Mod√®le LLM √©valuateur
        use_real_rag: Utiliser vrai RAG (True) ou simulation (False)
        dataset_type: Type de dataset ('intelia' ou 'weaviate')
    """
    logger.info("=" * 70)
    logger.info("üöÄ RAGAS EVALUATION - Intelia Expert LLM")
    logger.info("=" * 70)

    # G√©n√©rer dataset golden
    if dataset_type == "weaviate_v2":
        logger.info("üìä Chargement dataset WEAVIATE V2 (questions bas√©es sur documents Health r√©els)...")
        golden_dataset = get_weaviate_v2_test_dataset()
    elif dataset_type == "weaviate":
        logger.info("üìä Chargement dataset WEAVIATE (contenu narratif/qualitatif)...")
        golden_dataset = get_weaviate_test_dataset()
    else:
        logger.info("üìä Chargement dataset INTELIA (mixte PostgreSQL+Weaviate)...")
        golden_dataset = get_intelia_test_dataset()

    # Limiter nombre de cas si sp√©cifi√©
    if num_test_cases:
        golden_dataset = golden_dataset[:num_test_cases]
        logger.info(f"   Limit√© √† {num_test_cases} cas de test")

    logger.info(f"   ‚úÖ {len(golden_dataset)} cas de test g√©n√©r√©s")

    # Initialiser RAG engine si disponible
    rag_engine = None
    if use_real_rag and RAG_ENGINE_AVAILABLE:
        try:
            logger.info("üîß Initialisation InteliaRAGEngine...")
            rag_engine = InteliaRAGEngine()
            await rag_engine.initialize()
            logger.info("   ‚úÖ RAG Engine initialis√©")
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è Impossible d'initialiser RAG: {e}")
            logger.info("   ‚Üí Mode simulation activ√©")
            rag_engine = None
    else:
        logger.info("üîç Mode simulation (RAG Engine non disponible)")

    # Interroger le RAG pour chaque cas
    logger.info(f"üîç Interrogation du syst√®me RAG ({len(golden_dataset)} questions)...")

    for i, case in enumerate(golden_dataset, 1):
        logger.info(f"   [{i}/{len(golden_dataset)}] {case['question'][:60]}...")

        # Query RAG avec la langue du dataset
        expected_lang = case.get("lang", "en")  # Langue depuis le dataset
        rag_result = await query_rag_system(
            question=case["question"],
            rag_engine=rag_engine,
            expected_lang=expected_lang
        )

        # Mettre √† jour cas de test
        case["answer"] = rag_result["answer"]
        case["contexts"] = rag_result["contexts"]

        # Pause pour √©viter rate limiting
        await asyncio.sleep(0.5)

    logger.info("   ‚úÖ Toutes les questions trait√©es")

    # Initialiser √©valuateur RAGAS
    logger.info(f"üîß Initialisation RAGASEvaluator (mod√®le: {llm_model})...")
    evaluator = RAGASEvaluator(llm_model=llm_model, temperature=0.0)
    logger.info("   ‚úÖ √âvaluateur initialis√©")

    # √âvaluer avec RAGAS
    logger.info("üìä √âvaluation RAGAS en cours...")
    results = await evaluator.evaluate_async(golden_dataset)

    # Afficher r√©sum√©
    print("\n" + results["summary"])

    # Sauvegarder r√©sultats
    if not output_path:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"logs/ragas_evaluation_{timestamp}.json"

    evaluator.save_results(results, output_path)

    logger.info(f"üíæ R√©sultats sauvegard√©s: {output_path}")

    # Cleanup RAG engine
    if rag_engine:
        try:
            await rag_engine.close()
            logger.info("‚úÖ RAG Engine ferm√©")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur fermeture RAG: {e}")

    logger.info("=" * 70)
    logger.info("‚úÖ √âVALUATION TERMIN√âE")
    logger.info("=" * 70)

    return results


def main():
    """Point d'entr√©e principal"""

    # Parser arguments
    parser = argparse.ArgumentParser(
        description="√âvaluation RAGAS pour Intelia Expert LLM"
    )
    parser.add_argument(
        "--test-cases",
        type=int,
        default=None,
        help="Nombre de cas de test (d√©faut: tous)",
    )
    parser.add_argument(
        "--output", type=str, default=None, help="Chemin fichier de sortie JSON"
    )
    parser.add_argument(
        "--llm",
        type=str,
        default="gpt-4o",
        help="Mod√®le LLM √©valuateur (d√©faut: gpt-4o)",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default="intelia",
        choices=["intelia", "weaviate", "weaviate_v2"],
        help="Dataset: 'intelia' (mixte), 'weaviate' (narratif), 'weaviate_v2' (Health docs r√©els)",
    )
    parser.add_argument(
        "--simulate",
        action="store_true",
        help="Mode simulation (ne pas utiliser vrai RAG)",
    )

    args = parser.parse_args()

    # V√©rifier variable d'environnement OpenAI
    if not os.getenv("OPENAI_API_KEY"):
        logger.error("‚ùå OPENAI_API_KEY non configur√©e")
        sys.exit(1)

    # Ex√©cuter √©valuation
    try:
        asyncio.run(
            run_evaluation(
                num_test_cases=args.test_cases,
                output_path=args.output,
                llm_model=args.llm,
                use_real_rag=not args.simulate,
                dataset_type=args.dataset,
            )
        )
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è √âvaluation interrompue par l'utilisateur")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    # Compatibilit√© Windows
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    main()
