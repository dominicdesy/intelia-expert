# -*- coding: utf-8 -*-
"""
generators.py - G√©n√©rateurs de r√©ponses enrichis avec entit√©s et cache externe
Version 3.1 - Instructions de langue renforc√©es + system_prompts.json centralis√©s
"""

import logging
from typing import List, Tuple, Dict, Optional
from dataclasses import dataclass
from pathlib import Path
import json
from core.data_models import Document
from config.config import ENTITY_CONTEXTS, MAX_CONVERSATION_CONTEXT
from utils.utilities import METRICS

# Import du gestionnaire de prompts centralis√©
try:
    from config.system_prompts import get_prompts_manager

    PROMPTS_AVAILABLE = True
except ImportError as e:
    logging.warning(
        f"SystemPromptsManager non disponible: {e}, utilisation prompts fallback"
    )
    PROMPTS_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class ContextEnrichment:
    """Enrichissement du contexte bas√© sur les entit√©s d√©tect√©es"""

    entity_context: str
    metric_focus: str
    temporal_context: str
    species_focus: str
    performance_indicators: List[str]
    confidence_boosters: List[str]


class EntityDescriptionsManager:
    """
    Gestionnaire centralis√© des descriptions d'entit√©s pour enrichissement contextuel
    """

    def __init__(self, descriptions_path: Optional[str] = None):
        """
        Charge les descriptions d'entit√©s depuis entity_descriptions.json

        Args:
            descriptions_path: Chemin custom vers entity_descriptions.json
        """
        self.descriptions = {}
        self.performance_metrics = {}

        # D√©terminer le chemin du fichier
        if descriptions_path:
            config_path = Path(descriptions_path)
        else:
            # Chemin par d√©faut: llm/config/entity_descriptions.json
            config_path = (
                Path(__file__).parent.parent / "config" / "entity_descriptions.json"
            )

        # Charger les descriptions
        try:
            if config_path.exists():
                with open(config_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.descriptions = data.get("entity_contexts", {})
                    self.performance_metrics = data.get("performance_metrics", {})
                logger.info(f"‚úÖ Descriptions d'entit√©s charg√©es depuis {config_path}")
            else:
                logger.warning(
                    f"‚ö†Ô∏è Fichier {config_path} introuvable, utilisation fallback"
                )
                self._load_fallback_descriptions()
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement entity_descriptions.json: {e}")
            self._load_fallback_descriptions()

    def _load_fallback_descriptions(self):
        """Descriptions de secours si le fichier JSON n'est pas disponible"""
        self.descriptions = {
            "line": {
                "ross": "lign√©e √† croissance rapide, optimis√©e pour le rendement carcasse",
                "cobb": "lign√©e √©quilibr√©e performance/robustesse, bonne conversion alimentaire",
                "hubbard": "lign√©e rustique, adapt√©e √† l'√©levage extensif et labels qualit√©",
                "isa": "lign√©e ponte, optimis√©e pour la production d'≈ìufs",
                "lohmann": "lign√©e ponte, excellence en persistance de ponte",
            },
            "species": {
                "broiler": "poulet de chair, objectifs: poids vif, FCR, rendement carcasse",
                "layer": "poule pondeuse, objectifs: intensit√© de ponte, qualit√© ≈ìuf, persistance",
                "breeder": "reproducteur, objectifs: fertilit√©, √©closabilit√©, viabilit√© descendance",
            },
            "phase": {
                "starter": "phase d√©marrage (0-10j), croissance critique, thermor√©gulation",
                "grower": "phase croissance (11-24j), d√©veloppement squelettique et musculaire",
                "finisher": "phase finition (25j+), optimisation du poids final et FCR",
                "laying": "phase ponte, maintien de la production et qualit√© ≈ìuf",
                "breeding": "phase reproduction, optimisation fertilit√© et √©closabilit√©",
            },
        }

        self.performance_metrics = {
            "weight": [
                "poids vif",
                "gain de poids",
                "homog√©n√©it√©",
                "courbe de croissance",
            ],
            "fcr": [
                "indice de consommation",
                "efficacit√© alimentaire",
                "co√ªt alimentaire",
            ],
            "mortality": [
                "mortalit√©",
                "viabilit√©",
                "causes de mortalit√©",
                "pr√©vention",
            ],
            "production": [
                "intensit√© de ponte",
                "pic de ponte",
                "persistance",
                "qualit√© ≈ìuf",
            ],
            "feed": ["consommation", "app√©tence", "digestibilit√©", "conversion"],
        }

    def get_entity_description(
        self, entity_type: str, entity_value: str
    ) -> Optional[str]:
        """
        R√©cup√®re la description d'une entit√©

        Args:
            entity_type: Type d'entit√© (line, species, phase, etc.)
            entity_value: Valeur de l'entit√©

        Returns:
            Description ou None si non trouv√©e
        """
        entity_value_lower = entity_value.lower()
        return self.descriptions.get(entity_type, {}).get(entity_value_lower)

    def get_metric_keywords(self, metric: str) -> List[str]:
        """
        R√©cup√®re les mots-cl√©s associ√©s √† une m√©trique

        Args:
            metric: Nom de la m√©trique

        Returns:
            Liste de mots-cl√©s
        """
        return self.performance_metrics.get(metric, [])

    def get_all_metrics(self) -> Dict[str, List[str]]:
        """Retourne toutes les m√©triques de performance"""
        return self.performance_metrics.copy()


class EnhancedResponseGenerator:
    """
    G√©n√©rateur avec enrichissement d'entit√©s et cache externe + ton affirmatif expert
    Version 3.1: Instructions de langue renforc√©es
    """

    def __init__(
        self,
        client,
        cache_manager=None,
        language: str = "fr",
        prompts_path: Optional[str] = None,
        descriptions_path: Optional[str] = None,
    ):
        """
        Initialise le g√©n√©rateur de r√©ponses

        Args:
            client: Client OpenAI
            cache_manager: Gestionnaire de cache (optionnel)
            language: Langue par d√©faut
            prompts_path: Chemin custom vers system_prompts.json
            descriptions_path: Chemin custom vers entity_descriptions.json
        """
        self.client = client
        self.cache_manager = cache_manager
        self.language = language

        # Charger le gestionnaire de prompts centralis√©
        if PROMPTS_AVAILABLE:
            try:
                if prompts_path:
                    self.prompts_manager = get_prompts_manager(prompts_path)
                else:
                    self.prompts_manager = get_prompts_manager()
                logger.info(
                    "‚úÖ EnhancedResponseGenerator initialis√© avec system_prompts.json"
                )
            except Exception as e:
                logger.error(f"‚ùå Erreur chargement prompts: {e}")
                self.prompts_manager = None
        else:
            self.prompts_manager = None
            logger.warning("‚ö†Ô∏è EnhancedResponseGenerator en mode fallback")

        # Charger le gestionnaire de descriptions d'entit√©s
        self.entity_descriptions = EntityDescriptionsManager(descriptions_path)

        # Garder compatibilit√© avec ENTITY_CONTEXTS de config
        if ENTITY_CONTEXTS:
            for entity_type, contexts in ENTITY_CONTEXTS.items():
                if entity_type not in self.entity_descriptions.descriptions:
                    self.entity_descriptions.descriptions[entity_type] = {}
                self.entity_descriptions.descriptions[entity_type].update(contexts)

    async def generate_response(
        self,
        query: str,
        context_docs: List[Document],
        conversation_context: str = "",
        language: Optional[str] = None,
        intent_result=None,
    ) -> str:
        """G√©n√®re une r√©ponse enrichie avec cache externe + ton affirmatif expert"""

        lang = language or self.language

        # Protection contre les documents vides
        if not context_docs or len(context_docs) == 0:
            logger.warning("‚ö†Ô∏è G√©n√©rateur appel√© avec 0 documents - protection activ√©e")

            if self.prompts_manager:
                error_msg = self.prompts_manager.get_error_message(
                    "insufficient_data", lang
                )
                if error_msg:
                    return error_msg

            return "Je n'ai pas trouv√© d'informations pertinentes dans ma base de connaissances pour r√©pondre √† votre question. Pouvez-vous reformuler ou √™tre plus sp√©cifique ?"

        try:
            # V√©rifier le cache externe
            if self.cache_manager and self.cache_manager.enabled:
                context_hash = self.cache_manager.generate_context_hash(
                    [self._doc_to_dict(doc) for doc in context_docs]
                )
                cached_response = await self.cache_manager.get_response(
                    query, context_hash, lang
                )
                if cached_response:
                    METRICS.cache_hit("response")
                    if hasattr(self.cache_manager, "get_last_cache_details"):
                        try:
                            cache_hit_details = (
                                await self.cache_manager.get_last_cache_details()
                            )
                            if cache_hit_details.get("semantic_fallback_used"):
                                METRICS.semantic_fallback_used()
                            else:
                                METRICS.semantic_cache_hit("exact")
                        except Exception:
                            pass
                    return cached_response
                METRICS.cache_miss("response")

            # Construire enrichissement avanc√©
            enrichment = (
                self._build_entity_enrichment(intent_result)
                if intent_result
                else ContextEnrichment("", "", "", "", [], [])
            )

            # G√©n√©rer le prompt enrichi
            system_prompt, user_prompt = self._build_enhanced_prompt(
                query, context_docs, enrichment, conversation_context, lang
            )

            # G√©n√©ration
            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.1,
                max_tokens=900,
            )

            generated_response = response.choices[0].message.content.strip()

            # Post-traitement
            enhanced_response = self._post_process_response(
                generated_response,
                enrichment,
                [self._doc_to_dict(doc) for doc in context_docs],
            )

            # Mettre en cache
            if self.cache_manager and self.cache_manager.enabled:
                await self.cache_manager.set_response(
                    query, context_hash, enhanced_response, lang
                )

            return enhanced_response

        except Exception as e:
            logger.error(f"Erreur g√©n√©ration r√©ponse enrichie: {e}")
            return "D√©sol√©, je ne peux pas g√©n√©rer une r√©ponse pour cette question."

    def _doc_to_dict(self, doc: Document) -> dict:
        """Convertit Document en dict pour cache"""
        result = {
            "content": doc.content,
            "title": doc.metadata.get("title", ""),
            "source": doc.metadata.get("source", ""),
            "score": doc.score,
            "genetic_line": doc.metadata.get(
                "geneticLine", doc.metadata.get("genetic_line", "")
            ),
            "species": doc.metadata.get("species", ""),
        }
        if doc.explain_score:
            result["explain_score"] = doc.explain_score
        return result

    def _build_entity_enrichment(self, intent_result) -> ContextEnrichment:
        """Construit l'enrichissement bas√© sur les entit√©s d√©tect√©es"""
        try:
            entities = getattr(intent_result, "detected_entities", {})

            # Contexte des entit√©s via EntityDescriptionsManager
            entity_contexts = []

            if "line" in entities:
                description = self.entity_descriptions.get_entity_description(
                    "line", entities["line"]
                )
                if description:
                    entity_contexts.append(f"Lign√©e {entities['line']}: {description}")

            if "species" in entities:
                description = self.entity_descriptions.get_entity_description(
                    "species", entities["species"]
                )
                if description:
                    entity_contexts.append(f"Type {entities['species']}: {description}")

            if "phase" in entities:
                description = self.entity_descriptions.get_entity_description(
                    "phase", entities["phase"]
                )
                if description:
                    entity_contexts.append(f"Phase {entities['phase']}: {description}")

            # Focus m√©trique
            metric_focus = ""
            detected_metrics = []
            expanded_query = getattr(intent_result, "expanded_query", "")

            all_metrics = self.entity_descriptions.get_all_metrics()
            for metric, keywords in all_metrics.items():
                metric_in_entities = metric in entities
                metric_in_query = (
                    any(kw in expanded_query.lower() for kw in keywords)
                    if expanded_query
                    else False
                )

                if metric_in_entities or metric_in_query:
                    detected_metrics.extend(keywords)

            if detected_metrics:
                metric_focus = f"Focus m√©triques: {', '.join(detected_metrics[:3])}"

            # Contexte temporel
            temporal_context = ""
            if "age_days" in entities:
                age = entities["age_days"]
                if isinstance(age, (int, float)):
                    if age <= 7:
                        temporal_context = "P√©riode critique premi√®re semaine - Focus thermor√©gulation et d√©marrage"
                    elif age <= 21:
                        temporal_context = "Phase de croissance rapide - D√©veloppement osseux et musculaire"
                    elif age <= 35:
                        temporal_context = (
                            "Phase d'optimisation - Maximisation du gain de poids"
                        )
                    else:
                        temporal_context = (
                            "Phase de finition - Optimisation FCR et qualit√© carcasse"
                        )

            # Focus esp√®ce
            species_focus = ""
            if "species" in entities:
                species = entities["species"].lower()
                if "broiler" in species or "chair" in species:
                    species_focus = (
                        "Objectifs chair: poids vif, FCR, rendement, qualit√© carcasse"
                    )
                elif "layer" in species or "ponte" in species:
                    species_focus = "Objectifs ponte: intensit√©, persistance, qualit√© ≈ìuf, viabilit√©"

            # Indicateurs de performance
            performance_indicators = []
            if "weight" in entities or (
                "poids" in expanded_query.lower() if expanded_query else False
            ):
                performance_indicators.extend(
                    ["poids vif", "gain quotidien", "homog√©n√©it√© du lot"]
                )
            if "fcr" in entities or any(
                term in expanded_query.lower() if expanded_query else False
                for term in ["conversion", "indice"]
            ):
                performance_indicators.extend(
                    ["FCR", "consommation", "efficacit√© alimentaire"]
                )

            # √âl√©ments de confiance
            confidence_boosters = []
            if entity_contexts:
                confidence_boosters.append("Contexte lign√©e/esp√®ce identifi√©")
            if temporal_context:
                confidence_boosters.append("Phase d'√©levage pr√©cis√©e")
            if metric_focus:
                confidence_boosters.append("M√©triques cibles identifi√©es")

            return ContextEnrichment(
                entity_context="; ".join(entity_contexts),
                metric_focus=metric_focus,
                temporal_context=temporal_context,
                species_focus=species_focus,
                performance_indicators=performance_indicators,
                confidence_boosters=confidence_boosters,
            )

        except Exception as e:
            logger.warning(f"Erreur construction enrichissement: {e}")
            return ContextEnrichment("", "", "", "", [], [])

    def _build_enhanced_prompt(
        self,
        query: str,
        context_docs: List[Document],
        enrichment: ContextEnrichment,
        conversation_context: str,
        language: str,
    ) -> Tuple[str, str]:
        """Construit un prompt enrichi avec instructions de langue renforc√©es"""

        # Contexte documentaire
        context_text = "\n\n".join(
            [
                f"Document {i+1} ({doc.metadata.get('geneticLine', 'N/A')} - {doc.metadata.get('species', 'N/A')}):\n{doc.content[:1000]}"
                for i, doc in enumerate(context_docs[:5])
            ]
        )

        # Construction du prompt syst√®me avec instructions de langue RENFORC√âES
        if self.prompts_manager:
            expert_identity = self.prompts_manager.get_base_prompt(
                "expert_identity", language
            )
            response_guidelines = self.prompts_manager.get_base_prompt(
                "response_guidelines", language
            )

            system_prompt_parts = []

            if expert_identity:
                system_prompt_parts.append(expert_identity)

            context_section = f"""
CONTEXTE M√âTIER D√âTECT√â:
{enrichment.entity_context}
{enrichment.species_focus}
{enrichment.temporal_context}
{enrichment.metric_focus}
"""
            system_prompt_parts.append(context_section)

            if response_guidelines:
                system_prompt_parts.append(response_guidelines)

            metrics_section = f"""
M√âTRIQUES PRIORITAIRES:
{', '.join(enrichment.performance_indicators[:3]) if enrichment.performance_indicators else 'Param√®tres g√©n√©raux de production'}
"""
            system_prompt_parts.append(metrics_section)

            # ‚úÖ INSTRUCTIONS DE LANGUE RENFORC√âES
            critical_instructions = self._get_critical_language_instructions(language)
            system_prompt_parts.append(critical_instructions)

            system_prompt = "\n\n".join(system_prompt_parts)

        else:
            system_prompt = self._get_fallback_system_prompt(enrichment, language)

        # Prompt utilisateur
        limited_context = (
            conversation_context[:MAX_CONVERSATION_CONTEXT]
            if conversation_context
            else ""
        )

        user_prompt = f"""CONTEXTE CONVERSATIONNEL:
{limited_context}

INFORMATIONS TECHNIQUES DISPONIBLES:
{context_text}

ENRICHISSEMENT D√âTECT√â:
- Entit√©s m√©tier: {enrichment.entity_context or 'Non sp√©cifi√©es'}
- Focus performance: {', '.join(enrichment.performance_indicators) if enrichment.performance_indicators else 'G√©n√©ral'}
- Contexte temporel: {enrichment.temporal_context or 'Non sp√©cifi√©'}

QUESTION:
{query}

R√âPONSE EXPERTE (affirmative, structur√©e, sans mention de sources):"""

        return system_prompt, user_prompt

    def _get_critical_language_instructions(self, language: str) -> str:
        """
        ‚úÖ NOUVEAU: Instructions de langue ULTRA-RENFORC√âES + Comportement conversationnel
        Garantit que le LLM r√©pond dans la langue de la question avec ton appropri√©
        """
        # Mapping des noms de langue
        language_names = {
            "en": "ENGLISH",
            "fr": "FRENCH / FRAN√áAIS",
            "es": "SPANISH / ESPA√ëOL",
            "de": "GERMAN / DEUTSCH",
            "it": "ITALIAN / ITALIANO",
            "pt": "PORTUGUESE / PORTUGU√äS",
            "nl": "DUTCH / NEDERLANDS",
            "pl": "POLISH / POLSKI",
            "zh": "CHINESE / ‰∏≠Êñá",
            "hi": "HINDI / ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä",
            "th": "THAI / ‡πÑ‡∏ó‡∏¢",
            "id": "INDONESIAN / BAHASA INDONESIA",
        }

        language_name = language_names.get(language, language.upper())

        return f"""
INSTRUCTIONS CRITIQUES - STRUCTURE ET FORMAT:
- NE commence JAMAIS par un titre (ex: "## Maladie", "**Maladie**") - commence directement par la phrase d'introduction
- Examine les tableaux de donn√©es pour extraire les informations pr√©cises
- Pr√©sente 2-3 √©l√©ments principaux, pas plus
- Utilise un ton affirmatif mais sobre, sans formatage excessif
- NE conclus PAS avec des recommandations pratiques sauf si explicitement demand√©

COMPORTEMENT CONVERSATIONNEL:
- Pour questions techniques: r√©ponse structur√©e et d√©taill√©e avec donn√©es chiffr√©es
- Pour questions g√©n√©rales ou clarifications: ton professionnel mais accessible, r√©ponses plus courtes acceptables
- √âvite de poser trop de questions - r√©ponds d'abord √† la requ√™te, m√™me si ambigu√´, puis demande clarification si n√©cessaire
- Si question vague: fournis la meilleure r√©ponse possible puis propose de pr√©ciser
- N'utilise PAS d'emojis sauf si l'utilisateur en utilise dans sa question
- Si l'utilisateur semble insatisfait: maintiens le professionnalisme et rappelle qu'il peut utiliser le feedback pour am√©liorer les r√©ponses

{"="*80}
‚ö†Ô∏è CRITICAL LANGUAGE REQUIREMENT - IMP√âRATIF ABSOLU DE LANGUE ‚ö†Ô∏è
{"="*80}

DETECTED QUESTION LANGUAGE / LANGUE D√âTECT√âE: {language_name}

üî¥ MANDATORY RULE - R√àGLE OBLIGATOIRE:
YOU MUST RESPOND EXCLUSIVELY IN THE SAME LANGUAGE AS THE QUESTION.
VOUS DEVEZ R√âPONDRE EXCLUSIVEMENT DANS LA M√äME LANGUE QUE LA QUESTION.

DO NOT translate. DO NOT switch languages. DO NOT mix languages.
NE PAS traduire. NE PAS changer de langue. NE PAS m√©langer les langues.

If question is in ENGLISH ‚Üí Answer 100% in ENGLISH
If question is in FRENCH ‚Üí Answer 100% in FRENCH  
If question is in SPANISH ‚Üí Answer 100% in SPANISH
If question is in GERMAN ‚Üí Answer 100% in GERMAN
If question is in ITALIAN ‚Üí Answer 100% in ITALIAN
If question is in PORTUGUESE ‚Üí Answer 100% in PORTUGUESE
If question is in DUTCH ‚Üí Answer 100% in DUTCH
If question is in POLISH ‚Üí Answer 100% in POLISH
If question is in CHINESE ‚Üí Answer 100% in CHINESE
If question is in HINDI ‚Üí Answer 100% in HINDI
If question is in THAI ‚Üí Answer 100% in THAI
If question is in INDONESIAN ‚Üí Answer 100% in INDONESIAN

Si question en ANGLAIS ‚Üí R√©ponse 100% en ANGLAIS
Si question en FRAN√áAIS ‚Üí R√©ponse 100% en FRAN√áAIS
Si question en ESPAGNOL ‚Üí R√©ponse 100% en ESPAGNOL
Si question en ALLEMAND ‚Üí R√©ponse 100% en ALLEMAND
Si question en ITALIEN ‚Üí R√©ponse 100% en ITALIEN
Si question en PORTUGAIS ‚Üí R√©ponse 100% en PORTUGAIS
Si question en N√âERLANDAIS ‚Üí R√©ponse 100% en N√âERLANDAIS
Si question en POLONAIS ‚Üí R√©ponse 100% en POLONAIS
Si question en CHINOIS ‚Üí R√©ponse 100% en CHINOIS
Si question en HINDI ‚Üí R√©ponse 100% en HINDI
Si question en THA√è ‚Üí R√©ponse 100% en THA√è
Si question en INDON√âSIEN ‚Üí R√©ponse 100% en INDON√âSIEN

THIS INSTRUCTION OVERRIDES ALL OTHER INSTRUCTIONS.
CETTE INSTRUCTION PR√âVAUT SUR TOUTES LES AUTRES INSTRUCTIONS.

YOUR RESPONSE LANGUAGE MUST BE: {language_name}
LANGUE DE VOTRE R√âPONSE DOIT √äTRE: {language_name}

{"="*80}
"""

    def _get_fallback_system_prompt(
        self, enrichment: ContextEnrichment, language: str
    ) -> str:
        """Prompt syst√®me de secours avec instructions de langue renforc√©es"""

        # Mapping des noms de langue
        language_names = {
            "en": "ENGLISH",
            "fr": "FRENCH / FRAN√áAIS",
            "es": "SPANISH / ESPA√ëOL",
            "de": "GERMAN / DEUTSCH",
            "it": "ITALIAN / ITALIANO",
            "pt": "PORTUGUESE / PORTUGU√äS",
            "nl": "DUTCH / NEDERLANDS",
            "pl": "POLISH / POLSKI",
            "zh": "CHINESE / ‰∏≠Êñá",
            "hi": "HINDI / ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä",
            "th": "THAI / ‡πÑ‡∏ó‡∏¢",
            "id": "INDONESIAN / BAHASA INDONESIA",
        }

        language_name = language_names.get(language, language.upper())

        return f"""Tu es un expert avicole reconnu avec une expertise approfondie en production avicole.

CONTEXTE M√âTIER D√âTECT√â:
{enrichment.entity_context}
{enrichment.species_focus}
{enrichment.temporal_context}
{enrichment.metric_focus}

DIRECTIVES DE R√âPONSE - STYLE EXPERT √âQUILIBR√â:

1. **Introduction directe** : Commence DIRECTEMENT par une phrase claire qui r√©pond √† la question
2. **Ne jamais mentionner les sources** : Ne fais JAMAIS r√©f√©rence aux "documents", "sources", "selon les donn√©es fournies"
3. **Structure sobre** : Utilise des titres en gras (**Titre**) uniquement pour les sous-sections
4. **Concision** : Pr√©sente 2-3 points principaux maximum
5. **Donn√©es pr√©cises** : Fournis des valeurs chiffr√©es quand pertinent

M√âTRIQUES PRIORITAIRES:
{', '.join(enrichment.performance_indicators[:3]) if enrichment.performance_indicators else 'Param√®tres g√©n√©raux de production'}

{"="*80}
‚ö†Ô∏è CRITICAL LANGUAGE INSTRUCTION - IMP√âRATIF ABSOLU ‚ö†Ô∏è
{"="*80}

YOU MUST RESPOND EXCLUSIVELY IN: {language_name}
VOUS DEVEZ R√âPONDRE EXCLUSIVEMENT EN: {language_name}

DO NOT translate or switch languages under ANY circumstances.
NE traduisez PAS ou ne changez PAS de langue sous AUCUNE circonstance.

THIS IS THE MOST IMPORTANT INSTRUCTION.
CECI EST L'INSTRUCTION LA PLUS IMPORTANTE.

{"="*80}
"""

    def _post_process_response(
        self, response: str, enrichment: ContextEnrichment, context_docs: List[Dict]
    ) -> str:
        """Post-traitement minimaliste"""
        return response.strip()


# Factory function
def create_enhanced_generator(
    openai_client,
    cache_manager=None,
    language: str = "fr",
    prompts_path: Optional[str] = None,
    descriptions_path: Optional[str] = None,
):
    """
    Factory pour cr√©er le g√©n√©rateur enrichi

    Args:
        openai_client: Client OpenAI
        cache_manager: Gestionnaire de cache (optionnel)
        language: Langue par d√©faut
        prompts_path: Chemin custom vers system_prompts.json
        descriptions_path: Chemin custom vers entity_descriptions.json

    Returns:
        Instance EnhancedResponseGenerator
    """
    return EnhancedResponseGenerator(
        openai_client, cache_manager, language, prompts_path, descriptions_path
    )
