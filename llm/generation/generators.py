# -*- coding: utf-8 -*-
"""
generators.py - GÃ©nÃ©rateurs de rÃ©ponses enrichis avec entitÃ©s et cache externe
Version 3.0 - Utilise system_prompts.json + entity_descriptions.json centralisÃ©s
"""

import logging
from typing import List, Tuple, Dict, Optional
from dataclasses import dataclass
from pathlib import Path
import json
from core.data_models import Document
from config.config import ENTITY_CONTEXTS, MAX_CONVERSATION_CONTEXT
from utils.utilities import METRICS

# Import du gestionnaire de prompts centralisÃ©
try:
    from llm.config.system_prompts import get_prompts_manager

    PROMPTS_AVAILABLE = True
except ImportError:
    logging.warning("SystemPromptsManager non disponible, utilisation prompts fallback")
    PROMPTS_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class ContextEnrichment:
    """Enrichissement du contexte basÃ© sur les entitÃ©s dÃ©tectÃ©es"""

    entity_context: str
    metric_focus: str
    temporal_context: str
    species_focus: str
    performance_indicators: List[str]
    confidence_boosters: List[str]


class EntityDescriptionsManager:
    """
    Gestionnaire centralisÃ© des descriptions d'entitÃ©s pour enrichissement contextuel
    """

    def __init__(self, descriptions_path: Optional[str] = None):
        """
        Charge les descriptions d'entitÃ©s depuis entity_descriptions.json

        Args:
            descriptions_path: Chemin custom vers entity_descriptions.json
        """
        self.descriptions = {}
        self.performance_metrics = {}

        # DÃ©terminer le chemin du fichier
        if descriptions_path:
            config_path = Path(descriptions_path)
        else:
            # Chemin par dÃ©faut: llm/config/entity_descriptions.json
            config_path = (
                Path(__file__).parent.parent / "config" / "entity_descriptions.json"
            )

        # Charger les descriptions
        try:
            if config_path.exists():
                with open(config_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.descriptions = data.get("entity_contexts", {})
                    self.performance_metrics = data.get("performance_metrics", {})
                logger.info(f"âœ… Descriptions d'entitÃ©s chargÃ©es depuis {config_path}")
            else:
                logger.warning(
                    f"âš ï¸ Fichier {config_path} introuvable, utilisation fallback"
                )
                self._load_fallback_descriptions()
        except Exception as e:
            logger.error(f"âŒ Erreur chargement entity_descriptions.json: {e}")
            self._load_fallback_descriptions()

    def _load_fallback_descriptions(self):
        """Descriptions de secours si le fichier JSON n'est pas disponible"""
        self.descriptions = {
            "line": {
                "ross": "lignÃ©e Ã  croissance rapide, optimisÃ©e pour le rendement carcasse",
                "cobb": "lignÃ©e Ã©quilibrÃ©e performance/robustesse, bonne conversion alimentaire",
                "hubbard": "lignÃ©e rustique, adaptÃ©e Ã  l'Ã©levage extensif et labels qualitÃ©",
                "isa": "lignÃ©e ponte, optimisÃ©e pour la production d'Å“ufs",
                "lohmann": "lignÃ©e ponte, excellence en persistance de ponte",
            },
            "species": {
                "broiler": "poulet de chair, objectifs: poids vif, FCR, rendement carcasse",
                "layer": "poule pondeuse, objectifs: intensitÃ© de ponte, qualitÃ© Å“uf, persistance",
                "breeder": "reproducteur, objectifs: fertilitÃ©, Ã©closabilitÃ©, viabilitÃ© descendance",
            },
            "phase": {
                "starter": "phase dÃ©marrage (0-10j), croissance critique, thermorÃ©gulation",
                "grower": "phase croissance (11-24j), dÃ©veloppement squelettique et musculaire",
                "finisher": "phase finition (25j+), optimisation du poids final et FCR",
                "laying": "phase ponte, maintien de la production et qualitÃ© Å“uf",
                "breeding": "phase reproduction, optimisation fertilitÃ© et Ã©closabilitÃ©",
            },
        }

        self.performance_metrics = {
            "weight": [
                "poids vif",
                "gain de poids",
                "homogÃ©nÃ©itÃ©",
                "courbe de croissance",
            ],
            "fcr": [
                "indice de consommation",
                "efficacitÃ© alimentaire",
                "coÃ»t alimentaire",
            ],
            "mortality": [
                "mortalitÃ©",
                "viabilitÃ©",
                "causes de mortalitÃ©",
                "prÃ©vention",
            ],
            "production": [
                "intensitÃ© de ponte",
                "pic de ponte",
                "persistance",
                "qualitÃ© Å“uf",
            ],
            "feed": ["consommation", "appÃ©tence", "digestibilitÃ©", "conversion"],
        }

    def get_entity_description(
        self, entity_type: str, entity_value: str
    ) -> Optional[str]:
        """
        RÃ©cupÃ¨re la description d'une entitÃ©

        Args:
            entity_type: Type d'entitÃ© (line, species, phase, etc.)
            entity_value: Valeur de l'entitÃ©

        Returns:
            Description ou None si non trouvÃ©e
        """
        entity_value_lower = entity_value.lower()
        return self.descriptions.get(entity_type, {}).get(entity_value_lower)

    def get_metric_keywords(self, metric: str) -> List[str]:
        """
        RÃ©cupÃ¨re les mots-clÃ©s associÃ©s Ã  une mÃ©trique

        Args:
            metric: Nom de la mÃ©trique

        Returns:
            Liste de mots-clÃ©s
        """
        return self.performance_metrics.get(metric, [])

    def get_all_metrics(self) -> Dict[str, List[str]]:
        """Retourne toutes les mÃ©triques de performance"""
        return self.performance_metrics.copy()


class EnhancedResponseGenerator:
    """
    GÃ©nÃ©rateur avec enrichissement d'entitÃ©s et cache externe + ton affirmatif expert
    Version 3.0: Charge les prompts depuis system_prompts.json + entity_descriptions.json
    """

    def __init__(
        self,
        client,
        cache_manager=None,
        language: str = "fr",
        prompts_path: Optional[str] = None,
        descriptions_path: Optional[str] = None,
    ):
        """
        Initialise le gÃ©nÃ©rateur de rÃ©ponses

        Args:
            client: Client OpenAI
            cache_manager: Gestionnaire de cache (optionnel)
            language: Langue par dÃ©faut
            prompts_path: Chemin custom vers system_prompts.json
            descriptions_path: Chemin custom vers entity_descriptions.json
        """
        self.client = client
        self.cache_manager = cache_manager
        self.language = language

        # Charger le gestionnaire de prompts centralisÃ©
        if PROMPTS_AVAILABLE:
            try:
                if prompts_path:
                    self.prompts_manager = get_prompts_manager(prompts_path)
                else:
                    self.prompts_manager = get_prompts_manager()
                logger.info(
                    "âœ… EnhancedResponseGenerator initialisÃ© avec system_prompts.json"
                )
            except Exception as e:
                logger.error(f"âŒ Erreur chargement prompts: {e}")
                self.prompts_manager = None
        else:
            self.prompts_manager = None
            logger.warning("âš ï¸ EnhancedResponseGenerator en mode fallback")

        # Charger le gestionnaire de descriptions d'entitÃ©s
        self.entity_descriptions = EntityDescriptionsManager(descriptions_path)

        # Garder compatibilitÃ© avec ENTITY_CONTEXTS de config
        if ENTITY_CONTEXTS:
            for entity_type, contexts in ENTITY_CONTEXTS.items():
                if entity_type not in self.entity_descriptions.descriptions:
                    self.entity_descriptions.descriptions[entity_type] = {}
                self.entity_descriptions.descriptions[entity_type].update(contexts)

    async def generate_response(
        self,
        query: str,
        context_docs: List[Document],
        conversation_context: str = "",
        language: Optional[str] = None,
        intent_result=None,
    ) -> str:
        """GÃ©nÃ¨re une rÃ©ponse enrichie avec cache externe + ton affirmatif expert"""

        lang = language or self.language

        # Protection contre les documents vides
        if not context_docs or len(context_docs) == 0:
            logger.warning("âš ï¸ GÃ©nÃ©rateur appelÃ© avec 0 documents - protection activÃ©e")

            if self.prompts_manager:
                error_msg = self.prompts_manager.get_error_message(
                    "insufficient_data", lang
                )
                if error_msg:
                    return error_msg

            return "Je n'ai pas trouvÃ© d'informations pertinentes dans ma base de connaissances pour rÃ©pondre Ã  votre question. Pouvez-vous reformuler ou Ãªtre plus spÃ©cifique ?"

        try:
            # VÃ©rifier le cache externe
            if self.cache_manager and self.cache_manager.enabled:
                context_hash = self.cache_manager.generate_context_hash(
                    [self._doc_to_dict(doc) for doc in context_docs]
                )
                cached_response = await self.cache_manager.get_response(
                    query, context_hash, lang
                )
                if cached_response:
                    METRICS.cache_hit("response")
                    if hasattr(self.cache_manager, "get_last_cache_details"):
                        try:
                            cache_hit_details = (
                                await self.cache_manager.get_last_cache_details()
                            )
                            if cache_hit_details.get("semantic_fallback_used"):
                                METRICS.semantic_fallback_used()
                            else:
                                METRICS.semantic_cache_hit("exact")
                        except Exception:
                            pass
                    return cached_response
                METRICS.cache_miss("response")

            # Construire enrichissement avancÃ©
            enrichment = (
                self._build_entity_enrichment(intent_result)
                if intent_result
                else ContextEnrichment("", "", "", "", [], [])
            )

            # GÃ©nÃ©rer le prompt enrichi
            system_prompt, user_prompt = self._build_enhanced_prompt(
                query, context_docs, enrichment, conversation_context, lang
            )

            # GÃ©nÃ©ration
            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.1,
                max_tokens=900,
            )

            generated_response = response.choices[0].message.content.strip()

            # Post-traitement
            enhanced_response = self._post_process_response(
                generated_response,
                enrichment,
                [self._doc_to_dict(doc) for doc in context_docs],
            )

            # Mettre en cache
            if self.cache_manager and self.cache_manager.enabled:
                await self.cache_manager.set_response(
                    query, context_hash, enhanced_response, lang
                )

            return enhanced_response

        except Exception as e:
            logger.error(f"Erreur gÃ©nÃ©ration rÃ©ponse enrichie: {e}")
            return "DÃ©solÃ©, je ne peux pas gÃ©nÃ©rer une rÃ©ponse pour cette question."

    def _doc_to_dict(self, doc: Document) -> dict:
        """Convertit Document en dict pour cache"""
        result = {
            "content": doc.content,
            "title": doc.metadata.get("title", ""),
            "source": doc.metadata.get("source", ""),
            "score": doc.score,
            "genetic_line": doc.metadata.get(
                "geneticLine", doc.metadata.get("genetic_line", "")
            ),
            "species": doc.metadata.get("species", ""),
        }
        if doc.explain_score:
            result["explain_score"] = doc.explain_score
        return result

    def _build_entity_enrichment(self, intent_result) -> ContextEnrichment:
        """Construit l'enrichissement basÃ© sur les entitÃ©s dÃ©tectÃ©es"""
        try:
            entities = getattr(intent_result, "detected_entities", {})

            # Contexte des entitÃ©s via EntityDescriptionsManager
            entity_contexts = []

            if "line" in entities:
                description = self.entity_descriptions.get_entity_description(
                    "line", entities["line"]
                )
                if description:
                    entity_contexts.append(f"LignÃ©e {entities['line']}: {description}")

            if "species" in entities:
                description = self.entity_descriptions.get_entity_description(
                    "species", entities["species"]
                )
                if description:
                    entity_contexts.append(f"Type {entities['species']}: {description}")

            if "phase" in entities:
                description = self.entity_descriptions.get_entity_description(
                    "phase", entities["phase"]
                )
                if description:
                    entity_contexts.append(f"Phase {entities['phase']}: {description}")

            # Focus mÃ©trique
            metric_focus = ""
            detected_metrics = []
            expanded_query = getattr(intent_result, "expanded_query", "")

            all_metrics = self.entity_descriptions.get_all_metrics()
            for metric, keywords in all_metrics.items():
                metric_in_entities = metric in entities
                metric_in_query = (
                    any(kw in expanded_query.lower() for kw in keywords)
                    if expanded_query
                    else False
                )

                if metric_in_entities or metric_in_query:
                    detected_metrics.extend(keywords)

            if detected_metrics:
                metric_focus = f"Focus mÃ©triques: {', '.join(detected_metrics[:3])}"

            # Contexte temporel
            temporal_context = ""
            if "age_days" in entities:
                age = entities["age_days"]
                if isinstance(age, (int, float)):
                    if age <= 7:
                        temporal_context = "PÃ©riode critique premiÃ¨re semaine - Focus thermorÃ©gulation et dÃ©marrage"
                    elif age <= 21:
                        temporal_context = "Phase de croissance rapide - DÃ©veloppement osseux et musculaire"
                    elif age <= 35:
                        temporal_context = (
                            "Phase d'optimisation - Maximisation du gain de poids"
                        )
                    else:
                        temporal_context = (
                            "Phase de finition - Optimisation FCR et qualitÃ© carcasse"
                        )

            # Focus espÃ¨ce
            species_focus = ""
            if "species" in entities:
                species = entities["species"].lower()
                if "broiler" in species or "chair" in species:
                    species_focus = (
                        "Objectifs chair: poids vif, FCR, rendement, qualitÃ© carcasse"
                    )
                elif "layer" in species or "ponte" in species:
                    species_focus = "Objectifs ponte: intensitÃ©, persistance, qualitÃ© Å“uf, viabilitÃ©"

            # Indicateurs de performance
            performance_indicators = []
            if "weight" in entities or (
                "poids" in expanded_query.lower() if expanded_query else False
            ):
                performance_indicators.extend(
                    ["poids vif", "gain quotidien", "homogÃ©nÃ©itÃ© du lot"]
                )
            if "fcr" in entities or any(
                term in expanded_query.lower() if expanded_query else False
                for term in ["conversion", "indice"]
            ):
                performance_indicators.extend(
                    ["FCR", "consommation", "efficacitÃ© alimentaire"]
                )

            # Ã‰lÃ©ments de confiance
            confidence_boosters = []
            if entity_contexts:
                confidence_boosters.append("Contexte lignÃ©e/espÃ¨ce identifiÃ©")
            if temporal_context:
                confidence_boosters.append("Phase d'Ã©levage prÃ©cisÃ©e")
            if metric_focus:
                confidence_boosters.append("MÃ©triques cibles identifiÃ©es")

            return ContextEnrichment(
                entity_context="; ".join(entity_contexts),
                metric_focus=metric_focus,
                temporal_context=temporal_context,
                species_focus=species_focus,
                performance_indicators=performance_indicators,
                confidence_boosters=confidence_boosters,
            )

        except Exception as e:
            logger.warning(f"Erreur construction enrichissement: {e}")
            return ContextEnrichment("", "", "", "", [], [])

    def _build_enhanced_prompt(
        self,
        query: str,
        context_docs: List[Document],
        enrichment: ContextEnrichment,
        conversation_context: str,
        language: str,
    ) -> Tuple[str, str]:
        """Construit un prompt enrichi avec ton affirmatif expert"""

        # Contexte documentaire
        context_text = "\n\n".join(
            [
                f"Document {i+1} ({doc.metadata.get('geneticLine', 'N/A')} - {doc.metadata.get('species', 'N/A')}):\n{doc.content[:1000]}"
                for i, doc in enumerate(context_docs[:5])
            ]
        )

        # Construction du prompt systÃ¨me
        if self.prompts_manager:
            expert_identity = self.prompts_manager.get_base_prompt(
                "expert_identity", language
            )
            response_guidelines = self.prompts_manager.get_base_prompt(
                "response_guidelines", language
            )

            system_prompt_parts = []

            if expert_identity:
                system_prompt_parts.append(expert_identity)

            context_section = f"""
CONTEXTE MÃ‰TIER DÃ‰TECTÃ‰:
{enrichment.entity_context}
{enrichment.species_focus}
{enrichment.temporal_context}
{enrichment.metric_focus}
"""
            system_prompt_parts.append(context_section)

            if response_guidelines:
                system_prompt_parts.append(response_guidelines)

            metrics_section = f"""
MÃ‰TRIQUES PRIORITAIRES:
{', '.join(enrichment.performance_indicators[:3]) if enrichment.performance_indicators else 'ParamÃ¨tres gÃ©nÃ©raux de production'}
"""
            system_prompt_parts.append(metrics_section)

            critical_instructions = f"""
INSTRUCTIONS CRITIQUES:
- NE commence JAMAIS par un titre (ex: "## Maladie", "**Maladie**") - commence directement par la phrase d'introduction
- Examine les tableaux de donnÃ©es pour extraire les informations prÃ©cises
- PrÃ©sente 2-3 Ã©lÃ©ments principaux, pas plus
- Utilise un ton affirmatif mais sobre, sans formatage excessif
- NE conclus PAS avec des recommandations pratiques sauf si explicitement demandÃ©

LANGUE: RÃ©ponds STRICTEMENT en {language}
"""
            system_prompt_parts.append(critical_instructions)

            system_prompt = "\n\n".join(system_prompt_parts)

        else:
            system_prompt = self._get_fallback_system_prompt(enrichment, language)

        # Prompt utilisateur
        limited_context = (
            conversation_context[:MAX_CONVERSATION_CONTEXT]
            if conversation_context
            else ""
        )

        user_prompt = f"""CONTEXTE CONVERSATIONNEL:
{limited_context}

INFORMATIONS TECHNIQUES DISPONIBLES:
{context_text}

ENRICHISSEMENT DÃ‰TECTÃ‰:
- EntitÃ©s mÃ©tier: {enrichment.entity_context or 'Non spÃ©cifiÃ©es'}
- Focus performance: {', '.join(enrichment.performance_indicators) if enrichment.performance_indicators else 'GÃ©nÃ©ral'}
- Contexte temporel: {enrichment.temporal_context or 'Non spÃ©cifiÃ©'}

QUESTION:
{query}

RÃ‰PONSE EXPERTE (affirmative, structurÃ©e, sans mention de sources):"""

        return system_prompt, user_prompt

    def _get_fallback_system_prompt(
        self, enrichment: ContextEnrichment, language: str
    ) -> str:
        """Prompt systÃ¨me de secours"""
        return f"""Tu es un expert avicole reconnu avec une expertise approfondie en production avicole.

CONTEXTE MÃ‰TIER DÃ‰TECTÃ‰:
{enrichment.entity_context}
{enrichment.species_focus}
{enrichment.temporal_context}
{enrichment.metric_focus}

DIRECTIVES DE RÃ‰PONSE - STYLE EXPERT Ã‰QUILIBRÃ‰:

1. **Introduction directe** : Commence DIRECTEMENT par une phrase claire qui rÃ©pond Ã  la question
2. **Ne jamais mentionner les sources** : Ne fais JAMAIS rÃ©fÃ©rence aux "documents", "sources", "selon les donnÃ©es fournies"
3. **Structure sobre** : Utilise des titres en gras (**Titre**) uniquement pour les sous-sections
4. **Concision** : PrÃ©sente 2-3 points principaux maximum
5. **DonnÃ©es prÃ©cises** : Fournis des valeurs chiffrÃ©es quand pertinent

MÃ‰TRIQUES PRIORITAIRES:
{', '.join(enrichment.performance_indicators[:3]) if enrichment.performance_indicators else 'ParamÃ¨tres gÃ©nÃ©raux de production'}

LANGUE: RÃ©ponds STRICTEMENT en {language}"""

    def _post_process_response(
        self, response: str, enrichment: ContextEnrichment, context_docs: List[Dict]
    ) -> str:
        """Post-traitement minimaliste"""
        return response.strip()


# Factory function
def create_enhanced_generator(
    openai_client,
    cache_manager=None,
    language: str = "fr",
    prompts_path: Optional[str] = None,
    descriptions_path: Optional[str] = None,
):
    """
    Factory pour crÃ©er le gÃ©nÃ©rateur enrichi

    Args:
        openai_client: Client OpenAI
        cache_manager: Gestionnaire de cache (optionnel)
        language: Langue par dÃ©faut
        prompts_path: Chemin custom vers system_prompts.json
        descriptions_path: Chemin custom vers entity_descriptions.json

    Returns:
        Instance EnhancedResponseGenerator
    """
    return EnhancedResponseGenerator(
        openai_client, cache_manager, language, prompts_path, descriptions_path
    )


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(levelname)s - %(message)s")

    print("=" * 70)
    print("ğŸ§ª TESTS ENHANCED RESPONSE GENERATOR v3.0")
    print("=" * 70)

    # Test 1: EntityDescriptionsManager
    print("\nğŸ”¥ Test 1: EntityDescriptionsManager")
    try:
        desc_manager = EntityDescriptionsManager()
        print("  âœ… Manager crÃ©Ã©")

        # Test rÃ©cupÃ©ration description
        ross_desc = desc_manager.get_entity_description("line", "Ross")
        print(f"  ğŸ“Š Ross description: {ross_desc[:50]}...")

        # Test mÃ©triques
        weight_keywords = desc_manager.get_metric_keywords("weight")
        print(f"  ğŸ“Š Weight keywords: {weight_keywords}")
    except Exception as e:
        print(f"  âŒ Erreur: {e}")

    # Test 2: EnhancedResponseGenerator
    print("\nğŸ¯ Test 2: EnhancedResponseGenerator")
    try:

        class MockClient:
            pass

        generator = EnhancedResponseGenerator(MockClient(), language="fr")
        print("  âœ… GÃ©nÃ©rateur crÃ©Ã©")
        print(
            f"  ğŸ“Š Entity descriptions loaded: {len(generator.entity_descriptions.descriptions)}"
        )
    except Exception as e:
        print(f"  âŒ Erreur: {e}")

    # Test 3: Enrichissement
    print("\nğŸ”¬ Test 3: Construction enrichissement")

    class MockIntentResult:
        detected_entities = {"line": "Ross 308", "species": "broiler", "age_days": 35}
        expanded_query = "poids vif conversion alimentaire"

    enrichment = generator._build_entity_enrichment(MockIntentResult())
    print("  âœ… Enrichissement crÃ©Ã©")
    print(f"  ğŸ“Š Entity context: {enrichment.entity_context[:80]}...")
    print(f"  ğŸ“Š MÃ©triques: {enrichment.performance_indicators}")

    print("\n" + "=" * 70)
    print("âœ… TESTS TERMINÃ‰S")
    print("=" * 70)
