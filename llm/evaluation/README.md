# RAGAS Evaluation - Intelia Expert

Ce r√©pertoire contient le syst√®me d'√©valuation automatique du RAG utilisant RAGAS (Retrieval-Augmented Generation Assessment).

## üéØ Objectif

Mesurer automatiquement la qualit√© du syst√®me RAG sur 4 dimensions :
- **Context Precision** : Pertinence des documents r√©cup√©r√©s
- **Context Recall** : Couverture du contexte vs ground truth
- **Faithfulness** : Fid√©lit√© aux sources (pas d'hallucination)
- **Answer Relevancy** : Pertinence de la r√©ponse √† la question

## üìÅ Structure

```
evaluation/
‚îú‚îÄ‚îÄ ragas_evaluator.py          # Classe principale RAGAS
‚îú‚îÄ‚îÄ golden_dataset_intelia.py   # Dataset de test (28 questions)
‚îî‚îÄ‚îÄ README.md                    # Ce fichier

scripts/
‚îú‚îÄ‚îÄ run_ragas_evaluation.py     # Script d'ex√©cution principal
‚îî‚îÄ‚îÄ evaluate.sh                  # Helper bash pour ex√©cution facile
```

## üöÄ Utilisation (Digital Ocean)

### Pr√©requis

```bash
# 1. Installer d√©pendances (si pas d√©j√† fait)
pip install ragas datasets langchain-openai

# 2. V√©rifier OPENAI_API_KEY
echo $OPENAI_API_KEY  # Doit afficher votre cl√©
```

### Ex√©cution rapide

```bash
# Se placer dans le bon r√©pertoire
cd /app/llm

# Rendre le script ex√©cutable (premi√®re fois seulement)
chmod +x scripts/evaluate.sh

# Test rapide (3 questions, ~30s, ~$0.01)
./scripts/evaluate.sh test

# √âvaluation rapide (5 questions, ~1 min, ~$0.05)
./scripts/evaluate.sh quick

# √âvaluation compl√®te (28 questions, ~5 min, ~$0.50)
./scripts/evaluate.sh full
```

### Ex√©cution avanc√©e

```bash
# Sp√©cifier nombre de questions
python3 scripts/run_ragas_evaluation.py --test-cases 10

# Utiliser GPT-4 au lieu de GPT-4o-mini (plus pr√©cis mais cher)
python3 scripts/run_ragas_evaluation.py --llm gpt-4

# Sp√©cifier fichier de sortie
python3 scripts/run_ragas_evaluation.py --output results/eval_v2.json

# Mode simulation (ne pas appeler le RAG r√©el, pour tests)
python3 scripts/run_ragas_evaluation.py --simulate
```

## üìä Dataset de test (28 questions)

Le dataset couvre tous les aspects critiques :

| Cat√©gorie | Questions | Description |
|-----------|-----------|-------------|
| **Calculs** | 5 | Feed calculation, reverse lookup, projection, flock |
| **Diagnostics** | 3 | Sous-performance, FCR √©lev√©, stress thermique |
| **Nutrition** | 2 | Sp√©cifications Cobb 500, concepts starter/grower |
| **Environnement** | 2 | Temp√©rature, validation param√®tres |
| **Comparatifs** | 3 | Multi-√¢ges, subjectif (Ross vs Cobb) |
| **Multi-langue** | 2 | Fran√ßais, anglais, tha√Ø (non support√©) |
| **Conversationnel** | 4 | Follow-up avec m√©moire contextuelle |
| **Validation** | 3 | √Çge hors limites, questions incompl√®tes |
| **Out-of-domain** | 1 | Cryptomonnaie (doit √™tre rejet√©) |
| **M√©triques simples** | 3 | Poids, FCR, multi-m√©triques |

**Exemples de questions :**

```python
# Calcul complexe
"Je suis rendu au 18e jour et j'√©l√®ve du Ross 308 m√¢le.
 De combien de moul√©es pour atteindre 2,4 kg ?"

# Diagnostic
"Mon troupeau Ross 308 √† 28j p√®se 1300g au lieu de 1550g, c'est grave ?"

# Out-of-domain
"Qu'est-ce que la cryptomonnaie ?"  # Doit √™tre rejet√©

# Conversationnel (2 tours)
Tour 1: "Quel est le poids Ross 308 m√¢le √† 21 jours ?"
Tour 2: "Et √† 28 jours ?"  # Doit garder contexte
```

## üìà Interpr√©tation des scores

### Scores globaux

| Score | Interpr√©tation | Action |
|-------|----------------|--------|
| **‚â• 90%** | üèÜ Excellent | Syst√®me tr√®s performant |
| **80-90%** | ‚úÖ Tr√®s bon | Qualit√© √©lev√©e, am√©liorations mineures |
| **70-80%** | ‚ö†Ô∏è Bon | Am√©liorations recommand√©es |
| **< 70%** | ‚ùå Insuffisant | Optimisation requise |

### M√©triques individuelles

**Context Precision < 80% ?**
- Am√©liorer reranking (Cohere)
- Optimiser seuils de similarit√©
- Affiner recherche hybride (RRF)

**Context Recall < 80% ?**
- Augmenter top_k retrieval
- Enrichir base de connaissances
- Am√©liorer embeddings (fine-tuning)

**Faithfulness < 80% ?**
- Renforcer guardrails (hallucination detection)
- Ajuster prompts syst√®me
- R√©duire temp√©rature LLM

**Answer Relevancy < 80% ?**
- Optimiser g√©n√©ration de r√©ponse
- Affiner prompts contextuels
- Am√©liorer compr√©hension d'intention

## üìÅ R√©sultats g√©n√©r√©s

### Fichier JSON (`logs/ragas_evaluation_{timestamp}.json`)

```json
{
  "scores": {
    "context_precision": 0.892,
    "context_recall": 0.854,
    "faithfulness": 0.913,
    "answer_relevancy": 0.841,
    "overall": 0.875
  },
  "detailed_scores": [
    {
      "question": "Je suis rendu au 18e jour...",
      "answer": "Pour atteindre un poids cible de 2400g...",
      "context_precision": 0.95,
      "context_recall": 0.88,
      "faithfulness": 0.92,
      "answer_relevancy": 0.87
    }
  ],
  "timestamp": "2025-10-07T14:45:00",
  "llm_model": "gpt-4o-mini",
  "num_test_cases": 28,
  "duration_seconds": 297.3
}
```

### Voir les r√©sultats

```bash
# Lister les √©valuations r√©centes
ls -lht logs/ragas_evaluation_* | head -5

# Voir le r√©sum√© d'une √©valuation
cat logs/ragas_evaluation_20251007_144500.json | jq '.scores'

# Voir les questions qui ont √©chou√© (score < 0.7)
cat logs/ragas_evaluation_*.json | jq '.detailed_scores[] | select(.answer_relevancy < 0.7) | .question'
```

## üí∞ Co√ªts estim√©s

| Mode | Questions | Dur√©e | Co√ªt (GPT-4o-mini) |
|------|-----------|-------|-------------------|
| **test** | 3 | ~30s | ~$0.01 |
| **quick** | 5 | ~1 min | ~$0.05 |
| **full** | 28 | ~5 min | ~$0.40-0.60 |

**Budget mensuel recommand√© :**
- Tests apr√®s changements : ~$2-3/mois (5-10 √©valuations)
- Monitoring hebdomadaire : ~$2-3/mois (4 √©valuations √ó $0.60)
- **Total : ~$5-6/mois**

## üîß Ajouter de nouvelles questions

√âditer `evaluation/golden_dataset_intelia.py` :

```python
{
    "question": "Votre nouvelle question ici",
    "ground_truth": "R√©ponse attendue (valid√©e manuellement)",
    "category": "nutrition",  # ou calculation, diagnostic, etc.
    "expected_behavior": "Description du comportement attendu",
    "contexts": [],  # Rempli automatiquement
    "answer": ""     # Rempli automatiquement
},
```

## üìù Maintenance

### Quand r√©-√©valuer ?

‚úÖ **Apr√®s changements importants :**
- Modifications du query router
- Nouveaux handlers (calculation, comparative)
- Changements dans les prompts syst√®me
- Mise √† jour des embeddings
- Ajout de nouvelles donn√©es

‚úÖ **P√©riodiquement :**
- 1x/semaine : Quick evaluation (5 questions)
- 1x/mois : Full evaluation (28 questions)
- Avant d√©ploiement production

### Enrichir le dataset

Le dataset doit √©voluer avec le syst√®me :
- Ajouter questions bas√©es sur feedback utilisateurs
- Couvrir nouveaux features (pondeuses, vaccination)
- Cas d'edge d√©couverts en production
- Objectif : 50-100 questions √† terme

## üêõ Troubleshooting

**Erreur "OPENAI_API_KEY non configur√©e" :**
```bash
export OPENAI_API_KEY='sk-...'
# Ou ajouter dans .env
```

**Erreur "ragas not installed" :**
```bash
pip install ragas datasets langchain-openai
```

**Timeout lors de l'√©valuation :**
```bash
# R√©duire nombre de questions
./scripts/evaluate.sh test  # Au lieu de full
```

**Scores tr√®s bas (< 50%) :**
- V√©rifier que le RAG Engine est bien initialis√©
- V√©rifier connexion PostgreSQL/Weaviate
- Tester en mode `--simulate` pour isoler le probl√®me

## üìö R√©f√©rences

- [RAGAS Documentation](https://docs.ragas.io/en/stable/)
- [RAGAS GitHub](https://github.com/explodinggradients/ragas)
- [Paper: RAGAS Framework](https://arxiv.org/abs/2309.15217)

---

**Questions ?** Voir les logs d√©taill√©s dans `logs/ragas_evaluation_*.log`
