# -*- coding: utf-8 -*-
"""
ragas_evaluator.py - RAGAS-based evaluation for Intelia Expert LLM
Version: 1.4.1
Last modified: 2025-10-26
"""
"""
ragas_evaluator.py - RAGAS-based evaluation for Intelia Expert LLM

Implements comprehensive RAG evaluation using the RAGAS framework.
Metrics: Context Precision, Context Recall, Faithfulness, Answer Relevancy

Documentation: https://docs.ragas.io/en/stable/
"""

import logging
import os
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
import json
import numpy as np

# RAGAS imports
try:
    from ragas import evaluate
    from ragas.metrics import (
        context_precision,
        context_recall,
        faithfulness,
        answer_relevancy,
    )
    from datasets import Dataset, Features, Value, Sequence

    RAGAS_AVAILABLE = True
except ImportError:
    RAGAS_AVAILABLE = False
    logging.warning("‚ö†Ô∏è RAGAS not installed. Run: pip install ragas datasets")

# LangChain imports for LLM configuration
try:
    from langchain_openai import ChatOpenAI

    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    logging.warning(
        "‚ö†Ô∏è LangChain OpenAI not installed. Run: pip install langchain-openai"
    )

logger = logging.getLogger(__name__)


def convert_numpy_to_python(obj):
    """
    Convertit les types numpy en types Python natifs pour JSON serialization.

    Args:
        obj: Objet √† convertir (peut √™tre dict, list, ndarray, etc.)

    Returns:
        Objet converti en types Python natifs
    """
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.generic):
        return obj.item()
    elif isinstance(obj, dict):
        return {key: convert_numpy_to_python(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_to_python(item) for item in obj]
    else:
        return obj


class RAGASEvaluator:
    """
    √âvaluateur RAGAS pour syst√®me RAG Intelia Expert.

    Mesure 4 m√©triques principales:
    - Context Precision: Pertinence des documents r√©cup√©r√©s
    - Context Recall: Couverture du contexte par rapport √† la ground truth
    - Faithfulness: Fid√©lit√© de la r√©ponse au contexte fourni
    - Answer Relevancy: Pertinence de la r√©ponse √† la question
    """

    def __init__(
        self,
        llm_model: str = "gpt-4o",
        temperature: float = 0.0,
        openai_api_key: Optional[str] = None,
    ):
        """
        Initialise l'√©valuateur RAGAS.

        Args:
            llm_model: Mod√®le LLM pour √©valuation (d√©faut: gpt-4o)
            temperature: Temp√©rature pour g√©n√©ration (d√©faut: 0.0 pour d√©terminisme)
            openai_api_key: Cl√© API OpenAI (optionnel, prend OPENAI_API_KEY si None)
        """
        if not RAGAS_AVAILABLE:
            raise ImportError(
                "RAGAS non install√©. Ex√©cutez: pip install ragas datasets"
            )

        if not LANGCHAIN_AVAILABLE:
            raise ImportError(
                "LangChain OpenAI non install√©. Ex√©cutez: pip install langchain-openai"
            )

        self.llm_model = llm_model
        self.temperature = temperature
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")

        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY non configur√©e")

        # Initialiser LLM pour √©valuation
        self.evaluator_llm = ChatOpenAI(
            model=llm_model, temperature=temperature, api_key=self.openai_api_key
        )

        # M√©triques RAGAS
        self.metrics = [
            context_precision,
            context_recall,
            faithfulness,
            answer_relevancy,
        ]

        logger.info(f"‚úÖ RAGASEvaluator initialis√© avec {llm_model}")

    def prepare_evaluation_dataset(self, test_cases: List[Dict[str, Any]]) -> Dataset:
        """
        Pr√©pare un dataset RAGAS √† partir de cas de test.

        Args:
            test_cases: Liste de dictionnaires avec:
                - question: Question pos√©e (str)
                - answer: R√©ponse g√©n√©r√©e par le RAG (str)
                - contexts: Liste de contextes r√©cup√©r√©s (List[str])
                - ground_truth: R√©ponse attendue (str, optionnel)

        Returns:
            Dataset RAGAS format√©

        Example:
            >>> test_cases = [{
            ...     "question": "Quel est le poids cible Ross 308 √† 35j?",
            ...     "answer": "Le poids cible est de 2350g pour m√¢les...",
            ...     "contexts": ["Ross 308: poids 35j m√¢les 2350g...", ...],
            ...     "ground_truth": "2350g pour m√¢les, 2100g pour femelles"
            ... }]
            >>> dataset = evaluator.prepare_evaluation_dataset(test_cases)
        """
        # Valider structure
        required_keys = {"question", "answer", "contexts"}
        for i, case in enumerate(test_cases):
            missing = required_keys - set(case.keys())
            if missing:
                raise ValueError(f"Cas de test {i} manque cl√©s: {missing}")

            # V√©rifier types
            if not isinstance(case["question"], str):
                raise TypeError(f"Cas {i}: 'question' doit √™tre str")
            if not isinstance(case["answer"], str):
                raise TypeError(f"Cas {i}: 'answer' doit √™tre str")
            if not isinstance(case["contexts"], list):
                raise TypeError(f"Cas {i}: 'contexts' doit √™tre List[str]")
            if case.get("ground_truth") and not isinstance(case["ground_truth"], str):
                raise TypeError(f"Cas {i}: 'ground_truth' doit √™tre str")

        # Convertir en format RAGAS
        ragas_data = {
            "question": [case["question"] for case in test_cases],
            "answer": [case["answer"] for case in test_cases],
            "contexts": [case["contexts"] for case in test_cases],
        }

        # Ajouter ground_truth si disponible
        if any("ground_truth" in case for case in test_cases):
            ragas_data["ground_truth"] = [
                case.get("ground_truth", "") for case in test_cases
            ]

        # D√©finir features explicitement pour RAGAS 0.1.1
        features = Features({
            "question": Value("string"),
            "answer": Value("string"),
            "contexts": Sequence(Value("string")),
        })

        if "ground_truth" in ragas_data:
            features["ground_truth"] = Value("string")

        dataset = Dataset.from_dict(ragas_data, features=features)

        logger.info(f"üìä Dataset RAGAS cr√©√©: {len(test_cases)} cas de test")

        return dataset

    async def evaluate_async(self, test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        √âvalue le syst√®me RAG de mani√®re asynchrone.

        Args:
            test_cases: Liste de cas de test (voir prepare_evaluation_dataset)

        Returns:
            Dictionnaire avec:
                - scores: Scores par m√©trique (moyenne)
                - detailed_scores: Scores par cas de test
                - summary: R√©sum√© textuel
                - timestamp: Date d'√©valuation

        Example:
            >>> results = await evaluator.evaluate_async(test_cases)
            >>> print(f"Faithfulness: {results['scores']['faithfulness']:.2%}")
        """
        # Pr√©parer dataset
        dataset = self.prepare_evaluation_dataset(test_cases)

        logger.info(f"üîç D√©but √©valuation RAGAS ({len(test_cases)} cas)...")
        start_time = datetime.now()

        try:
            # √âvaluer avec RAGAS (sync, wrapper async)
            result = await asyncio.to_thread(
                evaluate, dataset=dataset, metrics=self.metrics, llm=self.evaluator_llm
            )

            # Extraire scores et convertir numpy en Python natif
            # result est un EvaluationResult object, convertir en dict via to_pandas()
            result_df = result.to_pandas()
            scores = {
                "context_precision": float(result_df["context_precision"].mean()),
                "context_recall": float(result_df["context_recall"].mean()),
                "faithfulness": float(result_df["faithfulness"].mean()),
                "answer_relevancy": float(result_df["answer_relevancy"].mean()),
            }

            # Score global (moyenne)
            overall_score = sum(scores.values()) / len(scores)
            scores["overall"] = float(overall_score)

            # D√©tails par cas de test (convertir numpy arrays)
            detailed_scores = []
            if hasattr(result, "to_pandas"):
                detailed_scores_raw = result.to_pandas().to_dict("records")
                detailed_scores = convert_numpy_to_python(detailed_scores_raw)

            # R√©sum√©
            duration = (datetime.now() - start_time).total_seconds()
            summary = self._generate_summary(scores, len(test_cases), duration)

            evaluation_result = {
                "scores": scores,
                "detailed_scores": detailed_scores,
                "summary": summary,
                "timestamp": datetime.now().isoformat(),
                "llm_model": self.llm_model,
                "num_test_cases": len(test_cases),
                "duration_seconds": float(duration),
            }

            logger.info(f"‚úÖ √âvaluation termin√©e: Overall={overall_score:.2%}")

            return evaluation_result

        except Exception as e:
            logger.error(f"‚ùå Erreur √©valuation RAGAS: {e}")
            raise

    def evaluate(self, test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        √âvalue le syst√®me RAG de mani√®re synchrone (wrapper).

        Args:
            test_cases: Liste de cas de test

        Returns:
            R√©sultats d'√©valuation (voir evaluate_async)
        """
        return asyncio.run(self.evaluate_async(test_cases))

    def _generate_summary(
        self, scores: Dict[str, float], num_cases: int, duration: float
    ) -> str:
        """
        G√©n√®re un r√©sum√© textuel de l'√©valuation.

        Args:
            scores: Scores par m√©trique
            num_cases: Nombre de cas de test
            duration: Dur√©e en secondes

        Returns:
            R√©sum√© format√©
        """
        summary = f"""
=================================================================
üìä RAGAS EVALUATION REPORT - Intelia Expert LLM
=================================================================

Test Cases:        {num_cases}
LLM Model:         {self.llm_model}
Duration:          {duration:.1f}s ({duration/60:.1f} min)
Timestamp:         {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

-----------------------------------------------------------------
SCORES
-----------------------------------------------------------------
Overall Score:          {scores['overall']:.2%}

Context Precision:      {scores['context_precision']:.2%}
  ‚Üí Pertinence des documents r√©cup√©r√©s

Context Recall:         {scores['context_recall']:.2%}
  ‚Üí Couverture du contexte par rapport √† ground truth

Faithfulness:           {scores['faithfulness']:.2%}
  ‚Üí Fid√©lit√© de la r√©ponse au contexte

Answer Relevancy:       {scores['answer_relevancy']:.2%}
  ‚Üí Pertinence de la r√©ponse √† la question

-----------------------------------------------------------------
INTERPRETATION
-----------------------------------------------------------------
"""

        # Interpr√©tation globale
        overall = scores["overall"]
        if overall >= 0.90:
            summary += "üèÜ Excellent: Syst√®me RAG performant (>90%)\n"
        elif overall >= 0.80:
            summary += "‚úÖ Tr√®s Bon: Qualit√© √©lev√©e (80-90%)\n"
        elif overall >= 0.70:
            summary += "‚ö†Ô∏è Bon: Am√©liorations possibles (70-80%)\n"
        else:
            summary += "‚ùå Insuffisant: Optimisation requise (<70%)\n"

        # Recommandations par m√©trique
        if scores["context_precision"] < 0.80:
            summary += "\nüí° Am√©liorer Context Precision:\n"
            summary += "   - Affiner le reranking (Cohere)\n"
            summary += "   - Optimiser les seuils de similarit√©\n"
            summary += "   - Am√©liorer la recherche hybride (RRF)\n"

        if scores["context_recall"] < 0.80:
            summary += "\nüí° Am√©liorer Context Recall:\n"
            summary += "   - Augmenter top_k retrieval\n"
            summary += "   - Enrichir la base de connaissances\n"
            summary += "   - Am√©liorer les embeddings (fine-tuning)\n"

        if scores["faithfulness"] < 0.80:
            summary += "\nüí° Am√©liorer Faithfulness:\n"
            summary += "   - Renforcer les guardrails (hallucination detection)\n"
            summary += "   - Ajuster les prompts syst√®me\n"
            summary += "   - R√©duire la temp√©rature du LLM\n"

        if scores["answer_relevancy"] < 0.80:
            summary += "\nüí° Am√©liorer Answer Relevancy:\n"
            summary += "   - Optimiser la g√©n√©ration de r√©ponse\n"
            summary += "   - Affiner les prompts contextuels\n"
            summary += "   - Am√©liorer la compr√©hension d'intention\n"

        summary += (
            "\n=================================================================\n"
        )

        return summary

    def save_results(self, results: Dict[str, Any], output_path: str):
        """
        Sauvegarde les r√©sultats d'√©valuation.

        Args:
            results: R√©sultats d'√©valuation (de evaluate_async)
            output_path: Chemin du fichier de sortie (.json)
        """
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(results, f, indent=2, ensure_ascii=False)

            logger.info(f"üíæ R√©sultats sauvegard√©s: {output_path}")

        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde r√©sultats: {e}")
            raise


# ============================================================================
# UTILITY: Generate Golden Dataset for Intelia Expert
# ============================================================================


def generate_poultry_golden_dataset() -> List[Dict[str, Any]]:
    """
    G√©n√®re un dataset golden de cas de test pour l'√©valuation.

    Contient des questions avicoles r√©elles avec r√©ponses attendues.

    Returns:
        Liste de cas de test format√©s pour RAGAS

    Note:
        Ce dataset doit √™tre maintenu et enrichi au fil du temps.
        Les ground_truth doivent √™tre valid√©es par des experts avicoles.
    """
    return [
        # Performance Standards - Ross 308
        {
            "question": "Quel est le poids cible pour des m√¢les Ross 308 √† 35 jours?",
            "ground_truth": "Le poids cible pour des m√¢les Ross 308 √† 35 jours est de 2350g selon les standards de performance 2024.",
            "contexts": [],  # √Ä remplir dynamiquement lors de l'√©valuation
            "answer": "",  # √Ä remplir par le RAG
        },
        {
            "question": "Quel FCR est attendu pour des Ross 308 mixte √† 42 jours?",
            "ground_truth": "Le FCR (Feed Conversion Ratio) attendu pour Ross 308 mixte √† 42 jours est de 1.65-1.70.",
            "contexts": [],
            "answer": "",
        },
        # Performance Standards - Cobb 500
        {
            "question": "Quel est le gain de poids journalier pour des m√¢les Cobb 500 √† 28 jours?",
            "ground_truth": "Le gain de poids journalier pour des m√¢les Cobb 500 √† 28 jours est d'environ 60-65g/jour.",
            "contexts": [],
            "answer": "",
        },
        # Nutrition
        {
            "question": "Quel taux de prot√©ine est requis dans l'aliment starter pour Ross 308 de 0-10 jours?",
            "ground_truth": "Le taux de prot√©ine requis dans l'aliment starter pour Ross 308 de 0-10 jours est de 22-23%.",
            "contexts": [],
            "answer": "",
        },
        {
            "question": "Quelle densit√© √©nerg√©tique est recommand√©e pour aliment grower Cobb 500?",
            "ground_truth": "La densit√© √©nerg√©tique recommand√©e pour l'aliment grower Cobb 500 est de 3100-3200 kcal/kg.",
            "contexts": [],
            "answer": "",
        },
        # Environment
        {
            "question": "Quelle temp√©rature ambiante optimale pour poussins Ross 308 jour 1?",
            "ground_truth": "La temp√©rature ambiante optimale pour les poussins Ross 308 au jour 1 est de 32-34¬∞C.",
            "contexts": [],
            "answer": "",
        },
        {
            "question": "Quel taux d'humidit√© optimal dans b√¢timent broiler semaine 4?",
            "ground_truth": "Le taux d'humidit√© optimal dans un b√¢timent broiler en semaine 4 est de 50-70%.",
            "contexts": [],
            "answer": "",
        },
        # Laying Hens - ISA Brown
        {
            "question": "Combien d'≈ìufs par jour pour 1000 poules ISA Brown √† 28 semaines?",
            "ground_truth": "1000 poules ISA Brown √† 28 semaines produisent environ 950-960 ≈ìufs par jour (95-96% de ponte).",
            "contexts": [],
            "answer": "",
        },
        {
            "question": "Quel poids corporel cible pour poulette ISA Brown √† 16 semaines?",
            "ground_truth": "Le poids corporel cible pour une poulette ISA Brown √† 16 semaines est de 1350-1450g.",
            "contexts": [],
            "answer": "",
        },
        # Health & Veterinary
        {
            "question": "Quel est le protocole de vaccination Newcastle pour pondeuses?",
            "ground_truth": "Le protocole de vaccination Newcastle pour pondeuses inclut g√©n√©ralement une primo-vaccination √† 10-14 jours (vaccin vivant att√©nu√©), un rappel √† 4-6 semaines, puis des rappels r√©guliers tous les 2-3 mois selon le niveau de pression sanitaire.",
            "contexts": [],
            "answer": "",
        },
        # Comparative Questions
        {
            "question": "Quelle diff√©rence de FCR entre Ross 308 et Cobb 500 √† 42 jours?",
            "ground_truth": "√Ä 42 jours, Ross 308 et Cobb 500 ont des FCR tr√®s similaires (1.65-1.70). La diff√©rence est minime (<0.05) avec un l√©ger avantage potentiel pour Ross 308 dans certaines conditions.",
            "contexts": [],
            "answer": "",
        },
        # Mathematical Operations
        {
            "question": "Combien de kg d'aliment pour √©lever 1000 Ross 308 de 0 √† 35 jours?",
            "ground_truth": "Pour √©lever 1000 Ross 308 de 0 √† 35 jours, il faut environ 3500-3700 kg d'aliment (FCR ~1.50 √ó poids moyen 2.3kg √ó 1000 poulets).",
            "contexts": [],
            "answer": "",
        },
    ]


# ============================================================================
# MAIN: Example Usage
# ============================================================================


async def main_example():
    """Exemple d'utilisation de RAGASEvaluator"""

    # Initialiser √©valuateur
    evaluator = RAGASEvaluator(llm_model="gpt-4o", temperature=0.0)

    # G√©n√©rer dataset golden
    golden_dataset = generate_poultry_golden_dataset()

    # Simuler r√©ponses du RAG (dans la pratique, interroger le vrai syst√®me)
    for case in golden_dataset[:3]:  # Test avec 3 cas
        # Simuler appel au RAG
        case["answer"] = f"R√©ponse simul√©e pour: {case['question']}"
        case["contexts"] = [
            "Contexte 1: Information pertinente...",
            "Contexte 2: Donn√©es additionnelles...",
        ]

    # √âvaluer
    results = await evaluator.evaluate_async(golden_dataset[:3])

    # Afficher r√©sum√©
    print(results["summary"])

    # Sauvegarder r√©sultats
    evaluator.save_results(results, "evaluation_results.json")


if __name__ == "__main__":
    # Exemple ex√©cution
    asyncio.run(main_example())
