# D√©cisions Techniques - Voice Realtime

Ce document r√©pond aux questions **Q5, Q6, Q7** apr√®s ex√©cution des POCs.

---

## Q5 : Strat√©gie d'Injection Contexte RAG

### Options √âvalu√©es

#### Option A : Injection Apr√®s VAD (Simple)

**Flow** :
```
1. User parle
2. VAD d√©tecte fin de parole (~200ms)
3. ‚Üí Query Weaviate (~280ms P95)
4. ‚Üí Injecter contexte dans OpenAI
5. ‚Üí OpenAI g√©n√®re r√©ponse (~300ms)
Total estim√©: ~780ms
```

**Avantages** :
- ‚úÖ Simple √† impl√©menter
- ‚úÖ Contexte toujours pertinent (bas√© sur transcription compl√®te)
- ‚úÖ Logique lin√©aire facile √† d√©bugger

**Inconv√©nients** :
- ‚ùå Latence perceptible (~780ms)
- ‚ùå D√©passe objectif 500ms P95
- ‚ùå User doit attendre fin query Weaviate

**Code simplifi√©** :
```python
async def on_speech_end(transcript: str):
    # 1. Query Weaviate
    context = await query_weaviate(transcript)  # +280ms

    # 2. Injecter comme message syst√®me
    await openai_session.send({
        "type": "conversation.item.create",
        "item": {
            "type": "message",
            "role": "system",
            "content": f"Contexte: {context}"
        }
    })

    # 3. Trigger r√©ponse
    await openai_session.send({"type": "response.create"})
```

---

#### Option B : Pr√©-chargement Pendant Parole (Optimal)

**Flow** :
```
1. User commence √† parler
2. ‚Üí Streaming transcription partielle (OpenAI)
3. ‚Üí Query Weaviate en parall√®le avec transcription partielle
4. VAD d√©tecte fin de parole (~200ms)
5. ‚Üí Contexte D√âJ√Ä PR√äT
6. ‚Üí OpenAI g√©n√®re imm√©diatement (~300ms)
Total estim√©: ~500ms
```

**Avantages** :
- ‚úÖ Latence optimale (~500ms P95)
- ‚úÖ Atteint objectif performance
- ‚úÖ Meilleure UX (pas d'attente visible)

**Inconv√©nients** :
- ‚ö†Ô∏è Complexe √† impl√©menter
- ‚ö†Ô∏è Risque query Weaviate avec transcription incompl√®te
- ‚ö†Ô∏è Gestion cache n√©cessaire (√©viter queries inutiles)

**Code avanc√©** :
```python
class RealtimeRAGHandler:
    def __init__(self):
        self.partial_transcript = ""
        self.context_cache = None
        self.context_ready = False

    async def on_partial_transcript(self, partial: str):
        """Appel√© d√®s qu'on a transcription partielle"""
        self.partial_transcript = partial

        # Si assez de mots, pr√©-charger contexte
        if len(partial.split()) >= 5:  # Au moins 5 mots
            if not self.context_ready:
                asyncio.create_task(self._preload_context(partial))

    async def _preload_context(self, query: str):
        """Query Weaviate en background"""
        self.context_cache = await query_weaviate(query)
        self.context_ready = True

    async def on_speech_end(self, final_transcript: str):
        """Fin de parole d√©tect√©e"""
        # Attendre contexte si pas encore pr√™t
        if not self.context_ready:
            await asyncio.wait_for(
                self._wait_context_ready(),
                timeout=0.5  # Max 500ms
            )

        # Si contexte pr√™t, utiliser
        context = self.context_cache if self.context_ready else None

        # G√©n√©rer r√©ponse
        await self._generate_response(final_transcript, context)
```

---

#### Option C : Injection Post-G√©n√©ration (Correction)

**Flow** :
```
1. User parle
2. OpenAI g√©n√®re r√©ponse imm√©diate (sans contexte)
3. ‚Üí En parall√®le, query Weaviate
4. ‚Üí Si r√©ponse incorrecte, corriger avec contexte
```

**Verdict** : ‚ùå **NON RECOMMAND√â**
- Mauvaise UX (r√©ponse puis correction visible)
- Complexe √† g√©rer (comment d√©tecter erreur ?)
- Gaspille tokens OpenAI

---

### üéØ D√âCISION RECOMMAND√âE

**Option B (Pr√©-chargement)** avec fallback Option A.

**Strat√©gie hybride** :
```python
async def adaptive_rag_injection(partial_transcript, final_transcript):
    """Strat√©gie hybride intelligente"""

    # Tentative pr√©-chargement (Option B)
    if len(partial_transcript.split()) >= 5:
        context = await asyncio.wait_for(
            query_weaviate(partial_transcript),
            timeout=0.3  # Max 300ms
        )
        if context:
            return context

    # Fallback: Query apr√®s VAD (Option A)
    # Mais avec timeout strict
    try:
        context = await asyncio.wait_for(
            query_weaviate(final_transcript),
            timeout=0.2  # Max 200ms
        )
        return context
    except asyncio.TimeoutError:
        # Si trop lent, continuer sans contexte
        logger.warning("Weaviate timeout, using LLM general knowledge")
        return None
```

**Crit√®res d'activation** :
- ‚úÖ Option B si transcription partielle >5 mots
- ‚úÖ Option A (timeout strict) si pas assez de mots
- ‚úÖ Fallback sans contexte si Weaviate >200ms

---

## Q6 : M√©canisme d'Interruption Utilisateur

### Sc√©nario Cible

```
1. LLM parle (g√©n√®re r√©ponse longue)
2. User commence √† parler (interruption)
3. ‚Üí Syst√®me doit arr√™ter imm√©diatement
4. ‚Üí √âcouter nouvelle question
5. ‚Üí Conserver contexte conversationnel
```

### D√©tection de l'Interruption

**M√©thode 1 : VAD C√¥t√© Client (Recommand√©e)**

```typescript
// Frontend: useVoiceRealtime.ts
const vadInstance = new VoiceActivityDetector({
  threshold: 0.3,
  minSpeechDuration: 300  // 300ms pour confirmer
});

vadInstance.on('speechStart', () => {
  if (isSpeaking) {  // LLM en train de parler
    handleInterruption();
  }
});

async function handleInterruption() {
  console.log('üõë User interruption detected');

  // 1. Arr√™ter lecture audio locale
  audioQueue.clear();
  audioContext.suspend();

  // 2. Notifier backend
  websocket.send({
    type: 'user.interrupt',
    timestamp: Date.now()
  });

  // 3. Pr√©parer √©coute nouvelle question
  setIsListening(true);
  setIsSpeaking(false);
}
```

**M√©thode 2 : VAD C√¥t√© Serveur (OpenAI)**

OpenAI Realtime API d√©tecte automatiquement parole pendant g√©n√©ration.

```python
# Backend: voice_realtime.py
async def on_openai_event(event):
    if event['type'] == 'input_audio_buffer.speech_started':
        # OpenAI a d√©tect√© parole user
        if currently_speaking:
            await handle_interruption()

async def handle_interruption():
    # 1. Annuler g√©n√©ration en cours
    await openai_session.send({
        'type': 'response.cancel'
    })

    # 2. Notifier frontend
    await websocket.send_json({
        'type': 'llm.interrupted',
        'timestamp': datetime.now().isoformat()
    })
```

---

### Gestion du Contexte Conversationnel

**Question** : Conserver ou r√©initialiser contexte ?

**Option A : Conserver Contexte (Recommand√©e)**
```python
# Historique conversation conserv√©
conversation_history = [
    {"role": "user", "content": "Quelle temp√©rature..."},
    {"role": "assistant", "content": "La temp√©rature d'incub‚Äî"},  # Interrompu
    {"role": "user", "content": "Non attends, plut√¥t l'humidit√© ?"}
]
```

**Option B : R√©initialiser**
```python
# Supprimer r√©ponse incompl√®te
conversation_history = [
    {"role": "user", "content": "Quelle temp√©rature..."},
    # R√©ponse interrompue supprim√©e
    {"role": "user", "content": "Plut√¥t l'humidit√© ?"}
]
```

**D√©cision** : **Option A (Conserver)**
- ‚úÖ Permet au LLM de comprendre correction ("non attends...")
- ‚úÖ Contexte conversationnel plus naturel
- ‚ö†Ô∏è Risque confusion si r√©ponse partielle incorrecte

---

### Gestion de la Queue Audio Frontend

```typescript
class AudioQueue {
  private queue: AudioBuffer[] = [];
  private playing: boolean = false;

  clear() {
    this.queue = [];
    this.playing = false;

    // Arr√™ter lecture en cours
    if (this.currentSource) {
      this.currentSource.stop();
      this.currentSource = null;
    }
  }

  async playNext() {
    if (!this.playing || this.queue.length === 0) return;

    const buffer = this.queue.shift();
    this.currentSource = this.audioContext.createBufferSource();
    this.currentSource.buffer = buffer;
    this.currentSource.connect(this.audioContext.destination);

    this.currentSource.onended = () => {
      this.playNext();  // Cha√Ænage
    };

    this.currentSource.start();
  }
}
```

---

### üéØ D√âCISION RECOMMAND√âE

**Strat√©gie d'interruption compl√®te** :

1. **D√©tection** : VAD client + VAD serveur (double s√©curit√©)
2. **Action imm√©diate** :
   - Frontend : Clear audio queue + suspend playback
   - Backend : `response.cancel` √† OpenAI
3. **Contexte** : Conserver historique avec r√©ponse partielle
4. **Transition** : `isSpeaking ‚Üí isListening` instantan√©

**Feedback UX** :
- Animation bouton : Bleu (speaking) ‚Üí Vert (listening) imm√©diat
- Feedback haptique mobile (vibration courte)
- Optionnel : Message toast "Vous m'avez interrompu, je vous √©coute"

---

## Q7 : Format Audio et Compatibilit√© Mobile

### Format Audio S√©lectionn√©

**Apr√®s test Q4**, choisir entre :

#### Option 1 : PCM16 Raw + Base64 JSON (Simple)

```json
{
  "type": "audio.input",
  "audio": "AAABAAACAAA...",  // Base64
  "format": "pcm16",
  "sample_rate": 16000
}
```

**Avantages** :
- ‚úÖ Compatible tous navigateurs
- ‚úÖ Debugging facile (JSON lisible)
- ‚úÖ Pas de codec complexe

**Inconv√©nients** :
- ‚ùå Overhead +33% bande passante
- ‚ùå Encoding/decoding CPU

**Verdict** : ‚úÖ **RECOMMAND√â pour MVP**

---

#### Option 2 : WebSocket Binaire + Opus (Optimis√©)

```javascript
// Frontend
const opusEncoder = new OpusEncoder({
  sampleRate: 16000,
  channels: 1,
  bitrate: 16000
});

const compressed = opusEncoder.encode(pcmData);
websocket.send(compressed);  // Binaire direct
```

**Avantages** :
- ‚úÖ √âconomie bande passante (compression ~10x)
- ‚úÖ Latence r√©duite (moins de data)

**Inconv√©nients** :
- ‚ùå Complexit√© (encoder/decoder)
- ‚ùå Compatibilit√© variable (iOS Safari)

**Verdict** : ‚ö†Ô∏è **Phase 2 si probl√®me bande passante**

---

### Sample Rate

**Options** :
- 16kHz (√©conomie)
- 44.1kHz (qualit√©)
- 48kHz (standard pro)

**D√©cision** : **16kHz**
- ‚úÖ Suffisant pour voix humaine
- ‚úÖ √âconomie bande passante 2.75x vs 44.1kHz
- ‚úÖ Support√© par OpenAI Realtime API

---

### Compatibilit√© Mobile

#### iOS Safari

**Probl√®mes connus** :
1. **Autoplay policy** : Audio ne d√©marre pas sans interaction user
2. **AudioContext** : Doit √™tre cr√©√© apr√®s user gesture
3. **Microphone** : Permission requise

**Solutions** :
```typescript
// Cr√©er AudioContext apr√®s click user
button.addEventListener('click', async () => {
  // 1. Cr√©er context (obligatoire sur iOS)
  const audioContext = new (window.AudioContext || window.webkitAudioContext)();
  await audioContext.resume();  // IMPORTANT pour iOS

  // 2. Demander micro
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

  // 3. D√©marrer WebSocket
  connectWebSocket();
});
```

**Gestion Bluetooth** :
```typescript
// D√©tecter changement sortie audio
navigator.mediaDevices.addEventListener('devicechange', () => {
  console.log('Audio device changed (Bluetooth ?)');
  // Reconnecter si n√©cessaire
});
```

---

#### Android Chrome

**Moins de probl√®mes qu'iOS** :
- ‚úÖ AudioContext plus permissif
- ‚úÖ Autoplay policy moins stricte

**Optimisations** :
```typescript
// Wake lock pour √©viter sleep pendant conversation
const wakeLock = await navigator.wakeLock.request('screen');
```

---

### Feedback Haptique

```typescript
// Vibration √† l'interruption
if ('vibrate' in navigator) {
  navigator.vibrate(50);  // 50ms
}

// Pattern pour d√©but/fin conversation
navigator.vibrate([100, 50, 100]);  // Bip-bip
```

---

### üéØ D√âCISION RECOMMAND√âE

**Configuration audio finale** :

```typescript
const AUDIO_CONFIG = {
  // Format
  format: 'pcm16',
  encoding: 'base64',  // JSON pour MVP

  // Sample rate
  sampleRate: 16000,
  channels: 1,

  // Buffering
  chunkDuration: 100,  // 100ms chunks
  bufferSize: 4096,

  // Mobile
  iosAutoplayFix: true,
  androidWakeLock: true,
  hapticFeedback: true,

  // Fallback
  codecFallback: ['pcm16', 'opus', 'aac']
};
```

**Tests requis** :
- [ ] iPhone 12+ Safari (iOS 16+)
- [ ] Android Chrome (Android 11+)
- [ ] Test avec AirPods (Bluetooth)
- [ ] Test avec connexion 4G lente

---

## üìä Tableau R√©capitulatif des D√©cisions

| Question | D√©cision | Justification | Complexit√© |
|----------|----------|---------------|------------|
| **Q5: Injection RAG** | Option B (pr√©-chargement) + fallback | Latence optimale ~500ms | ‚ö†Ô∏è Moyenne |
| **Q6: Interruption** | VAD client + serveur, contexte conserv√© | UX naturelle | ‚úÖ Simple |
| **Q7: Format audio** | PCM16 16kHz Base64 JSON | Compatibilit√© maximale | ‚úÖ Simple |

---

## üöÄ Prochaines √âtapes

Apr√®s validation de ces d√©cisions :

1. ‚úÖ **Phase 1** : Backend WebSocket (2-3 jours)
   - Impl√©menter Option B injection RAG
   - G√©rer interruption avec `response.cancel`
   - Format audio PCM16 Base64

2. ‚úÖ **Phase 2** : Frontend (2-3 jours)
   - Hook `useVoiceRealtime` avec VAD
   - AudioQueue avec gestion interruption
   - Fixes iOS Safari

3. ‚úÖ **Phase 3** : Tests Mobile (1 jour)
   - iPhone 12+ Safari
   - Android Chrome
   - Bluetooth audio

4. ‚úÖ **Phase 4** : Optimisations (1 jour)
   - Si bande passante probl√®me ‚Üí Opus codec
   - Cache Weaviate questions fr√©quentes
   - Monitoring latence production

---

## ‚úÖ Validation Finale

**Date** : __________

**D√©cisions valid√©es** :
- [ ] Q5 : Option B pr√©-chargement RAG
- [ ] Q6 : Interruption VAD double + contexte conserv√©
- [ ] Q7 : PCM16 16kHz Base64 JSON

**Signature tech lead** : __________

**GO pour Phase 1 d√©veloppement** : ‚òê OUI  ‚òê NON
