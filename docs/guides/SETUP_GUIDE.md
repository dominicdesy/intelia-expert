# Guide de Setup - Intelia LLM

**Version**: 1.0.0
**Date**: 2025-10-27
**Objectif**: DÃ©marrer le dÃ©veloppement du service LLM avec budget <$150/mois

---

## Ã‰tape 1: CrÃ©er Compte HuggingFace (5 min)

### 1.1 Inscription

1. Aller sur https://huggingface.co/join
2. CrÃ©er compte avec email professionnel Intelia
3. VÃ©rifier email

### 1.2 Accepter Termes Meta Llama

**IMPORTANT**: Requis pour accÃ©der au modÃ¨le Llama 3.1 8B

1. Aller sur https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct
2. Cliquer "Agree and access repository"
3. Accepter les conditions d'utilisation Meta
4. **Attendre approbation** (gÃ©nÃ©ralement instantanÃ©, parfois 1-2h)

### 1.3 CrÃ©er API Token

1. Aller sur https://huggingface.co/settings/tokens
2. Cliquer "New token"
3. **Name**: `intelia-llm-dev`
4. **Type**: Write (requis pour fine-tuning)
5. Copier le token (commence par `hf_...`)
6. **Sauvegarder dans 1Password/secrets manager** âš ï¸

---

## Ã‰tape 2: Configuration Digital Ocean (10 min)

### 2.1 CrÃ©er Service `llm`

**Dans Digital Ocean Console**:

1. Apps â†’ Create App
2. Source: GitHub
3. Repository: `intelia-expert` (monorepo)
4. **Branch**: `main`
5. **Source Directory**: `/llm` (sera crÃ©Ã© par Claude)
6. **Build Command**: (laisser auto-detect)
7. **Run Command**: (laisser auto-detect)

### 2.2 Configuration Service

**Nom**: `llm`
**Region**: Same as ai-service (US East ou Frankfurt)
**Instance Size**: Basic ($12/mois) âœ… Suffisant pour Phase 1 (proxy API)
**HTTP Port**: 8081

### 2.3 Environment Variables

Ajouter ces variables dans DO Console:

```bash
# Provider (Phase 1: HuggingFace Serverless)
LLM_PROVIDER=huggingface

# HuggingFace Credentials
HUGGINGFACE_API_KEY=hf_xxxxxxxxxxxxxxxxx  # SECRET - du Step 1.3
HUGGINGFACE_MODEL=meta-llama/Llama-3.1-8B-Instruct

# Server Config
PORT=8081
LOG_LEVEL=INFO
ENABLE_METRICS=true

# Monitoring (optionnel Phase 1)
LANGFUSE_PUBLIC_KEY=  # Ã€ configurer plus tard
LANGFUSE_SECRET_KEY=  # Ã€ configurer plus tard
```

**âš ï¸ IMPORTANT**: Marquer `HUGGINGFACE_API_KEY` comme **SECRET**

### 2.4 Internal Networking

VÃ©rifier que `llm` et `ai-service` sont dans le **mÃªme App** ou **mÃªme VPC**.

**Internal URLs**:
- `ai-service`: http://ai-service:8080
- `llm`: http://llm:8081 âœ… (nouveau)

### 2.5 Health Check

```
Path: /health
Initial Delay: 30s
Period: 10s
Timeout: 5s
```

---

## Ã‰tape 3: Validation AccÃ¨s Llama (2 min)

### Test accÃ¨s au modÃ¨le

```python
# Test local (avant dÃ©ploiement)
from huggingface_hub import InferenceClient

client = InferenceClient(token="hf_xxxxxxx")

# Test si accÃ¨s Llama accordÃ©
try:
    response = client.chat_completion(
        model="meta-llama/Llama-3.1-8B-Instruct",
        messages=[{"role": "user", "content": "Hello"}],
        max_tokens=20
    )
    print("âœ… AccÃ¨s Llama validÃ©!")
    print(response.choices[0].message.content)
except Exception as e:
    print("âŒ Erreur accÃ¨s Llama:", e)
    print("VÃ©rifier: 1) Token valide, 2) Terms acceptÃ©s, 3) Approbation reÃ§ue")
```

**Si erreur "Gated model"**: Attendre approbation Meta (vÃ©rifier email HF)

---

## Ã‰tape 4: Budget Phase 1 - DÃ©tails

### StratÃ©gie Optimisation CoÃ»ts

**Phase DÃ©veloppement (150-250 requÃªtes test)**:

| Service | Pricing | CoÃ»t 250 requÃªtes |
|---------|---------|-------------------|
| **HuggingFace Serverless API** | $0.20/1M tokens | ~$0.05 |
| Calcul: 250 requÃªtes Ã— 500 tokens moy Ã— $0.20/1M = $0.025 | | |

**CoÃ»t mensuel rÃ©el Phase 1**:

```
Digital Ocean llm (Basic):     $12/mois
HuggingFace Hub (private):      $9/mois (optionnel si modÃ¨le public OK)
Fine-tuning (2-3 runs):        $15 total (pas mensuel)
Dev API calls (250 tests):     ~$0.05

TOTAL PHASE 1: ~$21-36/mois âœ… Bien sous $150
```

**Notes**:
- HF Serverless API = pay-per-use (pas de minimum)
- Pas besoin d'Inference Endpoint dedicated en Phase 1
- Digital Ocean Basic tier suffit (juste un proxy FastAPI)

### Quand Upgrade?

**Phase 2 (Production)** - Quand >10,000 requÃªtes/mois:
- Upgrade DO: Basic â†’ Professional ($50/mois)
- Ajouter: HF Inference Endpoint ($200-300/mois)
- **Total**: ~$250-350/mois

---

## Ã‰tape 5: DÃ©veloppement (Claude Code)

### 5.1 Structure Ã  CrÃ©er

```
llm/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ config.py            # Config env vars
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ llm_client.py    # Provider abstraction
â”‚   â”‚   â””â”€â”€ schemas.py       # Pydantic models
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ chat.py          # /v1/chat/completions
â”‚   â”‚   â”œâ”€â”€ models.py        # /v1/models
â”‚   â”‚   â””â”€â”€ health.py        # /health
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ metrics.py       # Prometheus
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_providers.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .dockerignore
â””â”€â”€ README.md
```

### 5.2 API Ã  ImplÃ©menter

**POST /v1/chat/completions** (OpenAI-compatible)
```json
{
  "model": "intelia-llama-3.1-8b",
  "messages": [...],
  "temperature": 0.7,
  "max_tokens": 2000
}
```

**GET /v1/models** (list models)
**GET /health** (health check)
**GET /metrics** (Prometheus)

### 5.3 Provider Abstraction

Phase 1: HuggingFaceProvider (Serverless API)
Phase 2: vLLMProvider (Self-hosted)

**MÃªme code ai-service, zero changes lors migration** âœ…

---

## Ã‰tape 6: Tests DÃ©veloppement

### 6.1 Tests Locaux (150-250 requÃªtes)

**Questions test recommandÃ©es** (couvrir use cases):

**CatÃ©gorie 1: Questions simples** (50 questions)
- "C'est quoi la coccidiose ?"
- "Comment vacciner des poulets ?"
- "Quelle tempÃ©rature pour poussins ?"

**CatÃ©gorie 2: Diagnostics** (75 questions)
- "Mes poulets ont diarrhÃ©e, que faire ?"
- "MortalitÃ© 5% Ã  J+10, causes possibles ?"
- "DÃ©tresse respiratoire + lÃ©thargie, diagnostic ?"

**CatÃ©gorie 3: Optimisation** (50 questions)
- "Comment rÃ©duire indice de conversion ?"
- "AmÃ©liorer taux de ponte ?"
- "Optimiser croissance poulets de chair ?"

**CatÃ©gorie 4: Multilingual** (50 questions)
- English, EspaÃ±ol, PortuguÃªs (test multilingual)

**CatÃ©gorie 5: Edge cases** (25 questions)
- Questions hors-scope
- Questions ambiguÃ«s
- Questions multi-tours

**Total: 250 requÃªtes Ã— 500 tokens = ~$0.05** âœ…

### 6.2 Validation QualitÃ©

**MÃ©triques Ã  mesurer**:
- Latency (target: <3s Phase 1)
- Quality (human eval 1-5)
- Relevance (rÃ©pond Ã  la question ?)
- Factuality (pas d'hallucinations ?)

**Comparaison**:
- Llama 3.1 8B base vs GPT-4o (baseline)
- Objectif: QualitÃ© comparable ou meilleure sur domaine aviculture

---

## Ã‰tape 7: Fine-tuning (AprÃ¨s validation Phase 1)

### PrÃ©requis

âœ… Service llm dÃ©ployÃ© et fonctionnel
âœ… API validÃ©e avec 250 requÃªtes test
âœ… Dataset prÃ©parÃ© (10K+ conversations PostgreSQL)

### Process

1. **Upload dataset HuggingFace**
```bash
huggingface-cli upload intelia/aviculture-dataset ./data/train.jsonl
```

2. **Fine-tune avec AutoTrain**
```bash
autotrain llm \
  --train \
  --model meta-llama/Llama-3.1-8B-Instruct \
  --data-path intelia/aviculture-dataset \
  --project-name intelia-llama-aviculture
```

**CoÃ»t**: ~$5-10 par run

3. **Evaluate**
- Tester modÃ¨le fine-tunÃ© vs base
- Human evaluation
- A/B test si qualitÃ© supÃ©rieure

4. **Deploy**
- Changer `HUGGINGFACE_MODEL` â†’ `intelia/llama-3.1-8b-aviculture`
- Redeploy service
- Monitor qualitÃ©

---

## Checklist Setup Complet

### Phase 1 - Infrastructure (Cette semaine)

- [ ] Compte HuggingFace crÃ©Ã©
- [ ] Terms Meta Llama acceptÃ©s et approuvÃ©s
- [ ] API Token HuggingFace gÃ©nÃ©rÃ© et sÃ©curisÃ©
- [ ] Service DO `llm` crÃ©Ã© (Basic tier $12/mois)
- [ ] Environment variables configurÃ©es
- [ ] Health check configurÃ©
- [ ] Test accÃ¨s Llama validÃ© (script Python)

### Phase 2 - DÃ©veloppement (Semaine 2)

- [ ] Structure FastAPI crÃ©Ã©e par Claude
- [ ] API OpenAI-compatible implÃ©mentÃ©e
- [ ] Provider HuggingFace intÃ©grÃ©
- [ ] Tests unitaires passent
- [ ] Dockerfile validÃ©
- [ ] DÃ©ploiement DO rÃ©ussi
- [ ] `/health` retourne 200 OK

### Phase 3 - Validation (Semaine 3)

- [ ] 250 requÃªtes test exÃ©cutÃ©es (~$0.05)
- [ ] Latency <3s mesurÃ©e
- [ ] QualitÃ© comparable GPT-4o validÃ©e
- [ ] Pas d'hallucinations majeures
- [ ] Multilingual (FR/EN/ES) validÃ©

### Phase 4 - Integration ai-service (Semaine 4)

- [ ] ai-service appelle llm pour requÃªtes domain-specific
- [ ] Fallback GPT-4o configurÃ©
- [ ] Monitoring Prometheus actif
- [ ] A/B test 10% trafic â†’ llm
- [ ] User satisfaction â‰¥ baseline

### Phase 5 - Fine-tuning (Semaine 5-6)

- [ ] Dataset 10K+ conversations extrait PostgreSQL
- [ ] Fine-tuning Llama rÃ©ussi ($5-10)
- [ ] ModÃ¨le fine-tunÃ© dÃ©ployÃ©
- [ ] Quality improvement mesurÃ©
- [ ] 100% trafic domain â†’ llm fine-tunÃ©

---

## Troubleshooting

### Erreur: "Gated model - Access denied"

**Cause**: Terms Meta Llama pas acceptÃ©s ou approbation en attente

**Solution**:
1. Aller sur https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct
2. VÃ©rifier statut approbation
3. Attendre email confirmation (1-2h max)
4. VÃ©rifier token a scope "Read" minimum

### Erreur: "Rate limit exceeded"

**Cause**: Trop de requÃªtes sur HF Serverless (rare en dev)

**Solution**:
1. Attendre quelques minutes
2. Ou upgrade HF Pro ($9/mois) pour rate limit plus haut
3. Ou passer Ã  Inference Endpoint dedicated

### Service DO ne dÃ©marre pas

**Cause**: Dockerfile ou requirements.txt invalide

**Solution**:
1. VÃ©rifier logs DO Console
2. Tester build local: `docker build -t llm .`
3. VÃ©rifier PORT=8081 dans env vars
4. VÃ©rifier health check path `/health`

### Latency >10s

**Cause**: Cold start HF Serverless API

**Solution**:
- PremiÃ¨re requÃªte toujours lente (cold start)
- RequÃªtes suivantes <2s
- Si problÃ¨me persiste: upgrade Inference Endpoint

---

## Support & Resources

**HuggingFace**:
- Docs: https://huggingface.co/docs
- Community: https://discuss.huggingface.co
- Support: support@huggingface.co (si Pro)

**Meta Llama**:
- Model Card: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct
- License: https://llama.meta.com/llama3_1/license/

**Digital Ocean**:
- Docs: https://docs.digitalocean.com/products/app-platform/
- Support: Console â†’ Support â†’ New Ticket

---

## Prochaines Ã‰tapes ImmÃ©diatement

### TOI (Maintenant - 15 min):

1. **CrÃ©er compte HuggingFace** â†’ Copier API token
2. **Accepter terms Llama** â†’ Attendre approbation
3. **CrÃ©er service DO `llm`** â†’ Basic tier, env vars
4. **Me donner feu vert** â†’ Je dÃ©veloppe le code

### MOI (AprÃ¨s ton feu vert - 2h):

1. CrÃ©er structure complÃ¨te `llm/`
2. ImplÃ©menter API FastAPI OpenAI-compatible
3. Provider HuggingFace
4. Dockerfile + tests
5. Push + dÃ©ploiement DO

### NOUS (AprÃ¨s dÃ©ploiement - 1h):

1. Tests validation 50 requÃªtes
2. Mesurer latency/qualitÃ©
3. Ajustements si besoin

**Timeline totale Phase 1: 1 semaine** âœ…

---

**Dis-moi quand tu es prÃªt Ã  dÃ©marrer !** ðŸš€
