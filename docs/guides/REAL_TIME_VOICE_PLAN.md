# Plan d'ImplÃ©mentation: Conversation Vocale en Temps RÃ©el

## ğŸ“‹ Vue d'Ensemble

Transformer l'expÃ©rience actuelle (question â†’ attendre la rÃ©ponse complÃ¨te â†’ Ã©couter) en une conversation naturelle et fluide avec rÃ©ponses en streaming audio.

---

## ğŸ¯ Objectifs

1. **Streaming Audio ImmÃ©diat**: L'utilisateur entend la rÃ©ponse dÃ¨s les premiers mots gÃ©nÃ©rÃ©s
2. **Latence RÃ©duite**: ~500ms entre question vocale et dÃ©but de la rÃ©ponse audio
3. **Conversation Naturelle**: PossibilitÃ© d'interrompre et de reprendre comme une vraie conversation
4. **ExpÃ©rience Mobile Optimale**: FluiditÃ© parfaite sur iPhone/Android

---

## ğŸ—ï¸ Architecture ProposÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚
â”‚   (Next.js)     â”‚
â”‚                 â”‚
â”‚  â€¢ Microphone   â”‚
â”‚  â€¢ Web Audio    â”‚
â”‚  â€¢ Audio Queue  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ WebSocket
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Backend      â”‚
â”‚   (FastAPI)     â”‚
â”‚                 â”‚
â”‚  â€¢ WebSocket    â”‚
â”‚  â€¢ OpenAI RT    â”‚
â”‚  â€¢ Weaviate     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Technologies Requises

### Backend
- **OpenAI Realtime API**: Streaming audio bidirectionnel
- **WebSocket (FastAPI)**: Communication temps rÃ©el avec frontend
- **Weaviate**: RAG pour contexte mÃ©tier (dÃ©jÃ  en place)

### Frontend
- **WebSocket API**: Communication bidirectionnelle
- **Web Audio API**: Lecture audio fluide avec queue
- **MediaRecorder**: Capture audio microphone (dÃ©jÃ  en place)

---

## ğŸ“¦ Composants Ã  DÃ©velopper

### 1. Backend - Endpoint WebSocket (`/ws/voice`)

**Fichier**: `backend/app/api/v1/voice_realtime.py`

**ResponsabilitÃ©s**:
- Accepter connexions WebSocket des clients
- GÃ©rer session OpenAI Realtime API
- Router les messages audio bidirectionnels
- Injecter contexte RAG au bon moment
- GÃ©rer erreurs et reconnexions

**Flow**:
```
1. Client se connecte â†’ CrÃ©er session OpenAI RT
2. Client envoie audio â†’ TransfÃ©rer Ã  OpenAI
3. OpenAI gÃ©nÃ¨re rÃ©ponse â†’ Injecter contexte Weaviate si nÃ©cessaire
4. OpenAI stream audio â†’ TransfÃ©rer au client
5. Client dÃ©connecte â†’ Nettoyer session
```

**Pseudo-code clÃ©**:
```python
@router.websocket("/ws/voice")
async def voice_websocket(websocket: WebSocket):
    await websocket.accept()

    # CrÃ©er session OpenAI Realtime
    openai_session = await create_openai_realtime_session()

    # Router messages bidirectionnels
    async with asyncio.TaskGroup() as tg:
        tg.create_task(forward_client_to_openai(websocket, openai_session))
        tg.create_task(forward_openai_to_client(openai_session, websocket))
```

---

### 2. Frontend - Hook React (`useVoiceRealtime`)

**Fichier**: `frontend/lib/hooks/useVoiceRealtime.ts`

**ResponsabilitÃ©s**:
- GÃ©rer connexion WebSocket
- Capturer audio microphone en continu
- Envoyer chunks audio au backend
- Recevoir chunks audio de rÃ©ponse
- Jouer audio avec queue pour Ã©viter coupures
- GÃ©rer Ã©tats (listening, speaking, idle)

**API du Hook**:
```typescript
const {
  isConnected,           // WebSocket connectÃ©
  isListening,           // Microphone actif
  isSpeaking,            // LLM parle
  startConversation,     // DÃ©marrer session
  stopConversation,      // ArrÃªter session
  interrupt,             // Interrompre LLM
  audioLevel             // Volume micro (UI feedback)
} = useVoiceRealtime();
```

**Flow Audio**:
```
Microphone â†’ MediaRecorder â†’ chunks 100ms â†’ WebSocket â†’ Backend
Backend â†’ WebSocket â†’ Audio Queue â†’ Web Audio API â†’ Speakers
```

---

### 3. Frontend - Composant UI (`VoiceRealtimeButton`)

**Fichier**: `frontend/components/VoiceRealtimeButton.tsx`

**ResponsabilitÃ©s**:
- Bouton visuel pour dÃ©marrer/arrÃªter conversation
- Animation visuelle pendant Ã©coute/parole
- Indicateur de volume microphone (waveform)
- Gestion Ã©tats d'erreur
- Feedback haptique sur mobile

**Ã‰tats visuels**:
- ğŸ¤ **Idle**: Bouton microphone gris
- ğŸŸ¢ **Listening**: Animation verte pulsante
- ğŸ”µ **Speaking**: Animation bleue (LLM parle)
- â¸ï¸ **Interrupted**: Bouton pause
- âŒ **Error**: Indicateur rouge

---

## ğŸ”„ Flow Complet d'une Conversation

### Cas d'Usage: "Quelle est la tempÃ©rature d'incubation des Å“ufs?"

```
1. [USER] Appuie sur bouton vocal
   â†“
2. [FRONTEND] DÃ©marre WebSocket + Microphone
   â†“
3. [BACKEND] CrÃ©e session OpenAI Realtime
   â†“
4. [USER] Parle: "Quelle est la tempÃ©rature..."
   â†“
5. [FRONTEND] Envoie audio chunks (100ms) via WebSocket
   â†“
6. [BACKEND] TransfÃ¨re Ã  OpenAI Realtime API
   â†“
7. [OPENAI] DÃ©tecte fin de phrase (VAD)
   â†“
8. [BACKEND] Intercepte question â†’ Query Weaviate
   â†“
9. [BACKEND] Injecte contexte: "Voici infos pertinentes: ..."
   â†“
10. [OPENAI] GÃ©nÃ¨re rÃ©ponse avec contexte
    â†“
11. [OPENAI] Stream audio chunks
    â†“
12. [BACKEND] TransfÃ¨re chunks au client
    â†“
13. [FRONTEND] Queue audio + lecture immÃ©diate
    â†“
14. [USER] Entend rÃ©ponse ~500ms aprÃ¨s fin de question
    â†“
15. [USER] Peut interrompre ou poser nouvelle question
```

---

## ğŸ¨ IntÃ©gration RAG (Weaviate)

### StratÃ©gie d'Injection de Contexte

**Option A: Injection Transparente** (RecommandÃ©e)
```python
# DÃ¨s que OpenAI dÃ©tecte fin de question
async def on_speech_end(transcript: str):
    # Query Weaviate
    context = await query_weaviate(transcript)

    # Injecter comme message systÃ¨me
    await openai_session.send({
        "type": "conversation.item.create",
        "item": {
            "type": "message",
            "role": "system",
            "content": f"Context from database: {context}"
        }
    })

    # OpenAI gÃ©nÃ¨re rÃ©ponse avec contexte
```

**Option B: Instructions SystÃ¨me PrÃ©-configurÃ©es**
```python
# Au dÃ©but de session
system_instructions = """
Tu es un expert avicole. Utilise le contexte fourni.
Si contexte disponible, base ta rÃ©ponse dessus.
Si pas de contexte, utilise tes connaissances gÃ©nÃ©rales.
"""
```

---

## ğŸ“± Optimisations Mobile

### 1. Gestion Batterie
- Utiliser WebRTC VAD (Voice Activity Detection) pour Ã©conomiser bande passante
- ArrÃªter microphone quand LLM parle
- DÃ©connecter WebSocket aprÃ¨s 30s inactivitÃ©

### 2. Gestion RÃ©seau
- Reconnexion automatique si perte connexion
- Buffer audio local en cas de latence rÃ©seau
- DÃ©grader qualitÃ© audio si connexion lente (44kHz â†’ 16kHz)

### 3. Audio Mobile
- Utiliser AudioContext avec autoplay policy
- GÃ©rer changements audio route (Bluetooth, Ã©couteurs)
- Feedback haptique via Vibration API

---

## ğŸ› Gestion d'Erreurs

### Erreurs Critiques
1. **Microphone refusÃ©**: Fallback vers input texte
2. **WebSocket fermÃ©**: Tentative reconnexion (3x)
3. **OpenAI API timeout**: Message d'erreur + retry
4. **Weaviate indisponible**: Continuer sans contexte

### UX Errors
```typescript
// Afficher message contextuel
setError({
  type: 'microphone_denied',
  message: t('voice.error.microphone'),
  action: 'settings' // Lien vers paramÃ¨tres device
})
```

---

## ğŸ“Š MÃ©triques de SuccÃ¨s

### KPIs Ã  Tracker
1. **Latence P95**: < 500ms (question â†’ dÃ©but rÃ©ponse)
2. **Taux d'interruption**: % conversations interrompues par user
3. **DurÃ©e moyenne conversation**: Objectif 2-3 tours
4. **Taux d'erreur**: < 1% des sessions
5. **Adoption**: % utilisateurs utilisant mode vocal

---

## ğŸš€ Plan de DÃ©veloppement (EstimÃ©)

### Phase 1: Backend Foundation (2-3 jours)
- [ ] Endpoint WebSocket `/ws/voice`
- [ ] IntÃ©gration OpenAI Realtime API
- [ ] Tests basiques audio bidirectionnel

### Phase 2: Frontend Basic (2-3 jours)
- [ ] Hook `useVoiceRealtime`
- [ ] Composant `VoiceRealtimeButton`
- [ ] WebSocket client + audio queue

### Phase 3: RAG Integration (1-2 jours)
- [ ] Injection contexte Weaviate
- [ ] Tests avec vraies questions mÃ©tier
- [ ] Tuning prompts systÃ¨me

### Phase 4: Mobile Polish (1-2 jours)
- [ ] Optimisations batterie
- [ ] Gestion erreurs rÃ©seau
- [ ] Tests iPhone/Android

### Phase 5: Production (1 jour)
- [ ] Monitoring/logs
- [ ] Rate limiting
- [ ] Documentation API

**Total estimÃ©**: 7-10 jours de dÃ©veloppement

---

## ğŸ” SÃ©curitÃ© & CoÃ»ts

### SÃ©curitÃ©
- Authentifier WebSocket avec JWT token
- Limiter durÃ©e session (10 min max)
- Rate limiting par utilisateur (5 sessions/heure)

### CoÃ»ts OpenAI Realtime API
- **Input audio**: $0.06 / minute
- **Output audio**: $0.24 / minute
- **Estimation**: 1000 conversations/mois de 2 min = ~$600/mois

**Optimisation coÃ»ts**:
- ArrÃªter microphone quand LLM parle (-50% input)
- VAD pour ne transmettre que quand user parle
- Timeout aprÃ¨s inactivitÃ©

---

## ğŸ“š Ressources Techniques

### Documentation OpenAI
- [Realtime API Guide](https://platform.openai.com/docs/guides/realtime)
- [WebRTC Integration](https://platform.openai.com/docs/guides/realtime-webrtc)

### Exemples de RÃ©fÃ©rence
```bash
# Repo OpenAI officiel
git clone https://github.com/openai/openai-realtime-api-beta

# Exemple Next.js
cd examples/next-js-voice-assistant
```

### Web APIs UtilisÃ©es
- [WebSocket API](https://developer.mozilla.org/en-US/docs/Web/API/WebSocket)
- [Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)
- [MediaRecorder API](https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder)

---

## ğŸ¯ Alternative: ImplÃ©mentation Hybride

Si budget/temps limitÃ©, considÃ©rer approche hybride:

### Option Simple (dÃ©jÃ  fait)
- STT (Speech-to-Text) â†’ LLM â†’ TTS (Text-to-Speech)
- Latence: ~2-3 secondes
- CoÃ»t: ~$200/mois pour 1000 conversations

### Option Temps RÃ©el (ce document)
- Streaming audio bidirectionnel
- Latence: ~500ms
- CoÃ»t: ~$600/mois pour 1000 conversations

**Recommandation**: Commencer hybride, migrer vers temps rÃ©el si adoption forte.

---

## ğŸ“ Support & Questions

Ce plan sera la base de notre implÃ©mentation la semaine prochaine. Pour toute clarification:
- Relire ce document
- Consulter exemples OpenAI
- Tester dÃ©mo OpenAI Realtime Console

**Status**: âœ… Document de rÃ©fÃ©rence prÃªt pour dÃ©veloppement
