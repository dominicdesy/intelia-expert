# RAGAS Evaluation - Guide de dÃ©marrage rapide

## ğŸš€ Premiers pas sur Digital Ocean

### 1. Connexion SSH
```bash
ssh root@your-droplet-ip
cd /app/llm
```

### 2. VÃ©rifier prÃ©requis
```bash
# VÃ©rifier que RAGAS est installÃ©
python3 -c "import ragas; print('âœ… RAGAS OK')"

# Si erreur, installer
pip install ragas datasets langchain-openai

# VÃ©rifier OPENAI_API_KEY
echo $OPENAI_API_KEY
# Si vide, configurer:
export OPENAI_API_KEY='sk-...'
```

### 3. Premier test (30 secondes)
```bash
# Rendre le script exÃ©cutable
chmod +x scripts/evaluate.sh

# Lancer test rapide
./scripts/evaluate.sh test
```

**RÃ©sultat attendu :**
```
ğŸ§ª Test rapide (3 questions)
   DurÃ©e: ~30 secondes
   CoÃ»t estimÃ©: ~$0.01

ğŸš€ RAGAS EVALUATION - Intelia Expert LLM
ğŸ“Š Chargement dataset golden Intelia...
   âœ… 28 cas de test gÃ©nÃ©rÃ©s
   LimitÃ© Ã  3 cas de test
ğŸ”§ Initialisation InteliaRAGEngine...
   âœ… RAG Engine initialisÃ©
ğŸ” Interrogation du systÃ¨me RAG (3 questions)...
   [1/3] Je suis rendu au 18e jour de production...
   [2/3] Quelle maladie frappe le plus souvent...
   [3/3] Qu'est-ce que la cryptomonnaie ?
   âœ… Toutes les questions traitÃ©es

ğŸ“Š Ã‰valuation RAGAS en cours...

=================================================================
ğŸ“Š RAGAS EVALUATION REPORT - Intelia Expert LLM
=================================================================

Test Cases:        3
LLM Model:         gpt-4o-mini
Duration:          28.5s
Timestamp:         2025-10-07 14:30:00

-----------------------------------------------------------------
SCORES
-----------------------------------------------------------------
Overall Score:          85.30%

Context Precision:      88.40%
Context Recall:         82.10%
Faithfulness:           89.20%
Answer Relevancy:       81.50%

-----------------------------------------------------------------
INTERPRETATION
-----------------------------------------------------------------
âœ… TrÃ¨s Bon: QualitÃ© Ã©levÃ©e (80-90%)

=================================================================

âœ… Ã‰valuation terminÃ©e
ğŸ“ RÃ©sultats: logs/ragas_evaluation_20251007_143000.json
```

## ğŸ“‹ Commandes courantes

### Test rapide aprÃ¨s changements
```bash
./scripts/evaluate.sh quick    # 5 questions, ~1 min
```

### Ã‰valuation complÃ¨te avant dÃ©ploiement
```bash
./scripts/evaluate.sh full     # 28 questions, ~5 min
```

### Voir les rÃ©sultats rÃ©cents
```bash
# Lister les 5 derniÃ¨res Ã©valuations
ls -lht logs/ragas_evaluation_* | head -5

# Voir les scores de la derniÃ¨re Ã©valuation
cat $(ls -t logs/ragas_evaluation_*.json | head -1) | jq '.scores'
```

### Exemples de rÃ©sultats
```bash
# Scores globaux
cat logs/ragas_evaluation_*.json | jq '{
  overall: .scores.overall,
  precision: .scores.context_precision,
  recall: .scores.context_recall,
  faithfulness: .scores.faithfulness,
  relevancy: .scores.answer_relevancy
}'

# Questions qui ont Ã©chouÃ© (< 70%)
cat logs/ragas_evaluation_*.json | jq '
  .detailed_scores[] |
  select(.answer_relevancy < 0.7) |
  {question: .question, score: .answer_relevancy}
'
```

## âš ï¸ ProblÃ¨mes frÃ©quents

### "OPENAI_API_KEY non dÃ©finie"
```bash
export OPENAI_API_KEY='sk-proj-...'

# Pour rendre permanent, ajouter dans ~/.bashrc
echo "export OPENAI_API_KEY='sk-proj-...'" >> ~/.bashrc
source ~/.bashrc
```

### "Module ragas not found"
```bash
pip install ragas datasets langchain-openai
```

### "Permission denied: ./scripts/evaluate.sh"
```bash
chmod +x scripts/evaluate.sh
```

### Ã‰valuation trop lente
```bash
# Utiliser mode test (3 questions)
./scripts/evaluate.sh test

# Ou spÃ©cifier nombre rÃ©duit
python3 scripts/run_ragas_evaluation.py --test-cases 5
```

## ğŸ“Š InterprÃ©tation rapide

| Score Overall | Signification | Action |
|---------------|---------------|--------|
| **â‰¥ 90%** | ğŸ† Excellent | Continue comme Ã§a ! |
| **80-90%** | âœ… TrÃ¨s bon | RAS, petites optimisations possibles |
| **70-80%** | âš ï¸ Acceptable | Identifier questions problÃ©matiques |
| **< 70%** | âŒ ProblÃ¨me | Investigation requise |

### Si score baisse soudainement

1. **VÃ©rifier les logs :**
   ```bash
   tail -50 logs/ragas_evaluation_*.log
   ```

2. **Comparer avec Ã©valuation prÃ©cÃ©dente :**
   ```bash
   # Score actuel
   cat $(ls -t logs/ragas_evaluation_*.json | head -1) | jq '.scores.overall'

   # Score prÃ©cÃ©dent
   cat $(ls -t logs/ragas_evaluation_*.json | head -2 | tail -1) | jq '.scores.overall'
   ```

3. **Identifier questions problÃ©matiques :**
   ```bash
   cat logs/ragas_evaluation_*.json | jq '.detailed_scores[] | select(.answer_relevancy < 0.7)'
   ```

## ğŸ”„ Workflow recommandÃ©

### AprÃ¨s un changement de code
```bash
# 1. Test rapide
./scripts/evaluate.sh test

# 2. Si OK, Ã©valuation plus large
./scripts/evaluate.sh quick

# 3. Si tout OK, deployer
git push origin main
```

### Monitoring hebdomadaire
```bash
# Tous les lundis
./scripts/evaluate.sh quick > weekly_evaluation.txt
# VÃ©rifier que score > 80%
```

## ğŸ’¡ Conseils

âœ… **Bonnes pratiques :**
- Faire `test` aprÃ¨s chaque changement majeur
- Faire `quick` avant chaque dÃ©ploiement
- Faire `full` 1x/mois pour monitoring complet
- Garder historique des rÃ©sultats (dÃ©jÃ  dans logs/)

âŒ **Ã€ Ã©viter :**
- Ne pas lancer `full` trop souvent (coÃ»t)
- Ne pas ignorer score < 80%
- Ne pas dÃ©ployer sans tester

## ğŸ“ Support

**Documentation complÃ¨te :** `evaluation/README.md`

**Ajouter des questions :** Ã‰diter `evaluation/golden_dataset_intelia.py`

**ProblÃ¨me technique :** VÃ©rifier `logs/ragas_evaluation_*.log`
