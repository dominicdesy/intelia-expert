# Plan d'impl√©mentation: Claude Extended Thinking avec CoT

## üìä Analyse du syst√®me actuel

### Structure du r√©pertoire `llm/`

Le syst√®me actuel contient **265 fichiers Python** organis√©s en modules sp√©cialis√©s:

```
llm/
‚îú‚îÄ‚îÄ generation/          # G√©n√©ration de r√©ponses (CIBLE PRINCIPALE)
‚îú‚îÄ‚îÄ config/              # Prompts et configuration
‚îú‚îÄ‚îÄ core/                # RAG engine
‚îú‚îÄ‚îÄ retrieval/           # Recherche de documents
‚îú‚îÄ‚îÄ api/                 # Endpoints FastAPI
‚îú‚îÄ‚îÄ cache/               # Syst√®me de cache
‚îú‚îÄ‚îÄ security/            # Guardrails
‚îî‚îÄ‚îÄ monitoring/          # M√©triques
```

---

## üéØ Fichiers √† modifier (3 fichiers critiques)

### 1. **`llm/generation/generators.py`** (1,306 lignes) - PRIORIT√â 1

**R√¥le actuel:**
- Classe principale `EnhancedResponseGenerator`
- G√®re la g√©n√©ration avec GPT-4o et mod√®les O1
- Supporte d√©j√† CoT pour O1 (ligne 567-594)
- CoT pour autres mod√®les a √©t√© **supprim√©** (ligne 208-223)

**Modifications requises:**

#### A. Ajouter support Claude Extended Thinking (ligne 567-594)

**Actuellement:**
```python
# Line 567-594
is_o1_model = self.cot_model.startswith("o1-")
if is_o1_model:
    # Special handling for O1: no system msg, no temp, no max_tokens
    messages = [{"role": "user", "content": enhanced_prompt}]
    response = client.chat.completions.create(
        model=self.cot_model,
        messages=messages
    )
else:
    # Standard models (GPT-4o)
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": enhanced_prompt}
    ]
    response = client.chat.completions.create(
        model=self.cot_model,
        messages=messages,
        temperature=0.1,
        max_tokens=4096
    )
```

**Ajouter:**
```python
# Line 567-594 (MODIFIED)
is_o1_model = self.cot_model.startswith("o1-")
is_claude_extended = self.cot_model.startswith("claude-3.") or self.cot_model.startswith("claude-4.")

if is_o1_model:
    # O1 models: native reasoning, no system msg
    messages = [{"role": "user", "content": enhanced_prompt}]
    response = client.chat.completions.create(
        model=self.cot_model,
        messages=messages
    )

elif is_claude_extended:
    # Claude Extended Thinking: NOUVEAU CODE
    import anthropic

    anthropic_client = anthropic.Anthropic(
        api_key=os.getenv("ANTHROPIC_API_KEY")
    )

    # Budget de thinking selon complexit√©
    thinking_budget = self._calculate_thinking_budget(rag_result, query_intent)

    response = anthropic_client.messages.create(
        model=self.cot_model,
        max_tokens=16000,
        thinking={
            "type": "enabled",
            "budget_tokens": thinking_budget
        },
        messages=[{
            "role": "user",
            "content": f"""{system_prompt}

{enhanced_prompt}"""
        }]
    )

    # Extraire thinking et response
    thinking_content = None
    text_content = None

    for block in response.content:
        if block.type == "thinking":
            thinking_content = block.thinking
        elif block.type == "text":
            text_content = block.text

    # Envelopper dans format compatible OpenAI
    class ClaudeResponse:
        def __init__(self, thinking, text, usage):
            self.choices = [type('obj', (object,), {
                'message': type('obj', (object,), {
                    'content': text
                })()
            })()]
            self.usage = type('obj', (object,), {
                'prompt_tokens': usage.input_tokens,
                'completion_tokens': usage.output_tokens,
                'total_tokens': usage.input_tokens + usage.output_tokens
            })()
            self.thinking = thinking  # NOUVEAU: stocker le CoT

    response = ClaudeResponse(thinking_content, text_content, response.usage)

else:
    # Standard models (GPT-4o)
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": enhanced_prompt}
    ]
    response = client.chat.completions.create(
        model=self.cot_model,
        messages=messages,
        temperature=0.1,
        max_tokens=4096
    )
```

#### B. Ajouter m√©thode calcul budget thinking (NOUVEAU)

```python
# Ajouter apr√®s ligne 700
def _calculate_thinking_budget(
    self,
    rag_result: Optional[RAGResult],
    query_intent: Optional[Dict[str, Any]]
) -> int:
    """
    Calcule le budget de thinking tokens selon la complexit√© de la requ√™te

    Returns:
        int: Budget entre 2000-16000 tokens
    """
    base_budget = 4000  # Budget de base

    # Facteurs augmentant la complexit√©
    complexity_factors = []

    # 1. Source des donn√©es (RAG = plus complexe)
    if rag_result:
        source = rag_result.get("source", RAGSource.UNKNOWN)
        if source == RAGSource.WEAVIATE:
            complexity_factors.append(2000)  # RAG documents
        elif source == RAGSource.POSTGRESQL:
            complexity_factors.append(1000)  # Donn√©es structur√©es

    # 2. Nombre de documents
    if rag_result and rag_result.get("documents"):
        doc_count = len(rag_result["documents"])
        if doc_count > 5:
            complexity_factors.append(2000)  # Beaucoup de contexte
        elif doc_count > 2:
            complexity_factors.append(1000)

    # 3. Type de requ√™te (intent)
    if query_intent:
        intent_type = query_intent.get("intent_type", "")
        if intent_type in ["comparative", "temporal", "multi_step"]:
            complexity_factors.append(3000)  # Requ√™tes complexes
        elif intent_type in ["calculation", "analysis"]:
            complexity_factors.append(2000)

    # 4. Longueur du contexte
    if rag_result:
        context_length = len(str(rag_result.get("documents", "")))
        if context_length > 5000:
            complexity_factors.append(2000)  # Beaucoup de contexte

    # Calculer budget final
    total_budget = base_budget + sum(complexity_factors)

    # Limiter entre 2000 et 16000
    return min(max(total_budget, 2000), 16000)
```

#### C. Modifier retour de `generate_response()` pour inclure CoT (ligne 479-631)

**Actuellement (ligne 625-631):**
```python
return {
    "answer": final_answer,
    "source": source_label,
    "context_summary": context_summary,
    "documents": rag_result.documents if rag_result else [],
    "metrics": metrics,
    "cache_hit": cache_hit,
}
```

**Modifier en:**
```python
# Line 625-631 (MODIFIED)
result = {
    "answer": final_answer,
    "source": source_label,
    "context_summary": context_summary,
    "documents": rag_result.documents if rag_result else [],
    "metrics": metrics,
    "cache_hit": cache_hit,
}

# NOUVEAU: Ajouter thinking si disponible
if hasattr(response, 'thinking') and response.thinking:
    result["cot_thinking"] = response.thinking
    result["cot_model"] = self.cot_model
    logger.info(f"‚úÖ CoT thinking included ({len(response.thinking)} chars)")

return result
```

#### D. Configuration mod√®le (ligne 70)

**Actuellement:**
```python
self.cot_model = os.getenv("COT_MODEL", "gpt-4o")
```

**Modifier en:**
```python
self.cot_model = os.getenv("COT_MODEL", "claude-3-7-sonnet-20250219")
# Options support√©es:
# - gpt-4o, gpt-4o-2024-08-06 (OpenAI standard)
# - o1-preview, o1-mini (OpenAI reasoning)
# - claude-3-7-sonnet-20250219 (Claude Extended Thinking)
# - claude-sonnet-4-5-20250929 (Claude 4.5 Extended Thinking)
```

---

### 2. **`llm/generation/llm_router.py`** (150 lignes) - PRIORIT√â 2

**R√¥le actuel:**
- Route les requ√™tes vers le meilleur LLM (DeepSeek/Claude/GPT-4o)
- Bas√© sur la complexit√© et la source de donn√©es

**Modifications requises:**

#### A. Ajouter option Extended Thinking (ligne 26-28)

**Actuellement:**
```python
class LLMProvider(str, Enum):
    GPT4O = "gpt4o"
    CLAUDE = "claude"
    DEEPSEEK = "deepseek"
```

**Ajouter:**
```python
class LLMProvider(str, Enum):
    GPT4O = "gpt4o"
    CLAUDE = "claude"
    CLAUDE_EXTENDED = "claude-extended"  # NOUVEAU
    DEEPSEEK = "deepseek"
    O1 = "o1"  # NOUVEAU
```

#### B. Modifier r√®gles de routing (ligne 101-150)

**Ajouter r√®gle pour Extended Thinking:**

```python
def route_query(
    self,
    query: str,
    rag_result: Optional[RAGResult],
    query_intent: Optional[Dict[str, Any]]
) -> LLMProvider:
    """Route query to optimal LLM"""

    # PostgreSQL direct hit ‚Üí DeepSeek (simple)
    if rag_result and rag_result.source == RAGSource.POSTGRESQL:
        if rag_result.documents and rag_result.documents[0].score > 0.9:
            return LLMProvider.DEEPSEEK

    # NOUVEAU: Requ√™tes complexes ‚Üí Claude Extended Thinking
    if query_intent and query_intent.get("intent_type") in [
        "comparative", "temporal", "multi_step", "synthesis"
    ]:
        return LLMProvider.CLAUDE_EXTENDED

    # NOUVEAU: RAG multi-documents ‚Üí Claude Extended Thinking
    if rag_result and rag_result.source == RAGSource.WEAVIATE:
        if rag_result.documents and len(rag_result.documents) >= 3:
            return LLMProvider.CLAUDE_EXTENDED

    # Weaviate RAG ‚Üí Claude standard
    if rag_result and rag_result.source == RAGSource.WEAVIATE:
        return LLMProvider.CLAUDE

    # Default ‚Üí GPT-4o
    return LLMProvider.GPT4O
```

#### C. Mapping mod√®le ‚Üí nom (ligne 40-50)

**Ajouter:**
```python
PROVIDER_TO_MODEL = {
    LLMProvider.GPT4O: "gpt-4o",
    LLMProvider.CLAUDE: "claude-3-5-sonnet-20241022",
    LLMProvider.CLAUDE_EXTENDED: "claude-3-7-sonnet-20250219",  # NOUVEAU
    LLMProvider.DEEPSEEK: "deepseek-chat",
    LLMProvider.O1: "o1-mini",  # NOUVEAU
}
```

---

### 3. **`llm/config/system_prompts.py`** (100 lignes) - PRIORIT√â 3

**R√¥le actuel:**
- Charge `system_prompts.json`
- G√®re prompts hi√©rarchiques (PostgreSQL/Weaviate/General)

**Modifications requises:**

#### A. Ajouter prompts sp√©cifiques Extended Thinking

**Fichier: `llm/config/system_prompts.json`**

**Ajouter section:**
```json
{
  "base_prompts": { ... },
  "specialized_prompts": { ... },
  "hierarchical_rag": { ... },

  "extended_thinking": {
    "general": {
      "en": "You are an expert poultry farming advisor with deep reasoning capabilities. When analyzing this question:\n\n1. Break down the question into components\n2. Consider multiple perspectives and approaches\n3. Evaluate evidence from provided context\n4. Reason through implications and trade-offs\n5. Synthesize a comprehensive answer\n\nUse your extended thinking to provide thorough, well-reasoned responses.",

      "fr": "Vous √™tes un conseiller expert en aviculture avec des capacit√©s de raisonnement avanc√©es. Pour analyser cette question:\n\n1. D√©composez la question en composantes\n2. Consid√©rez plusieurs perspectives et approches\n3. √âvaluez les preuves du contexte fourni\n4. Raisonnez sur les implications et compromis\n5. Synth√©tisez une r√©ponse compl√®te\n\nUtilisez votre r√©flexion approfondie pour fournir des r√©ponses compl√®tes et bien raisonn√©es."
    },

    "comparative": {
      "en": "You are comparing multiple options or approaches. In your extended thinking:\n\n1. Identify key comparison criteria\n2. Analyze strengths and weaknesses of each option\n3. Consider context-specific factors\n4. Evaluate trade-offs systematically\n5. Provide evidence-based recommendations",

      "fr": "Vous comparez plusieurs options ou approches. Dans votre r√©flexion:\n\n1. Identifiez les crit√®res de comparaison cl√©s\n2. Analysez les forces et faiblesses de chaque option\n3. Consid√©rez les facteurs contextuels\n4. √âvaluez les compromis syst√©matiquement\n5. Fournissez des recommandations bas√©es sur les preuves"
    },

    "synthesis": {
      "en": "You are synthesizing information from multiple sources. In your extended thinking:\n\n1. Identify key themes and patterns\n2. Resolve conflicting information\n3. Build coherent understanding\n4. Connect insights across sources\n5. Create comprehensive synthesis",

      "fr": "Vous synth√©tisez des informations de plusieurs sources. Dans votre r√©flexion:\n\n1. Identifiez les th√®mes et motifs cl√©s\n2. R√©solvez les informations contradictoires\n3. Construisez une compr√©hension coh√©rente\n4. Reliez les insights entre les sources\n5. Cr√©ez une synth√®se compl√®te"
    }
  }
}
```

#### B. Modifier `system_prompts.py` pour charger Extended Thinking

**Ajouter m√©thode:**
```python
def get_extended_thinking_prompt(
    self,
    query_type: str = "general",
    language: str = "en"
) -> str:
    """
    Get Extended Thinking specific prompt

    Args:
        query_type: "general", "comparative", "synthesis", etc.
        language: Language code

    Returns:
        Extended Thinking system prompt
    """
    try:
        prompts = self.prompts.get("extended_thinking", {})
        type_prompts = prompts.get(query_type, prompts.get("general", {}))
        return type_prompts.get(language, type_prompts.get("en", ""))
    except Exception as e:
        logger.warning(f"Failed to load Extended Thinking prompt: {e}")
        return ""
```

---

## üì¶ Fichiers suppl√©mentaires (modifications mineures)

### 4. **`llm/requirements.txt`** - V√©rifier version Anthropic

**Actuellement:**
```
anthropic>=0.40.0
```

**V√©rifier que Extended Thinking est support√©:**
```
anthropic>=0.40.0  # Supporte Extended Thinking (ok)
```

Si version trop ancienne, mettre √† jour:
```
anthropic>=0.48.0  # Version avec Extended Thinking stable
```

---

### 5. **`llm/generation/llm_ensemble.py`** - Ajouter Claude Extended dans ensemble

**Ligne 45-60 (AJOUTER option):**

```python
def generate_with_ensemble(
    self,
    query: str,
    context: str,
    mode: str = "BEST_OF_N"  # BEST_OF_N, FUSION, VOTING
) -> Dict[str, Any]:
    """Generate responses from multiple LLMs in parallel"""

    # MODIFIER: Ajouter Claude Extended Thinking
    llm_configs = [
        {"provider": "claude-extended", "model": "claude-3-7-sonnet-20250219"},
        {"provider": "gpt4", "model": "gpt-4o"},
        {"provider": "deepseek", "model": "deepseek-chat"},
    ]

    # ... reste du code ...
```

---

## üóÑÔ∏è Base de donn√©es: Nouvelle colonne pour CoT

### Migration SQL

**Fichier: `backend/sql/migrations/30_add_cot_thinking.sql`**

```sql
-- Add Chain of Thought (CoT) columns to messages table
ALTER TABLE messages
ADD COLUMN IF NOT EXISTS cot_thinking TEXT,
ADD COLUMN IF NOT EXISTS cot_model VARCHAR(100),
ADD COLUMN IF NOT EXISTS cot_tokens INTEGER;

-- Add index for querying CoT messages
CREATE INDEX IF NOT EXISTS idx_messages_cot_model
ON messages(cot_model) WHERE cot_model IS NOT NULL;

-- Comment
COMMENT ON COLUMN messages.cot_thinking IS 'Chain of Thought reasoning from Claude Extended Thinking';
COMMENT ON COLUMN messages.cot_model IS 'Model used for CoT generation (e.g., claude-3-7-sonnet)';
COMMENT ON COLUMN messages.cot_tokens IS 'Number of thinking tokens used';
```

### Modifier backend pour sauvegarder CoT

**Fichier: `backend/app/api/v1/chat.py`** (approximativement ligne 400-500)

**Ajouter lors de la sauvegarde du message:**

```python
# Sauvegarder message assistant
assistant_message = {
    "conversation_id": conversation_id,
    "role": "assistant",
    "content": answer,
    "metadata": {
        "source": result.get("source"),
        "context_summary": result.get("context_summary"),
        # NOUVEAU: Ajouter CoT
        "cot_thinking": result.get("cot_thinking"),
        "cot_model": result.get("cot_model"),
    }
}

# Si CoT disponible, sauvegarder dans colonne d√©di√©e
if result.get("cot_thinking"):
    # Calculer tokens (approximatif: 1 token ‚âà 4 chars)
    cot_tokens = len(result["cot_thinking"]) // 4

    # SQL avec colonnes CoT
    cursor.execute("""
        INSERT INTO messages (
            conversation_id, role, content, metadata,
            cot_thinking, cot_model, cot_tokens
        ) VALUES (%s, %s, %s, %s, %s, %s, %s)
    """, (
        conversation_id,
        "assistant",
        answer,
        json.dumps(assistant_message["metadata"]),
        result["cot_thinking"],
        result["cot_model"],
        cot_tokens
    ))
```

---

## üé® Frontend: Afficher le CoT

### Modifier composant Message

**Fichier: `frontend/app/chat/components/MessageBubble.tsx`** (ou similaire)

**Ajouter section CoT:**

```tsx
interface Message {
  role: "user" | "assistant";
  content: string;
  metadata?: {
    source?: string;
    cot_thinking?: string;
    cot_model?: string;
  };
}

export function MessageBubble({ message }: { message: Message }) {
  const [showCoT, setShowCoT] = useState(false);
  const hasCot = message.metadata?.cot_thinking;

  return (
    <div className="message-bubble">
      {/* Message content */}
      <div className="message-content">
        {message.content}
      </div>

      {/* CoT section (si disponible) */}
      {hasCot && (
        <div className="mt-3 border-t border-gray-200 pt-3">
          <button
            onClick={() => setShowCoT(!showCoT)}
            className="flex items-center gap-2 text-sm text-gray-600 hover:text-gray-800"
          >
            <Brain className="w-4 h-4" />
            {showCoT ? "Masquer" : "Voir"} le raisonnement
            {message.metadata.cot_model && (
              <span className="text-xs text-gray-400">
                ({message.metadata.cot_model})
              </span>
            )}
          </button>

          {showCoT && (
            <div className="mt-2 p-3 bg-blue-50 rounded-lg">
              <div className="text-xs font-semibold text-blue-700 mb-2">
                üß† Chain of Thought (R√©flexion du mod√®le)
              </div>
              <pre className="text-xs text-gray-700 whitespace-pre-wrap font-mono">
                {message.metadata.cot_thinking}
              </pre>
            </div>
          )}
        </div>
      )}
    </div>
  );
}
```

---

## ‚öôÔ∏è Variables d'environnement

### Backend `.env`

**Ajouter:**
```bash
# Claude Extended Thinking
ANTHROPIC_API_KEY=sk-ant-...
COT_MODEL=claude-3-7-sonnet-20250219
ENABLE_EXTENDED_THINKING=true

# Thinking budget (optionnel, d√©faut selon complexit√©)
DEFAULT_THINKING_BUDGET=8000
MAX_THINKING_BUDGET=16000
MIN_THINKING_BUDGET=2000

# Router
ENABLE_LLM_ROUTING=true
DEFAULT_LLM_PROVIDER=claude-extended
```

---

## üìä R√©sum√© des modifications

| Fichier | Lignes modifi√©es | Type de modification | Complexit√© |
|---------|------------------|----------------------|------------|
| `generators.py` | 567-594, 70, 625-631, +100 (nouvelle m√©thode) | Logique g√©n√©ration | ‚≠ê‚≠ê‚≠ê √âlev√©e |
| `llm_router.py` | 26-28, 40-50, 101-150 | Routing | ‚≠ê‚≠ê Moyenne |
| `system_prompts.py` | +30 lignes | Configuration prompts | ‚≠ê Faible |
| `system_prompts.json` | +60 lignes JSON | Prompts Extended Thinking | ‚≠ê Faible |
| `requirements.txt` | 1 ligne | Version Anthropic | ‚≠ê Faible |
| `llm_ensemble.py` | 45-60 | Ajout option ensemble | ‚≠ê‚≠ê Moyenne |
| `30_add_cot_thinking.sql` | Nouveau fichier | Migration DB | ‚≠ê Faible |
| `chat.py` (backend) | +20 lignes | Sauvegarde CoT | ‚≠ê‚≠ê Moyenne |
| `MessageBubble.tsx` | +40 lignes | UI affichage CoT | ‚≠ê‚≠ê Moyenne |

**Total estim√©: ~300 lignes de code ajout√©es/modifi√©es**

---

## üöÄ Ordre d'impl√©mentation recommand√©

### Phase 1: Core (Backend)
1. ‚úÖ Ajouter migration SQL `30_add_cot_thinking.sql`
2. ‚úÖ Modifier `generators.py` (Extended Thinking support)
3. ‚úÖ Ajouter m√©thode `_calculate_thinking_budget()`
4. ‚úÖ Modifier retour `generate_response()` pour inclure CoT
5. ‚úÖ Tester g√©n√©ration avec Claude Extended Thinking

### Phase 2: Routing & Configuration
6. ‚úÖ Modifier `llm_router.py` (nouvelle option CLAUDE_EXTENDED)
7. ‚úÖ Ajouter prompts dans `system_prompts.json`
8. ‚úÖ Modifier `system_prompts.py` (m√©thode `get_extended_thinking_prompt()`)
9. ‚úÖ Configurer variables d'environnement

### Phase 3: Sauvegarde & UI
10. ‚úÖ Modifier `chat.py` backend pour sauvegarder CoT
11. ‚úÖ Cr√©er composant frontend pour afficher CoT
12. ‚úÖ Tester end-to-end

### Phase 4: Optimisations (optionnel)
13. ‚ö†Ô∏è Ajouter Claude Extended dans `llm_ensemble.py`
14. ‚ö†Ô∏è Ajouter m√©triques CoT (tokens, latence)
15. ‚ö†Ô∏è Optimiser calcul budget thinking

---

## üß™ Tests recommand√©s

### Test 1: G√©n√©ration simple
```python
from llm.generation.generators import EnhancedResponseGenerator

generator = EnhancedResponseGenerator()
result = generator.generate_response(
    query="Comment am√©liorer la production d'≈ìufs?",
    rag_result=None,
    language="fr"
)

print("Answer:", result["answer"])
print("CoT:", result.get("cot_thinking", "N/A"))
```

### Test 2: G√©n√©ration avec RAG complexe
```python
# Avec plusieurs documents Weaviate
result = generator.generate_response(
    query="Comparez les syst√®mes d'alimentation pour pondeuses",
    rag_result=rag_result_with_5_docs,
    language="fr"
)

# Devrait utiliser budget thinking √©lev√© (>8000 tokens)
```

### Test 3: Routing
```python
from llm.generation.llm_router import LLMRouter

router = LLMRouter()
provider = router.route_query(
    query="Analyse comparative de 3 races de poules",
    rag_result=complex_rag_result,
    query_intent={"intent_type": "comparative"}
)

assert provider == LLMProvider.CLAUDE_EXTENDED
```

---

## üí∞ Estimation des co√ªts

### Claude 3.7 Sonnet Extended Thinking
- **Input**: $3 / 1M tokens
- **Output**: $15 / 1M tokens
- **Thinking tokens**: Compt√©s comme output (4x plus cher que input)

### Exemple de co√ªt par requ√™te
```
Input: 1000 tokens (prompt + context)     = $0.003
Thinking: 8000 tokens (CoT reasoning)     = $0.120
Output: 500 tokens (r√©ponse finale)       = $0.0075
----------------------------------------------------
Total par requ√™te: ~$0.13
```

**Comparaison:**
- GPT-4o: $0.02 par requ√™te (sans CoT)
- O1-mini: $0.05 par requ√™te (CoT cach√©)
- Claude Extended: $0.13 par requ√™te (CoT visible)

**Recommandation:** Utiliser Extended Thinking pour **requ√™tes complexes uniquement** (via router intelligent).

---

## ‚úÖ Checklist de d√©ploiement

- [ ] Migration SQL ex√©cut√©e
- [ ] Variables d'environnement configur√©es
- [ ] Tests unitaires passent
- [ ] Tests d'int√©gration passent
- [ ] M√©triques Anthropic activ√©es
- [ ] Monitoring des co√ªts configur√©
- [ ] Documentation utilisateur mise √† jour
- [ ] Backend red√©marr√© en production
- [ ] Frontend d√©ploy√©
- [ ] Tests end-to-end en production

---

**Derni√®re mise √† jour:** 2025-10-22
**Auteur:** Claude Code
**Statut:** üìã Plan pr√™t pour impl√©mentation
