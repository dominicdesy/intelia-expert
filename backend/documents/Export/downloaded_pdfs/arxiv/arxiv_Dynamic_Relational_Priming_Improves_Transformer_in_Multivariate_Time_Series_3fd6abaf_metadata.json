{
  "title": "Dynamic Relational Priming Improves Transformer in Multivariate Time\n  Series",
  "authors": [
    "Hunjae Lee",
    "Corey Clark"
  ],
  "abstract": "Standard attention mechanisms in transformers employ static token\nrepresentations that remain unchanged across all pair-wise computations in each\nlayer. This limits their representational alignment with the potentially\ndiverse relational dynamics of each token-pair interaction. While they excel in\ndomains with relatively homogeneous relationships, standard attention's static\nrelational learning struggles to capture the diverse, heterogeneous\ninter-channel dependencies of multivariate time series (MTS) data--where\ndifferent channel-pair interactions within a single system may be governed by\nentirely different physical laws or temporal dynamics. To better align the\nattention mechanism for such domain phenomena, we propose attention with\ndynamic relational priming (prime attention). Unlike standard attention where\neach token presents an identical representation across all of its pair-wise\ninteractions, prime attention tailors each token dynamically (or per\ninteraction) through learnable modulations to best capture the unique\nrelational dynamics of each token pair, optimizing each pair-wise interaction\nfor that specific relationship. This representational plasticity of prime\nattention enables effective extraction of relationship-specific information in\nMTS while maintaining the same asymptotic computational complexity as standard\nattention. Our results demonstrate that prime attention consistently\noutperforms standard attention across benchmarks, achieving up to 6.5\\%\nimprovement in forecasting accuracy. In addition, we find that prime attention\nachieves comparable or superior performance using up to 40\\% less sequence\nlength compared to standard attention, further demonstrating its superior\nrelational modeling capabilities.",
  "pdf_url": "https://arxiv.org/pdf/2509.12196v1.pdf",
  "doi": "",
  "pmcid": "",
  "source": "arxiv",
  "year": "2025",
  "journal": "arXiv preprint",
  "file_path": "downloaded_pdfs\\arxiv\\arxiv_Dynamic_Relational_Priming_Improves_Transformer_in_Multivariate_Time_Series_3fd6abaf.pdf",
  "file_size": 1732291,
  "download_timestamp": 1757991627.877085
}