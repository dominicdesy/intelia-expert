{
  "title": "How to Evaluate Medical AI",
  "authors": [
    "Ilia Kopanichuk",
    "Petr Anokhin",
    "Vladimir Shaposhnikov",
    "Vladimir Makharev",
    "Ekaterina Tsapieva",
    "Iaroslav Bespalov",
    "Dmitry V. Dylov",
    "Ivan Oseledets"
  ],
  "abstract": "The integration of artificial intelligence (AI) into medical diagnostic\nworkflows requires robust and consistent evaluation methods to ensure\nreliability, clinical relevance, and the inherent variability in expert\njudgments. Traditional metrics like precision and recall often fail to account\nfor the inherent variability in expert judgments, leading to inconsistent\nassessments of AI performance. Inter-rater agreement statistics like Cohen's\nKappa are more reliable but they lack interpretability. We introduce Relative\nPrecision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new\nevaluation metrics that compare AI outputs against multiple expert opinions\nrather than a single reference. By normalizing performance against inter-expert\ndisagreement, these metrics provide a more stable and realistic measure of the\nquality of predicted diagnosis. In addition to the comprehensive analysis of\ndiagnostic quality measures, our study contains a very important side result.\nOur evaluation methodology allows us to avoid selecting diagnoses from a\nlimited list when evaluating a given case. Instead, both the models being\ntested and the examiners verifying them arrive at a free-form diagnosis. In\nthis automated methodology for establishing the identity of free-form clinical\ndiagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our\napproach using 360 medical dialogues, comparing multiple large language models\n(LLMs) against a panel of physicians. Large-scale study shows that\ntop-performing models, such as DeepSeek-V3, achieve consistency on par with or\nexceeding expert consensus. Moreover, we demonstrate that expert judgments\nexhibit significant variability - often greater than that between AI and\nhumans. This finding underscores the limitations of any absolute metrics and\nsupports the need to adopt relative metrics in medical AI.",
  "pdf_url": "https://arxiv.org/pdf/2509.11941v1.pdf",
  "doi": "",
  "pmcid": "",
  "source": "arxiv",
  "year": "2025",
  "journal": "arXiv preprint",
  "file_path": "downloaded_pdfs\\arxiv\\arxiv_How_to_Evaluate_Medical_AI_f15d64e5.pdf",
  "file_size": 1606777,
  "download_timestamp": 1757991628.366798
}