{
  "title": "Advancing Medical Artificial Intelligence Using a Century of Cases",
  "authors": [
    "Thomas A. Buckley",
    "Riccardo Conci",
    "Peter G. Brodeur",
    "Jason Gusdorf",
    "Sourik Beltr√°n",
    "Bita Behrouzi",
    "Byron Crowe",
    "Jacob Dockterman",
    "Muzzammil Muhammad",
    "Sarah Ohnigian",
    "Andrew Sanchez",
    "James A. Diao",
    "Aashna P. Shah",
    "Daniel Restrepo",
    "Eric S. Rosenberg",
    "Andrew S. Lea",
    "Marinka Zitnik",
    "Scott H. Podolsky",
    "Zahir Kanjee",
    "Raja-Elie E. Abdulnour",
    "Jacob M. Koshy",
    "Adam Rodman",
    "Arjun K. Manrai"
  ],
  "abstract": "BACKGROUND: For over a century, the New England Journal of Medicine\nClinicopathological Conferences (CPCs) have tested the reasoning of expert\nphysicians and, recently, artificial intelligence (AI). However, prior AI\nevaluations have focused on final diagnoses without addressing the multifaceted\nreasoning and presentation skills required of expert discussants.\n  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),\nwe conducted extensive physician annotation and automated processing to create\nCPC-Bench, a physician-validated benchmark spanning 10 text-based and\nmultimodal tasks, against which we evaluated leading large language models\n(LLMs). Then, we developed \"Dr. CaBot,\" an AI discussant designed to produce\nwritten and slide-based video presentations using only the case presentation,\nmodeling the role of the human expert in these cases.\n  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the\nfinal diagnosis first in 60% of cases and within the top ten in 84% of cases,\noutperforming a 20-physician baseline; next-test selection accuracy reached\n98%. Event-level physician annotations quantified AI diagnostic accuracy per\nunit of information. Performance was lower on literature search and image\ntasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image\nchallenges. In blinded comparisons of CaBot vs. human expert-generated text,\nphysicians misclassified the source of the differential in 46 of 62 (74%) of\ntrials, and scored CaBot more favorably across quality dimensions. To promote\nresearch, we are releasing CaBot and CPC-Bench.\n  CONCLUSIONS: LLMs exceed physician performance on complex text-based\ndifferential diagnosis and convincingly emulate expert medical presentations,\nbut image interpretation and literature retrieval remain weaker. CPC-Bench and\nCaBot may enable transparent and continued tracking of progress in medical AI.",
  "pdf_url": "https://arxiv.org/pdf/2509.12194v1.pdf",
  "doi": "",
  "pmcid": "",
  "source": "arxiv",
  "year": "2025",
  "journal": "arXiv preprint",
  "file_path": "downloaded_pdfs\\arxiv\\arxiv_Advancing_Medical_Artificial_Intelligence_Using_a_Century_of_Cases_e386feb5.pdf",
  "file_size": 4190781,
  "download_timestamp": 1757991631.2410495
}