# -*- coding: utf-8 -*-
# app/main.py - VERSION 4.1 NETTOYEE - ENDPOINTS SELECTIONNES UNIQUEMENT

from __future__ import annotations

import os
import time
import json
import logging
import asyncio
import psutil
import pathlib
from datetime import datetime
from contextlib import asynccontextmanager
from typing import Optional, Dict

from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

# .env (facultatif)
try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:  # Correction E722: Exception sp√©cifique au lieu de bare except
    pass

# === LOGGING GLOBAL ===
logger = logging.getLogger("app.main")
logging.basicConfig(level=os.getenv("LOG_LEVEL", "INFO"))

# === CONFIG CORS ===
ALLOWED_ORIGINS = os.getenv(
    "ALLOWED_ORIGINS",
    "https://expert.intelia.com,https://app.intelia.com,http://localhost:3000",
).split(",")

# ACTIVATION SYNTHESE LLM AU DEMARRAGE (CONSERVE)
synthesis_enabled = str(os.getenv("ENABLE_SYNTH_PROMPT", "0")).lower() in (
    "1",
    "true",
    "yes",
    "on",
)
if synthesis_enabled:
    logger.info("‚úÖ Synthese LLM activee (ENABLE_SYNTH_PROMPT=1)")
else:
    logger.info("‚ÑπÔ∏è Synthese LLM desactivee (ENABLE_SYNTH_PROMPT=0)")

# ========== VARIABLES GLOBALES DE MONITORING (RESTAUREES) ==========
request_counter = 0
error_counter = 0
start_time = time.time()
active_requests = 0

# Variables pour le systeme de cache statistiques
stats_scheduler_task = None
cache_update_counter = 0
cache_error_counter = 0

# === DETECTION DU SYSTEME DE CACHE STATISTIQUES ===
STATS_CACHE_AVAILABLE = False
try:
    from app.api.v1.stats_cache import get_stats_cache
    from app.api.v1.stats_updater import run_update_cycle

    STATS_CACHE_AVAILABLE = True
    logger.info("‚úÖ Systeme de cache statistiques importe avec succes")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Systeme de cache statistiques non disponible: {e}")
    logger.info("‚ÑπÔ∏è L'application fonctionnera normalement sans le cache optimise")


# === NOUVEAU: CALCUL MEMOIRE CONTENEUR PRECIS ===
def _read_int_safe(path: str) -> Optional[int]:
    """Lit un entier depuis un fichier cgroup de maniere securisee"""
    try:
        content = pathlib.Path(path).read_text().strip()
        if content == "" or content.lower() == "max":
            return None
        return int(content)
    except (FileNotFoundError, ValueError, PermissionError, OSError):
        return None


def get_container_memory_percent() -> float:
    """
    Retourne le pourcentage REEL d'utilisation memoire du conteneur.
    Priorite aux limites cgroup qui refletent les quotas du conteneur.
    """
    # cgroup v2 (moderne - Docker/Kubernetes recents)
    current_v2 = _read_int_safe("/sys/fs/cgroup/memory.current")
    max_v2 = _read_int_safe("/sys/fs/cgroup/memory.max")

    if current_v2 is not None and max_v2 is not None and max_v2 > 0:
        percent = (current_v2 / max_v2) * 100
        return round(percent, 1)

    # cgroup v1 (legacy mais encore utilise)
    usage_v1 = _read_int_safe("/sys/fs/cgroup/memory/memory.usage_in_bytes")
    limit_v1 = _read_int_safe("/sys/fs/cgroup/memory/memory.limit_in_bytes")

    if usage_v1 is not None and limit_v1 is not None and limit_v1 > 0:
        # Verifier que la limite n'est pas artificielle (> 100TB = pas de limite reelle)
        if limit_v1 < (100 * 1024 * 1024 * 1024 * 1024):  # 100TB
            percent = (usage_v1 / limit_v1) * 100
            return round(percent, 1)

    # Fallback : calcul psutil manuel (pour developpement local)
    memory = psutil.virtual_memory()
    percent = (memory.used / memory.total) * 100
    return round(percent, 1)


# === OUTILS SUPPLEMENTAIRES ===
def get_rag_paths() -> Dict[str, str]:
    """TOUS LES RAG : Global + Broiler + Layer"""
    base_path = "/workspace/backend/rag_index"
    return {
        "global": f"{base_path}/global",
        "broiler": f"{base_path}/broiler",
        "layer": f"{base_path}/layer",
    }


# === FONCTION AMELIOREE DE DETECTION RAG ===
def is_rag_functional(embedder) -> bool:
    """
    Detecte si un RAG embedder est fonctionnel.
    Essaie plusieurs methodes de verification.
    """
    if embedder is None:
        return False

    try:
        # Methode 1: has_search_engine (si disponible)
        if hasattr(embedder, "has_search_engine"):
            if embedder.has_search_engine():
                return True

        # Methode 2: Verifier l'index FAISS
        if hasattr(embedder, "index") and embedder.index is not None:
            return True

        # Methode 3: Verifier les stats
        if hasattr(embedder, "get_index_stats"):
            stats = embedder.get_index_stats()
            if stats and stats.get("faiss_total", 0) > 0:
                return True

        # Methode 4: Essai de recherche simple
        try:
            results = embedder.search("test", top_k=1)
            return bool(results)
        except Exception:  # Correction E722: Exception sp√©cifique
            pass

        return False
    except Exception as e:
        logger.debug(f"Erreur detection RAG: {e}")
        return False


# === TACHES PERIODIQUES ===
async def periodic_monitoring():
    """Monitoring periodique des performances serveur avec logging en base"""
    while True:
        try:
            await asyncio.sleep(300)  # Toutes les 5 minutes

            # Calcul des metriques
            current_time = time.time()
            uptime_hours = (current_time - start_time) / 3600
            requests_per_minute = (
                request_counter / (uptime_hours * 60) if uptime_hours > 0 else 0
            )
            error_rate_percent = (error_counter / max(request_counter, 1)) * 100

            # Metriques systeme avec calcul memoire conteneur precis
            cpu_percent = psutil.cpu_percent(interval=1)
            memory_percent = get_container_memory_percent()  # Calcul precis

            # Determiner le status de sante
            if error_rate_percent > 10 or cpu_percent > 90 or memory_percent > 90:
                health_status = "critical"
            elif error_rate_percent > 5 or cpu_percent > 70 or memory_percent > 70:
                health_status = "degraded"
            else:
                health_status = "healthy"

            # Log des metriques serveur dans la base
            try:
                from app.api.v1.logging import get_analytics_manager

                analytics = get_analytics_manager()

                # Calculer l'heure tronquee pour le groupement
                current_hour = datetime.now().replace(minute=0, second=0, microsecond=0)

                # Log des metriques
                analytics.log_server_performance(
                    timestamp_hour=current_hour,
                    total_requests=request_counter,
                    successful_requests=request_counter - error_counter,
                    failed_requests=error_counter,
                    avg_response_time_ms=int(250),
                    health_status=health_status,
                    error_rate_percent=error_rate_percent,
                )

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur logging metriques en base: {e}")

            # Ajouter les metriques du cache
            cache_info = ""
            if STATS_CACHE_AVAILABLE:
                cache_info = f", cache: {cache_update_counter} updates, {cache_error_counter} erreurs"

            logger.info(
                f"üìä Metriques: {requests_per_minute:.1f} req/min, "
                f"erreurs: {error_rate_percent:.1f}%, "
                f"CPU: {cpu_percent:.1f}%, RAM: {memory_percent:.1f}%, "
                f"sante: {health_status}{cache_info}"
            )

        except Exception as e:
            logger.error(f"‚è∞ Erreur monitoring periodique: {e}")
            await asyncio.sleep(60)  # Retry dans 1 minute en cas d'erreur


async def periodic_stats_update():
    """Mise a jour periodique du cache statistiques toutes les heures"""
    global cache_update_counter, cache_error_counter

    if not STATS_CACHE_AVAILABLE:
        logger.warning("‚ö†Ô∏è Cache statistiques non disponible - arret scheduler")
        return

    # Attendre 5 minutes avant la premiere mise a jour
    await asyncio.sleep(300)

    # Premiere mise a jour au demarrage
    logger.info("üöÄ Lancement premiere mise a jour cache statistiques au demarrage")
    try:
        result = await run_update_cycle()
        if result.get("status") == "completed":
            cache_update_counter += 1
            logger.info("‚úÖ Premiere mise a jour cache reussie au demarrage")
        else:
            cache_error_counter += 1
            logger.warning(
                f"‚ö†Ô∏è Premiere mise a jour cache echouee: {result.get('error', 'Unknown')}"
            )
    except Exception as e:
        cache_error_counter += 1
        logger.error(f"‚ö∞ Erreur premiere mise a jour cache: {e}")

    # Boucle principale - mise a jour toutes les heures
    while True:
        try:
            await asyncio.sleep(3600)  # 1 heure

            logger.info("üîÑ Debut mise a jour periodique cache statistiques")
            start_update = time.time()

            result = await run_update_cycle()
            update_duration = (time.time() - start_update) * 1000  # en ms

            if result.get("status") == "completed":
                cache_update_counter += 1
                successful = result.get("successful_updates", 0)
                total = result.get("total_updates", 0)
                duration = result.get("duration_ms", update_duration)

                logger.info(
                    f"‚úÖ Cache mis a jour: {successful}/{total} succes en {duration:.0f}ms"
                )

                errors = result.get("errors", [])
                if errors:
                    logger.warning(f"‚ö†Ô∏è Erreurs durant la mise a jour: {errors}")

            elif result.get("status") == "failed":
                cache_error_counter += 1
                error_msg = result.get("error", "Erreur inconnue")
                logger.error(f"‚ö∞ Mise a jour cache echouee: {error_msg}")

            else:
                cache_error_counter += 1
                logger.warning(f"‚ö†Ô∏è Mise a jour cache statut inattendu: {result}")

        except Exception as e:
            cache_error_counter += 1
            logger.error(f"‚ö∞ Erreur durant mise a jour periodique cache: {e}")
            await asyncio.sleep(600)  # Attendre 10 minutes avant retry


# === LIFESPAN COMPLET AVEC DETECTION RAG AMELIOREE ===
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ========== INITIALISATION AU DEMARRAGE ==========
    logger.info("üöÄ Demarrage de l'application Expert API avec systeme complet")

    # ========== INITIALISATION DES SERVICES ==========
    try:
        logger.info("üìä Initialisation des services analytics et facturation...")
        database_url = os.getenv("DATABASE_URL")
        if database_url:
            logger.info("‚úÖ DATABASE_URL configuree")

            # Analytics
            try:
                from app.api.v1.logging import get_analytics

                analytics_status = get_analytics()
                logger.info(
                    f"‚úÖ Service analytics: {analytics_status.get('status', 'unknown')}"
                )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Service analytics partiellement disponible: {e}")

            # Billing
            try:
                from app.api.v1.billing import get_billing_manager

                billing = get_billing_manager()
                logger.info(f"‚úÖ Service billing: {len(billing.plans)} plans charges")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Service billing partiellement disponible: {e}")

            # Cache statistiques (optionnel)
            if STATS_CACHE_AVAILABLE:
                try:
                    cache = get_stats_cache()
                    if hasattr(cache, "get_cache_stats"):
                        cache_stats = cache.get_cache_stats()
                    elif hasattr(cache, "get_stats"):
                        cache_stats = cache.get_stats()
                    else:
                        cache_stats = "cache initialise"
                    logger.info(f"‚úÖ Cache statistiques initialise: {cache_stats}")
                except Exception as e:
                    logger.warning(
                        f"‚ö†Ô∏è Cache statistiques partiellement disponible: {e}"
                    )

            # Nettoyage sessions
            try:
                from app.api.v1.pipeline.postgres_memory import PostgresMemory

                memory = PostgresMemory()
                cleaned = memory.cleanup_old_sessions(days_old=7)
                if cleaned > 0:
                    logger.info(f"üßπ {cleaned} anciennes sessions nettoyees")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Nettoyage sessions echoue: {e}")
        else:
            logger.warning("‚ö†Ô∏è DATABASE_URL manquante - services analytics desactives")
    except Exception as e:
        logger.error(f"‚è∞ Erreur initialisation services: {e}")

    # ========== SUPABASE ==========
    app.state.supabase = None
    try:
        from supabase import create_client

        url, key = os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_ANON_KEY")
        if url and key:
            app.state.supabase = create_client(url, key)
            logger.info("‚úÖ Supabase pret")
        else:
            logger.info("‚ÑπÔ∏è Supabase non configure")
    except Exception as e:
        logger.warning("‚ÑπÔ∏è Supabase indisponible: %s", e)

    # CHARGEMENT DES 3 RAG AVEC DETECTION AMELIOREE (OPTIONNEL)
    app.state.rag = None
    app.state.rag_broiler = None
    app.state.rag_layer = None
    total_rags = 0  # Initialiser par defaut

    try:
        from rag.embedder import FastRAGEmbedder

        rag_paths = get_rag_paths()
        logger.info(f"üîç Chargement des 3 RAG: {list(rag_paths.keys())}")

        def _load_rag(name: str, path: str):
            """Fonction helper pour charger un RAG"""
            logger.info(f"üîç Chargement RAG {name.capitalize()}: {path}")

            if not os.path.exists(path):
                logger.warning(f"‚ö†Ô∏è RAG {name.capitalize()}: Chemin inexistant {path}")
                return None

            try:
                embedder = FastRAGEmbedder(
                    index_dir=path,
                    debug=False,
                    cache_embeddings=True,
                    max_workers=2,
                )

                if is_rag_functional(embedder):
                    logger.info(f"‚úÖ RAG {name.capitalize()} charge: {path}")
                    return embedder
                else:
                    logger.warning(
                        f"‚ö†Ô∏è RAG {name.capitalize()}: Instance creee mais non fonctionnelle"
                    )
                    return None

            except Exception as e:
                logger.exception(f"üí• RAG {name.capitalize()} init failed: {e}")
                return None

        # Chargement des 3 RAG
        app.state.rag = _load_rag("global", rag_paths["global"])
        app.state.rag_broiler = _load_rag("broiler", rag_paths["broiler"])
        app.state.rag_layer = _load_rag("layer", rag_paths["layer"])

        total_rags = sum(
            1
            for rag in [app.state.rag, app.state.rag_broiler, app.state.rag_layer]
            if rag
        )
        logger.info(f"üìä Status final des RAG: {total_rags}/3 charges")

    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Module RAG non disponible: {e}")
        logger.info("‚ÑπÔ∏è L'application fonctionnera sans le systeme RAG")
        total_rags = 0
    except Exception as e:
        logger.error("‚è∞ Erreur critique initialisation RAG: %s", e)
        total_rags = 0

    # ========== DEMARRAGE DU MONITORING & SCHEDULER ==========
    monitoring_task = None
    global stats_scheduler_task

    try:
        monitoring_task = asyncio.create_task(periodic_monitoring())
        logger.info("üìä Monitoring periodique demarre")
    except Exception as e:
        logger.error(f"‚è∞ Erreur demarrage monitoring: {e}")

    if STATS_CACHE_AVAILABLE:
        try:
            stats_scheduler_task = asyncio.create_task(periodic_stats_update())
            logger.info("üîÑ Scheduler cache statistiques demarre")
        except Exception as e:
            logger.error(f"‚ö∞ Erreur demarrage scheduler cache: {e}")
    else:
        logger.info("‚ÑπÔ∏è Scheduler cache statistiques desactive")

    # ========== L'APPLICATION DEMARRE ==========
    system_features = []
    if STATS_CACHE_AVAILABLE:
        system_features.append("cache statistiques optimise")
    system_features.extend(
        [f"{total_rags}/3 RAG", "monitoring", "analytics", "billing"]
    )
    logger.info(f"üéØ Application Expert API prete avec: {', '.join(system_features)}")
    yield  # --- L'APPLICATION FONCTIONNE ---

    # ========== NETTOYAGE A L'ARRET ==========
    logger.info("üõë Arret de l'application Expert API")

    if monitoring_task:
        try:
            monitoring_task.cancel()
            await asyncio.gather(monitoring_task, return_exceptions=True)
            logger.info("üìä Monitoring periodique arrete")
        except Exception as e:
            logger.error(f"‚è∞ Erreur arret monitoring: {e}")

    if stats_scheduler_task:
        try:
            stats_scheduler_task.cancel()
            await asyncio.gather(stats_scheduler_task, return_exceptions=True)
            logger.info("üîÑ Scheduler cache statistiques arrete")
        except Exception as e:
            logger.error(f"‚ö∞ Erreur arret scheduler cache: {e}")

    # Statistiques finales
    uptime_hours = (time.time() - start_time) / 3600
    final_stats = (
        f"{request_counter} requetes en {uptime_hours:.1f}h, {error_counter} erreurs"
    )

    if STATS_CACHE_AVAILABLE and cache_update_counter > 0:
        cache_success_rate = (
            cache_update_counter / max(cache_update_counter + cache_error_counter, 1)
        ) * 100
        final_stats += f", cache: {cache_update_counter} mises a jour ({cache_success_rate:.1f}% succes)"

    logger.info(f"üìà Statistiques finales: {final_stats}")


# === FASTAPI APP ===
app = FastAPI(
    title="Intelia Expert API",
    version="4.1.0",
    root_path="/api",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
    lifespan=lifespan,
)

# === MIDDLEWARE CORS CORRIGE ===
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://expert.intelia.com",
        "http://localhost:3000",
        "http://localhost:8080",
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"],
    allow_headers=[
        "Accept",
        "Accept-Language",
        "Content-Language",
        "Content-Type",
        "Authorization",
        "X-Requested-With",
        "Origin",
        "Access-Control-Request-Method",
        "Access-Control-Request-Headers",
        "X-Session-ID",
    ],
    expose_headers=[
        "Access-Control-Allow-Origin",
        "Access-Control-Allow-Credentials",
        "Access-Control-Allow-Methods",
        "Access-Control-Allow-Headers",
    ],
)


# === MIDDLEWARE DE MONITORING DES REQUETES ===
@app.middleware("http")
async def monitoring_middleware(request: Request, call_next):
    """Middleware pour tracker les performances en temps reel"""
    global request_counter, error_counter, active_requests

    start_time_req = time.time()
    active_requests += 1
    request_counter += 1

    try:
        response = await call_next(request)

        if response.status_code >= 400:
            error_counter += 1

        return response

    except Exception as e:
        error_counter += 1
        logger.error(f"‚è∞ Erreur dans middleware monitoring: {e}")
        raise

    finally:
        active_requests -= 1
        processing_time = (time.time() - start_time_req) * 1000

        if processing_time > 5000:  # Plus de 5 secondes
            logger.warning(
                f"üåÄ Requete lente: {request.method} {request.url.path} - {processing_time:.0f}ms"
            )


# === MIDDLEWARE D'AUTHENTIFICATION ===
try:
    from app.middleware.auth_middleware import auth_middleware

    app.middleware("http")(auth_middleware)
    logger.info("‚úÖ Middleware d'authentification active")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Middleware d'authentification non disponible: {e}")
except Exception as e:
    logger.error(f"‚è∞ Erreur lors de l'activation du middleware d'auth: {e}")


# === ENDPOINTS CHAT DIRECTS (HORS V1) ===
@app.post("/chat/stream", tags=["Chat"])
async def chat_stream_direct(request: Request):
    """Endpoint chat direct - proxy vers service LLM externe avec logging"""
    start_time = time.time()
    user_question = ""
    user_email = ""
    session_id = ""

    try:
        import httpx

        # Lire et parser le corps de la requete
        body_bytes = await request.body()

        # Parser le JSON pour transformation et extraction des donnees
        try:
            body_json = json.loads(body_bytes.decode("utf-8"))

            # Extraire les donnees pour le logging
            user_question = body_json.get(
                "message_preview", body_json.get("question", "")
            )
            session_id = body_json.get("session_id", "")

            # Extraire user_email depuis l'auth header si possible
            auth_header = request.headers.get("Authorization")
            if auth_header:
                try:
                    from app.api.v1.auth import get_current_user_from_token

                    user_info = await get_current_user_from_token(
                        auth_header.replace("Bearer ", "")
                    )
                    user_email = user_info.get("email", "")
                except Exception:  # Correction E722
                    user_email = "anonymous"

            # Transformation des donn√©es pour le service LLM
            if "message_preview" in body_json and "message" not in body_json:
                body_json["message"] = body_json.pop("message_preview")
                logger.info("Transformation: message_preview -> message")

            if "question" in body_json and "message" not in body_json:
                body_json["message"] = body_json.pop("question")
                logger.info("Transformation: question -> message")

            # Reconvertir en bytes
            body_bytes = json.dumps(body_json).encode("utf-8")
            logger.info(f"Question extraite: {user_question[:100]}...")

        except (json.JSONDecodeError, UnicodeDecodeError):
            logger.warning(
                "Impossible de parser/transformer le body JSON, envoi tel quel"
            )

        # Headers a transferer
        headers = {
            "Content-Type": "application/json",
            "X-Frontend-Origin": "intelia",
            "User-Agent": "Intelia-Expert-Proxy/1.0",
            "Accept": "*/*",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }

        # Transferer tous les headers importants du frontend
        for header_name in [
            "Authorization",
            "Cookie",
            "X-Session-ID",
            "X-User-ID",
            "X-Tenant-ID",
        ]:
            header_value = request.headers.get(header_name)
            if header_value:
                headers[header_name] = header_value

        # Transferer l'origine si presente
        origin = request.headers.get("Origin")
        if origin:
            headers["Origin"] = origin
            headers["Referer"] = origin

        # URL du service LLM externe
        target_url = "https://expert.intelia.com/llm/chat/stream"

        # Log pour debug
        logger.info(f"Proxy chat vers {target_url}")

        # Proxy vers le service LLM externe
        async with httpx.AsyncClient(follow_redirects=True) as client:
            response = await client.post(
                target_url, content=body_bytes, headers=headers, timeout=60.0
            )

            logger.info(f"Reponse LLM service: {response.status_code}")

            if response.status_code == 200:
                # Collecter la r√©ponse pour le logging
                response_chunks = []

                async def response_collector():
                    async for chunk in response.aiter_text():
                        response_chunks.append(chunk)
                        yield chunk

                # Streamer la r√©ponse en collectant le contenu
                from fastapi.responses import StreamingResponse

                stream_response = StreamingResponse(
                    response_collector(),
                    media_type="text/event-stream",
                    headers={
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive",
                        "Access-Control-Allow-Origin": request.headers.get(
                            "Origin", "*"
                        ),
                        "Access-Control-Allow-Credentials": "true",
                    },
                )

                # Programmer l'enregistrement en arri√®re-plan apr√®s le streaming
                async def log_interaction_async():
                    try:
                        # Reconstituer la r√©ponse compl√®te
                        full_response = "".join(response_chunks)

                        # Extraire le texte de la r√©ponse depuis les √©v√©nements SSE
                        extracted_response = ""
                        for line in full_response.split("\n"):
                            if line.startswith("data: "):
                                try:
                                    event_data = json.loads(
                                        line[6:]
                                    )  # Enlever "data: "
                                    if event_data.get("type") == "final":
                                        extracted_response = event_data.get(
                                            "answer", ""
                                        )
                                        break
                                    elif event_data.get("type") == "delta":
                                        extracted_response += event_data.get("text", "")
                                except json.JSONDecodeError:
                                    continue

                        # Enregistrer dans la base de donn√©es
                        await log_user_question_async(
                            user_email=user_email or "anonymous",
                            question=user_question,
                            response_text=extracted_response,
                            response_source="llm_service",
                            processing_time_ms=int((time.time() - start_time) * 1000),
                            session_id=session_id,
                            language=(
                                body_json.get("lang", "fr")
                                if "body_json" in locals()
                                else "fr"
                            ),
                        )

                        logger.info(
                            f"Question enregistr√©e: {user_email} -> {len(user_question)} chars question, {len(extracted_response)} chars response"
                        )

                    except Exception as log_error:
                        logger.error(f"Erreur enregistrement question: {log_error}")

                # Lancer l'enregistrement en arri√®re-plan
                asyncio.create_task(log_interaction_async())

                return stream_response

            else:
                # Enregistrer l'erreur aussi
                processing_time = int((time.time() - start_time) * 1000)
                try:
                    error_content = response.text
                    await log_user_question_async(
                        user_email=user_email or "anonymous",
                        question=user_question,
                        response_text=f"Erreur LLM: {response.status_code}",
                        response_source="llm_service_error",
                        processing_time_ms=processing_time,
                        session_id=session_id,
                        status="error",
                    )
                except Exception as log_error:
                    logger.error(f"Erreur enregistrement erreur: {log_error}")

                logger.error(
                    f"LLM service error {response.status_code}: {error_content}"
                )

                return JSONResponse(
                    status_code=response.status_code,
                    content={
                        "detail": f"LLM service error: {response.status_code}",
                        "upstream_error": (
                            response.text if response.text else "No details"
                        ),
                    },
                )

    except Exception as e:
        # Enregistrer l'erreur proxy aussi
        processing_time = int((time.time() - start_time) * 1000)
        try:
            await log_user_question_async(
                user_email=user_email or "anonymous",
                question=user_question,
                response_text=f"Erreur proxy: {str(e)}",
                response_source="proxy_error",
                processing_time_ms=processing_time,
                session_id=session_id,
                status="error",
            )
        except Exception as log_error:
            logger.error(f"Erreur enregistrement erreur proxy: {log_error}")

        logger.error(f"Erreur proxy chat direct: {e}")
        return JSONResponse(
            status_code=500, content={"detail": f"Proxy error: {str(e)}"}
        )


# Fonction utilitaire pour l'enregistrement asynchrone
async def log_user_question_async(
    user_email: str,
    question: str,
    response_text: str,
    response_source: str = "llm_service",
    processing_time_ms: int = 0,
    session_id: str = "",
    language: str = "fr",
    status: str = "completed",
):
    """Enregistre une question/r√©ponse de mani√®re asynchrone"""
    try:
        from app.api.v1.logging import get_analytics_manager

        analytics = get_analytics_manager()

        # Utiliser la m√©thode log_user_question si elle existe
        if hasattr(analytics, "log_user_question"):
            analytics.log_user_question(
                user_email=user_email,
                question=question,
                response_text=response_text,
                response_source=response_source,
                response_confidence=0.8,  # Score par d√©faut pour LLM
                processing_time_ms=processing_time_ms,
                session_id=session_id,
                language=language,
                status=status,
            )
        else:
            logger.warning("M√©thode log_user_question non disponible dans analytics")

    except Exception as e:
        logger.error(f"Erreur log_user_question_async: {e}")


@app.get("/chat/health", tags=["Chat"])
async def chat_health_direct():
    """Health check du service LLM externe"""
    try:
        import httpx

        target_url = "https://expert.intelia.com/llm/health"

        async with httpx.AsyncClient() as client:
            response = await client.get(target_url, timeout=10.0)
            return response.json()

    except Exception as e:
        return {"ok": False, "error": str(e)}


# === MONTAGE DES ROUTERS ===
try:
    from app.api.v1 import router as api_v1_router

    app.include_router(api_v1_router)
    logger.info("‚úÖ Router API v1 charge depuis __init__.py")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Impossible de charger le router v1 depuis __init__.py: {e}")
    logger.info("üîß Creation d'un router v1 temporaire avec endpoints selectionnes...")

    from fastapi import APIRouter

    temp_v1_router = APIRouter(prefix="/v1", tags=["v1"])

    # === ENDPOINTS SELECTIONNES UNIQUEMENT ===

    # Conversations router
    try:
        from app.api.v1.conversations import router as conversations_router

        temp_v1_router.include_router(
            conversations_router, prefix="/conversations", tags=["conversations"]
        )
        logger.info("‚úÖ Conversations router ajoute")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Conversations router non disponible: {e}")

    # Auth router
    try:
        from app.api.v1.auth import router as auth_router

        temp_v1_router.include_router(auth_router, prefix="", tags=["auth"])
        logger.info("‚úÖ Auth router ajoute")
    except ImportError as e:
        logger.error(f"‚è∞ Auth router non disponible: {e}")

    # Admin router
    try:
        from app.api.v1.admin import router as admin_router

        temp_v1_router.include_router(admin_router, tags=["admin"])
        logger.info("‚úÖ Admin router ajoute")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Admin router non disponible: {e}")

    # Health router
    try:
        from app.api.v1.health import router as health_router

        temp_v1_router.include_router(health_router, tags=["health"])
        logger.info("‚úÖ Health router ajoute")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Health router non disponible: {e}")

    # System router
    try:
        from app.api.v1.system import router as system_router

        temp_v1_router.include_router(system_router, tags=["system"])
        logger.info("‚úÖ System router ajoute")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è System router non disponible: {e}")

    # Users router
    try:
        from app.api.v1.users import router as users_router

        temp_v1_router.include_router(users_router, tags=["users"])
        logger.info("‚úÖ Users router ajoute")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Users router non disponible: {e}")

    # Invitations router
    try:
        from app.api.v1.invitations import router as invitations_router

        temp_v1_router.include_router(invitations_router, tags=["invitations"])
        logger.info("‚úÖ Invitations router ajoute")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Invitations router non disponible: {e}")

    # Logging router
    try:
        from app.api.v1.logging import router as logging_router

        temp_v1_router.include_router(logging_router, tags=["logging"])
        logger.info("‚úÖ Logging router ajoute")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Logging router non disponible: {e}")

    # Billing router
    try:
        from app.api.v1.billing import router as billing_router

        temp_v1_router.include_router(billing_router, tags=["billing"])
        logger.info("‚úÖ Billing router ajoute")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Billing router non disponible: {e}")

    # Billing OpenAI router
    try:
        from app.api.v1.billing_openai import router as billing_openai_router

        temp_v1_router.include_router(
            billing_openai_router, prefix="/billing", tags=["billing-openai"]
        )
        logger.info("‚úÖ Billing OpenAI router ajoute")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Billing OpenAI router non disponible: {e}")

    # Stats Fast router
    try:
        from app.api.v1.stats_fast import router as stats_fast_router

        temp_v1_router.include_router(stats_fast_router, tags=["statistics-fast"])
        logger.info("‚úÖ Stats Fast router ajoute")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Stats Fast router non disponible: {e}")

    # Stats Admin router
    if STATS_CACHE_AVAILABLE:
        try:
            from app.api.v1.stats_admin import router as stats_admin_router

            temp_v1_router.include_router(stats_admin_router, tags=["statistics-admin"])
            logger.info("‚úÖ Stats Admin router ajoute")
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è Stats Admin router non disponible: {e}")

    # Monter le router temporaire
    app.include_router(temp_v1_router)
    logger.info("‚úÖ Router v1 temporaire monte avec endpoints selectionnes")


# === ENDPOINTS PRINCIPAUX ===
@app.get("/health/complete", tags=["Health"])
async def complete_health_check():
    """Check de sante complet du systeme"""
    try:
        health_status = {
            "timestamp": datetime.utcnow().isoformat(),
            "status": "healthy",
            "components": {},
        }

        # Check base de donnees et analytics
        try:
            from app.api.v1.logging import get_analytics_manager

            get_analytics_manager()
            health_status["components"]["analytics"] = {
                "status": "healthy",
                "type": "postgresql",
                "tables_created": True,
            }
        except Exception as e:
            health_status["components"]["analytics"] = {
                "status": "unhealthy",
                "error": str(e),
            }
            health_status["status"] = "degraded"

        # Check systeme de facturation
        try:
            from app.api.v1.billing import get_billing_manager

            billing = get_billing_manager()
            health_status["components"]["billing"] = {
                "status": "healthy",
                "plans_loaded": len(billing.plans),
                "quota_enforcement": True,
            }
        except Exception as e:
            health_status["components"]["billing"] = {
                "status": "unhealthy",
                "error": str(e),
            }
            health_status["status"] = "degraded"

        # Check systeme de cache statistiques
        if STATS_CACHE_AVAILABLE:
            try:
                cache = get_stats_cache()
                if hasattr(cache, "get_cache_stats"):
                    cache_stats = cache.get_cache_stats()
                else:
                    cache_stats = "disponible"

                scheduler_active = (
                    stats_scheduler_task is not None and not stats_scheduler_task.done()
                )
                cache_success_rate = 100
                if cache_update_counter + cache_error_counter > 0:
                    cache_success_rate = (
                        cache_update_counter
                        / (cache_update_counter + cache_error_counter)
                    ) * 100

                health_status["components"]["statistics_cache"] = {
                    "status": (
                        "healthy" if cache_stats and scheduler_active else "degraded"
                    ),
                    "scheduler_active": scheduler_active,
                    "cache_updates": cache_update_counter,
                    "cache_errors": cache_error_counter,
                    "success_rate": round(cache_success_rate, 1),
                    "cache_entries": cache_stats,
                }
            except Exception as e:
                health_status["components"]["statistics_cache"] = {
                    "status": "unhealthy",
                    "error": str(e),
                }
                health_status["status"] = "degraded"
        else:
            health_status["components"]["statistics_cache"] = {
                "status": "not_available",
                "message": "Module cache statistiques non importe",
            }

        # Check RAG
        total_rags = sum(
            1
            for rag in [
                getattr(app.state, "rag", None),
                getattr(app.state, "rag_broiler", None),
                getattr(app.state, "rag_layer", None),
            ]
            if rag and is_rag_functional(rag)
        )

        health_status["components"]["rag"] = {
            "status": (
                "healthy"
                if total_rags >= 2
                else "degraded" if total_rags >= 1 else "unhealthy"
            ),
            "loaded_rags": total_rags,
            "total_rags": 3,
        }

        # Check OpenAI
        openai_key = os.getenv("OPENAI_API_KEY")
        health_status["components"]["openai"] = {
            "status": "configured" if openai_key else "not_configured"
        }

        # Check Auth system
        try:
            jwt_secret = os.getenv("JWT_SECRET") or os.getenv("SUPABASE_JWT_SECRET")
            supabase_url = os.getenv("SUPABASE_URL")
            supabase_key = os.getenv("SUPABASE_ANON_KEY")

            auth_status = "healthy"
            if not (jwt_secret and supabase_url and supabase_key):
                auth_status = "partially_configured"
            if not jwt_secret:
                auth_status = "not_configured"

            health_status["components"]["auth"] = {
                "status": auth_status,
                "has_jwt_secret": bool(jwt_secret),
                "has_supabase_config": bool(supabase_url and supabase_key),
                "routing_fixed": True,
            }
        except Exception as e:
            health_status["components"]["auth"] = {"status": "error", "error": str(e)}

        # Metriques systeme avec calcul memoire conteneur
        uptime_hours = (time.time() - start_time) / 3600
        health_status["metrics"] = {
            "uptime_hours": round(uptime_hours, 2),
            "total_requests": request_counter,
            "error_rate_percent": round(
                (error_counter / max(request_counter, 1)) * 100, 2
            ),
            "active_requests": active_requests,
            "cache_updates": cache_update_counter,
            "cache_errors": cache_error_counter,
            "scheduler_active": (
                stats_scheduler_task is not None and not stats_scheduler_task.done()
                if STATS_CACHE_AVAILABLE
                else False
            ),
            "memory_percent_container": get_container_memory_percent(),
        }

        # Determiner le statut global
        component_statuses = [
            comp["status"]
            for comp in health_status["components"].values()
            if comp["status"] in ["healthy", "degraded", "unhealthy"]
        ]

        if any(status == "unhealthy" for status in component_statuses):
            health_status["status"] = "unhealthy"
        elif any(status == "degraded" for status in component_statuses):
            health_status["status"] = "degraded"

        return health_status

    except Exception as e:
        logger.error(f"‚è∞ Erreur health check: {e}")
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "status": "error",
            "error": str(e),
        }


@app.get("/", tags=["Root"])
async def root():
    def rag_status() -> str:
        total_rags = sum(
            1
            for rag in [
                getattr(app.state, "rag", None),
                getattr(app.state, "rag_broiler", None),
                getattr(app.state, "rag_layer", None),
            ]
            if rag and is_rag_functional(rag)
        )

        if total_rags == 3:
            return "optimal_3_rags"
        elif total_rags == 1:
            return "suboptimal_1_rag"
        else:
            return f"partial_{total_rags}_rags"

    uptime_hours = (time.time() - start_time) / 3600

    cache_status = "not_available"
    if STATS_CACHE_AVAILABLE:
        if stats_scheduler_task and not stats_scheduler_task.done():
            cache_status = "active"
        else:
            cache_status = "available_but_inactive"

    return {
        "status": "running",
        "version": "4.1.0",
        "environment": os.getenv("ENV", "production"),
        "database": bool(getattr(app.state, "supabase", None)),
        "postgresql": bool(os.getenv("DATABASE_URL")),
        "rag": rag_status(),
        "synthesis_enabled": synthesis_enabled,
        "cors_fix": "applied_v3_credentials",
        "optimization": "three_rag_system_enabled",
        "auth_routing_fix": "applied",
        "catch_22_resolved": True,
        "cors_middleware_fixed": True,
        "direct_auth_endpoints": True,
        "cors_credentials_fixed": True,
        "statistics_cache_system": cache_status,
        "stats_admin_router_enabled": True,
        "improved_rag_detection": True,
        "container_memory_calculation": True,
        "uptime_hours": round(uptime_hours, 2),
        "requests_processed": request_counter,
        "cache_updates": cache_update_counter if STATS_CACHE_AVAILABLE else 0,
        "memory_percent": get_container_memory_percent(),
        "last_update": "2025-09-10T18:55:00Z",
        "deployment_version": "v4.1.0-fixed-complete",
    }


# === GESTIONNAIRES D'EXCEPTIONS AVEC CORS CORRIGE ===
@app.exception_handler(HTTPException)
async def http_exc_handler(request: Request, exc: HTTPException):
    response = JSONResponse(
        status_code=exc.status_code,
        content={
            "detail": exc.detail,
            "timestamp": datetime.utcnow().isoformat() + "Z",
        },
        headers={"content-type": "application/json; charset=utf-8"},
    )

    origin = request.headers.get("Origin")
    allowed_origins = [
        "https://expert.intelia.com",
        "http://localhost:3000",
        "http://localhost:8080",
    ]

    if origin and origin in allowed_origins:
        response.headers["Access-Control-Allow-Origin"] = origin
        response.headers["Access-Control-Allow-Credentials"] = "true"
    else:
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Credentials"] = "false"

    response.headers["Access-Control-Allow-Methods"] = (
        "GET, POST, PUT, DELETE, OPTIONS, PATCH"
    )
    response.headers["Access-Control-Allow-Headers"] = (
        "Content-Type, Authorization, X-Session-ID"
    )

    return response


@app.exception_handler(Exception)
async def generic_exc_handler(request: Request, exc: Exception):
    logger.exception("Unhandled: %s", exc)

    response = JSONResponse(
        status_code=500,
        content={
            "detail": "Internal server error",
            "timestamp": datetime.utcnow().isoformat() + "Z",
        },
        headers={"content-type": "application/json; charset=utf-8"},
    )

    origin = request.headers.get("Origin")
    allowed_origins = [
        "https://expert.intelia.com",
        "http://localhost:3000",
        "http://localhost:8080",
    ]

    if origin and origin in allowed_origins:
        response.headers["Access-Control-Allow-Origin"] = origin
        response.headers["Access-Control-Allow-Credentials"] = "true"
    else:
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Credentials"] = "false"

    response.headers["Access-Control-Allow-Methods"] = (
        "GET, POST, PUT, DELETE, OPTIONS, PATCH"
    )
    response.headers["Access-Control-Allow-Headers"] = (
        "Content-Type, Authorization, X-Session-ID"
    )

    return response


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        app, host=os.getenv("HOST", "0.0.0.0"), port=int(os.getenv("PORT", "8000"))
    )
