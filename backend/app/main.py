# app/main.py - VERSION 4.1 COMPL√àTE AVEC TOUTES LES FONCTIONNALIT√âS
# ‚úÖ FUSION: D√©tection RAG corrig√©e + Toutes les fonctionnalit√©s du backup
# üöÄ NOUVEAU: Syst√®me de cache statistiques automatique
# üîß CORRECTION CORS POUR CREDENTIALS: 'INCLUDE' - VERSION FINALE CONSERV√âE
# üéØ COMPLET: Tous les endpoints, middleware, monitoring, auth, etc.

from __future__ import annotations

# tout en haut du fichier
import os
import time
import logging
import asyncio
import psutil
from pathlib import Path
from datetime import datetime, timedelta
from contextlib import asynccontextmanager
from typing import Optional, Dict, Any, List

from fastapi import FastAPI, Request, Response, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

def _rag_index_dir(name: str) -> str:
    """
    R√©sout le chemin de l'index RAG pour un espace donn√©.
    Priorit√©:
      1) ENV RAG_INDEX_DIR_<n> (ex: RAG_INDEX_DIR_GLOBAL)
      2) d√©faut /workspace/backend/rag_index/<n>
    """
    return os.getenv(f"RAG_INDEX_DIR_{name.upper()}", f"/workspace/backend/rag_index/{name}")

# .env (facultatif)
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# === LOGGING GLOBAL ===
logger = logging.getLogger("app.main")
logging.basicConfig(level=os.getenv("LOG_LEVEL", "INFO"))

# === CONFIG CORS ===
ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "https://expert.intelia.com,https://app.intelia.com,http://localhost:3000").split(",")

# üÜï ACTIVATION SYNTH√àSE LLM AU D√âMARRAGE (CONSERV√â)
synthesis_enabled = str(os.getenv("ENABLE_SYNTH_PROMPT", "0")).lower() in ("1", "true", "yes", "on")
if synthesis_enabled:
    logger.info("‚úÖ Synth√®se LLM activ√©e (ENABLE_SYNTH_PROMPT=1)")
else:
    logger.info("‚ÑπÔ∏è Synth√®se LLM d√©sactiv√©e (ENABLE_SYNTH_PROMPT=0)")

# ========== VARIABLES GLOBALES DE MONITORING (RESTAUR√âES) ==========
request_counter = 0
error_counter = 0
start_time = time.time()
active_requests = 0

# üöÄ NOUVEAU: Variables pour le syst√®me de cache statistiques
stats_scheduler_task = None
cache_update_counter = 0
cache_error_counter = 0

# === D√âTECTION DU SYST√àME DE CACHE STATISTIQUES ===
STATS_CACHE_AVAILABLE = False
try:
    from app.api.v1.stats_cache import get_stats_cache
    from app.api.v1.stats_updater import get_stats_updater, run_update_cycle, force_update_all
    STATS_CACHE_AVAILABLE = True
    logger.info("‚úÖ Syst√®me de cache statistiques import√© avec succ√®s")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Syst√®me de cache statistiques non disponible: {e}")
    logger.info("‚ÑπÔ∏è L'application fonctionnera normalement sans le cache optimis√©")

# === OUTILS SUPPL√âMENTAIRES ===
def get_rag_paths() -> Dict[str, str]:
    """üéØ TOUS LES RAG : Global + Broiler + Layer"""
    base_path = "/workspace/backend/rag_index"
    return {
        "global": f"{base_path}/global",
        "broiler": f"{base_path}/broiler",
        "layer": f"{base_path}/layer"
    }

# === FONCTION AM√âLIOR√âE DE D√âTECTION RAG ===
def is_rag_functional(embedder) -> bool:
    """
    D√©tecte si un RAG embedder est fonctionnel.
    Essaie plusieurs m√©thodes de v√©rification.
    """
    if embedder is None:
        return False
    
    try:
        # M√©thode 1: has_search_engine (si disponible)
        if hasattr(embedder, "has_search_engine"):
            if embedder.has_search_engine():
                return True
        
        # M√©thode 2: V√©rifier l'index FAISS
        if hasattr(embedder, "index") and embedder.index is not None:
            return True
        
        # M√©thode 3: V√©rifier les stats
        if hasattr(embedder, "get_index_stats"):
            stats = embedder.get_index_stats()
            if stats and stats.get("faiss_total", 0) > 0:
                return True
        
        # M√©thode 4: Essai de recherche simple
        try:
            results = embedder.search("test", top_k=1)
            return bool(results)
        except:
            pass
            
        return False
    except Exception as e:
        logger.debug(f"Erreur d√©tection RAG: {e}")
        return False

# === T√ÇCHES P√âRIODIQUES ===
async def periodic_monitoring():
    """Monitoring p√©riodique des performances serveur avec logging en base"""
    while True:
        try:
            await asyncio.sleep(300)  # Toutes les 5 minutes
            
            # Calcul des m√©triques
            current_time = time.time()
            uptime_hours = (current_time - start_time) / 3600
            requests_per_minute = request_counter / (uptime_hours * 60) if uptime_hours > 0 else 0
            error_rate_percent = (error_counter / max(request_counter, 1)) * 100
            
            # M√©triques syst√®me
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            
            # D√©terminer le status de sant√©
            if error_rate_percent > 10 or cpu_percent > 90 or memory_percent > 90:
                health_status = "critical"
            elif error_rate_percent > 5 or cpu_percent > 70 or memory_percent > 70:
                health_status = "degraded"
            else:
                health_status = "healthy"
            
            # Log des m√©triques serveur dans la base
            try:
                from app.api.v1.logging import get_analytics_manager
                analytics = get_analytics_manager()
                
                # Calculer l'heure tronqu√©e pour le groupement
                current_hour = datetime.now().replace(minute=0, second=0, microsecond=0)
                
                # Log des m√©triques (cette fonction devra √™tre ajout√©e dans logging.py)
                analytics.log_server_performance(
                    timestamp_hour=current_hour,
                    total_requests=request_counter,
                    successful_requests=request_counter - error_counter,
                    failed_requests=error_counter,
                    avg_response_time_ms=int(250),  # √Ä calculer r√©ellement si n√©cessaire
                    health_status=health_status,
                    error_rate_percent=error_rate_percent
                )
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur logging m√©triques en base: {e}")
                
            # üöÄ NOUVEAU: Ajouter les m√©triques du cache
            cache_info = ""
            if STATS_CACHE_AVAILABLE:
                cache_info = f", cache: {cache_update_counter} updates, {cache_error_counter} erreurs"
                
            logger.info(f"üìä M√©triques: {requests_per_minute:.1f} req/min, "
                       f"erreurs: {error_rate_percent:.1f}%, "
                       f"CPU: {cpu_percent:.1f}%, RAM: {memory_percent:.1f}%, "
                       f"sant√©: {health_status}{cache_info}")
            
        except Exception as e:
            logger.error(f"‚åõ Erreur monitoring p√©riodique: {e}")
            await asyncio.sleep(60)  # Retry dans 1 minute en cas d'erreur

async def periodic_stats_update():
    """üîÑ Mise √† jour p√©riodique du cache statistiques toutes les heures"""
    global cache_update_counter, cache_error_counter
    
    if not STATS_CACHE_AVAILABLE:
        logger.warning("‚ö†Ô∏è Cache statistiques non disponible - arr√™t scheduler")
        return
    
    # Attendre 5 minutes avant la premi√®re mise √† jour (laisser le syst√®me s'initialiser)
    await asyncio.sleep(300)
    
    # Premi√®re mise √† jour au d√©marrage
    logger.info("üöÄ Lancement premi√®re mise √† jour cache statistiques au d√©marrage")
    try:
        result = await run_update_cycle()
        if result.get("status") == "completed":
            cache_update_counter += 1
            logger.info("‚úÖ Premi√®re mise √† jour cache r√©ussie au d√©marrage")
        else:
            cache_error_counter += 1
            logger.warning(f"‚ö†Ô∏è Premi√®re mise √† jour cache √©chou√©e: {result.get('error', 'Unknown')}")
    except Exception as e:
        cache_error_counter += 1
        logger.error(f"‚ùå Erreur premi√®re mise √† jour cache: {e}")
    
    # Boucle principale - mise √† jour toutes les heures
    while True:
        try:
            # Attendre 1 heure (3600 secondes)
            await asyncio.sleep(3600)
            
            logger.info("üîÑ D√©but mise √† jour p√©riodique cache statistiques")
            start_update = time.time()
            
            # Lancer la mise √† jour
            result = await run_update_cycle()
            
            update_duration = (time.time() - start_update) * 1000  # en ms
            
            if result.get("status") == "completed":
                cache_update_counter += 1
                successful = result.get("successful_updates", 0)
                total = result.get("total_updates", 0)
                duration = result.get("duration_ms", update_duration)
                
                logger.info(f"‚úÖ Cache mis √† jour: {successful}/{total} succ√®s en {duration:.0f}ms")
                
                # Log d√©taill√© si des erreurs
                errors = result.get("errors", [])
                if errors:
                    logger.warning(f"‚ö†Ô∏è Erreurs durant la mise √† jour: {errors}")
                
            elif result.get("status") == "failed":
                cache_error_counter += 1
                error_msg = result.get("error", "Erreur inconnue")
                logger.error(f"‚ùå Mise √† jour cache √©chou√©e: {error_msg}")
                
            else:
                cache_error_counter += 1
                logger.warning(f"‚ö†Ô∏è Mise √† jour cache statut inattendu: {result}")
            
        except Exception as e:
            cache_error_counter += 1
            logger.error(f"‚ùå Erreur durant mise √† jour p√©riodique cache: {e}")
            # Attendre 10 minutes avant de retry en cas d'erreur
            await asyncio.sleep(600)

# === LIFESPAN COMPLET AVEC D√âTECTION RAG AM√âLIOR√âE ===
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ========== INITIALISATION AU D√âMARRAGE ==========
    logger.info("üöÄ D√©marrage de l'application Expert API avec syst√®me complet + d√©tection RAG am√©lior√©e")

    # ========== INITIALISATION DES SERVICES AM√âLIOR√âE ==========
    try:
        logger.info("üìä Initialisation des services analytics et facturation...")
        database_url = os.getenv("DATABASE_URL")
        if database_url:
            logger.info("‚úÖ DATABASE_URL configur√©e")

            # Analytics
            try:
                from app.api.v1.logging import get_analytics
                analytics_status = get_analytics()
                logger.info(f"‚úÖ Service analytics: {analytics_status.get('status', 'unknown')}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Service analytics partiellement disponible: {e}")

            # Billing
            try:
                from app.api.v1.billing import get_billing_manager
                billing = get_billing_manager()
                logger.info(f"‚úÖ Service billing: {len(billing.plans)} plans charg√©s")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Service billing partiellement disponible: {e}")

            # üöÄ Cache statistiques (optionnel)
            if STATS_CACHE_AVAILABLE:
                try:
                    cache = get_stats_cache()
                    # M√©thodes possibles pour obtenir les stats
                    if hasattr(cache, 'get_cache_stats'):
                        cache_stats = cache.get_cache_stats()
                    elif hasattr(cache, 'get_stats'):
                        cache_stats = cache.get_stats()
                    else:
                        cache_stats = "cache initialis√©"
                    logger.info(f"‚úÖ Cache statistiques initialis√©: {cache_stats}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Cache statistiques partiellement disponible: {e}")

            # Nettoyage sessions
            try:
                from app.api.v1.pipeline.postgres_memory import PostgresMemory
                memory = PostgresMemory()
                cleaned = memory.cleanup_old_sessions(days_old=7)
                if cleaned > 0:
                    logger.info(f"üßπ {cleaned} anciennes sessions nettoy√©es")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Nettoyage sessions √©chou√©: {e}")
        else:
            logger.warning("‚ö†Ô∏è DATABASE_URL manquante - services analytics d√©sactiv√©s")
    except Exception as e:
        logger.error(f"‚åõ Erreur initialisation services: {e}")

    # ========== SUPABASE ==========
    app.state.supabase = None
    try:
        from supabase import create_client
        url, key = os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_ANON_KEY")
        if url and key:
            app.state.supabase = create_client(url, key)
            logger.info("‚úÖ Supabase pr√™t")
        else:
            logger.info("‚ÑπÔ∏è Supabase non configur√©")
    except Exception as e:
        logger.warning("‚ÑπÔ∏è Supabase indisponible: %s", e)

    # üöÄ CHARGEMENT DES 3 RAG AVEC D√âTECTION AM√âLIOR√âE
    app.state.rag = None
    app.state.rag_broiler = None
    app.state.rag_layer = None

    try:
        from rag.embedder import FastRAGEmbedder

        rag_paths = get_rag_paths()
        logger.info(f"üîç Chargement des 3 RAG: {list(rag_paths.keys())}")

        # Variables d'environnement (override si d√©finies)
        env_override = {
            "global": os.getenv("RAG_INDEX_GLOBAL"),
            "broiler": os.getenv("RAG_INDEX_BROILER"),
            "layer": os.getenv("RAG_INDEX_LAYER"),
        }
        for key, env_path in env_override.items():
            if env_path and os.path.exists(env_path):
                rag_paths[key] = env_path
                logger.info(f"üîß Override ENV pour {key}: {env_path}")

        def _log_loaded(name: str, path: str, emb) -> None:
            try:
                stats = emb.get_index_stats() if hasattr(emb, "get_index_stats") else {}
                n_docs = stats.get("n_docs")
                faiss_total = stats.get("faiss_total")
                chunks_loaded = stats.get("chunks_loaded", faiss_total)
                dim = stats.get("embedding_dim", "unknown")
                model = stats.get("model_name", "unknown")
                logger.info(
                    f"‚úÖ RAG {name.capitalize()} charg√©: {path} "
                    f"(docs={n_docs if n_docs is not None else 'unknown'}, "
                    f"chunks={chunks_loaded if chunks_loaded is not None else 'unknown'}, "
                    f"dim={dim}, model={model})"
                )
            except Exception:
                logger.info(f"‚úÖ RAG {name.capitalize()} charg√©: {path}")

        def _load_rag(name: str, path: str, debug: bool = False):
            """Fonction helper pour charger un RAG avec gestion d'erreurs am√©lior√©e."""
            logger.info(f"üîç Chargement RAG {name.capitalize()}: {path}")
            
            if not os.path.exists(path):
                logger.warning(f"‚ö†Ô∏è RAG {name.capitalize()}: Chemin inexistant {path}")
                return None
            
            try:
                # V√©rifier le contenu du dossier
                entries = os.listdir(path)
                logger.debug(f"üìÇ {name.upper()} dir entries ({len(entries)}): {entries[:10]}")
                
                # Chercher les fichiers requis
                required_files = ["index.faiss"]
                missing_files = [f for f in required_files if f not in entries]
                if missing_files:
                    logger.warning(f"‚ö†Ô∏è RAG {name.capitalize()}: Fichiers manquants {missing_files}")
                
                # CORRECTION PRINCIPALE: Utiliser index_dir dans le constructeur (comme dans les logs qui fonctionnaient)
                embedder = FastRAGEmbedder(
                    index_dir=path,  # RETOUR √Ä LA M√âTHODE QUI FONCTIONNAIT
                    debug=debug,
                    cache_embeddings=True,
                    max_workers=2,
                )
                
                # Tester la fonctionnalit√© avec notre fonction am√©lior√©e
                if is_rag_functional(embedder):
                    _log_loaded(name, path, embedder)
                    return embedder
                else:
                    logger.warning(f"‚ö†Ô∏è RAG {name.capitalize()}: Instance cr√©√©e mais non fonctionnelle")
                    # Log des d√©tails pour debug
                    if hasattr(embedder, "get_index_stats"):
                        stats = embedder.get_index_stats()
                        logger.debug(f"üìä Stats {name}: {stats}")
                    return None
                    
            except Exception as e:
                logger.exception(f"üí• RAG {name.capitalize()} init failed: {e}")
                return None

        # üöÄ Chargement des 3 RAG
        app.state.rag = _load_rag("global", rag_paths["global"], debug=True)
        app.state.rag_broiler = _load_rag("broiler", rag_paths["broiler"])
        app.state.rag_layer = _load_rag("layer", rag_paths["layer"])

        # üìä R√©sum√© final des 3 RAG
        total_rags = sum(1 for rag in [app.state.rag, app.state.rag_broiler, app.state.rag_layer] if rag)

        rag_summary = {
            "global": "‚úÖ Actif" if app.state.rag else "‚åõ CRITIQUE",
            "broiler": "‚úÖ Actif" if app.state.rag_broiler else "‚åõ Absent",
            "layer": "‚úÖ Actif" if app.state.rag_layer else "‚åõ Absent",
            "total_loaded": total_rags
        }
        logger.info(f"üìä Status final des RAG: {rag_summary}")

        if total_rags == 3:
            logger.info("üéâ PARFAIT: Les 3 RAG sont charg√©s (Global + Broiler + Layer)")
        elif total_rags >= 1:
            logger.warning(f"‚ö†Ô∏è Seulement {total_rags}/3 RAG charg√©s - fonctionnement partiel")
        else:
            logger.error("‚ùå CRITIQUE: Aucun RAG charg√© - l'application ne peut pas fonctionner correctement")

    except Exception as e:
        logger.error("‚åõ Erreur critique initialisation RAG: %s", e)

    # ========== D√âMARRAGE DU MONITORING & SCHEDULER ==========
    monitoring_task = None
    global stats_scheduler_task
    
    try:
        monitoring_task = asyncio.create_task(periodic_monitoring())
        logger.info("üìä Monitoring p√©riodique d√©marr√©")
    except Exception as e:
        logger.error(f"‚åõ Erreur d√©marrage monitoring: {e}")

    if STATS_CACHE_AVAILABLE:
        try:
            stats_scheduler_task = asyncio.create_task(periodic_stats_update())
            logger.info("üîÑ Scheduler cache statistiques d√©marr√© (mise √† jour toutes les heures)")
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©marrage scheduler cache: {e}")
    else:
        logger.info("‚ÑπÔ∏è Scheduler cache statistiques d√©sactiv√© (module non disponible)")

    # ========== L'APPLICATION D√âMARRE ==========
    system_features = []
    if STATS_CACHE_AVAILABLE:
        system_features.append("cache statistiques optimis√©")
    system_features.extend([f"{total_rags}/3 RAG", "monitoring", "analytics", "billing"])
    logger.info(f"üéØ Application Expert API pr√™te avec: {', '.join(system_features)}")
    yield  # --- L'APPLICATION FONCTIONNE ---

    # ========== NETTOYAGE √Ä L'ARR√äT ==========
    logger.info("üõë Arr√™t de l'application Expert API")
    
    # Arr√™ter le monitoring
    if monitoring_task:
        try:
            monitoring_task.cancel()
            await asyncio.gather(monitoring_task, return_exceptions=True)
            logger.info("üìä Monitoring p√©riodique arr√™t√©")
        except Exception as e:
            logger.error(f"‚åõ Erreur arr√™t monitoring: {e}")
    
    # üöÄ Arr√™ter le scheduler cache
    if stats_scheduler_task:
        try:
            stats_scheduler_task.cancel()
            await asyncio.gather(stats_scheduler_task, return_exceptions=True)
            logger.info("üîÑ Scheduler cache statistiques arr√™t√©")
        except Exception as e:
            logger.error(f"‚ùå Erreur arr√™t scheduler cache: {e}")
    
    # Statistiques finales
    uptime_hours = (time.time() - start_time) / 3600
    final_stats = f"{request_counter} requ√™tes en {uptime_hours:.1f}h, {error_counter} erreurs ({(error_counter/max(request_counter,1)*100):.1f}%)"
    
    if STATS_CACHE_AVAILABLE and cache_update_counter > 0:
        cache_success_rate = ((cache_update_counter / max(cache_update_counter + cache_error_counter, 1)) * 100)
        final_stats += f", cache: {cache_update_counter} mises √† jour ({cache_success_rate:.1f}% succ√®s)"
    
    logger.info(f"üìà Statistiques finales: {final_stats}")

# === FASTAPI APP ===
app = FastAPI(
    title="Intelia Expert API",
    version="4.1.0",
    root_path="/api",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
    lifespan=lifespan,
)

# === MIDDLEWARE CORS CORRIG√â ===
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://expert.intelia.com",
        "https://expert-app-cngws.ondigitalocean.app",
        "http://localhost:3000",
        "http://localhost:8080",
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"],
    allow_headers=[
        "Accept",
        "Accept-Language", 
        "Content-Language",
        "Content-Type",
        "Authorization",
        "X-Requested-With",
        "Origin",
        "Access-Control-Request-Method",
        "Access-Control-Request-Headers",
        "X-Session-ID"
    ],
    expose_headers=[
        "Access-Control-Allow-Origin",
        "Access-Control-Allow-Credentials",
        "Access-Control-Allow-Methods",
        "Access-Control-Allow-Headers"
    ]
)

# === MIDDLEWARE DE MONITORING DES REQU√äTES (RESTAUR√â) ===
@app.middleware("http")
async def monitoring_middleware(request: Request, call_next):
    """Middleware pour tracker les performances en temps r√©el"""
    global request_counter, error_counter, active_requests
    
    start_time_req = time.time()
    active_requests += 1
    request_counter += 1
    
    try:
        response = await call_next(request)
        
        # Tracker les erreurs
        if response.status_code >= 400:
            error_counter += 1
            
        return response
        
    except Exception as e:
        error_counter += 1
        logger.error(f"‚åõ Erreur dans middleware monitoring: {e}")
        raise
        
    finally:
        active_requests -= 1
        processing_time = (time.time() - start_time_req) * 1000
        
        # Log des requ√™tes lentes
        if processing_time > 5000:  # Plus de 5 secondes
            logger.warning(f"üåÄ Requ√™te lente: {request.method} {request.url.path} - {processing_time:.0f}ms")

# === MIDDLEWARE D'AUTHENTIFICATION ===
try:
    from app.middleware.auth_middleware import auth_middleware
    app.middleware("http")(auth_middleware)
    logger.info("‚úÖ Middleware d'authentification activ√©")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Middleware d'authentification non disponible: {e}")
except Exception as e:
    logger.error(f"‚åõ Erreur lors de l'activation du middleware d'auth: {e}")

# === MONTAGE DES ROUTERS ===
try:
    from app.api.v1 import router as api_v1_router
    app.include_router(api_v1_router)
    logger.info("‚úÖ Router API v1 charg√© depuis __init__.py")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Impossible de charger le router v1 depuis __init__.py: {e}")
    logger.info("üîß Cr√©ation d'un router v1 temporaire...")
    
    # Cr√©er un router temporaire et inclure manuellement les composants
    from fastapi import APIRouter
    temp_v1_router = APIRouter(prefix="/v1", tags=["v1"])
    
    # Expert router
    try:
        from app.api.v1.expert import router as expert_router
        temp_v1_router.include_router(expert_router, tags=["expert"])
        logger.info("‚úÖ Expert router ajout√©")
    except ImportError as e:
        logger.error(f"‚åõ Impossible de charger expert router: {e}")
    
    # Auth router avec correction
    try:
        from app.api.v1.auth import router as auth_router
        
        logger.info(f"üîç Auth router prefix avant montage: {getattr(auth_router, 'prefix', 'None')}")
        logger.info(f"üîç Auth router routes count: {len(auth_router.routes)}")
        
        temp_v1_router.include_router(
            auth_router, 
            prefix="",  # Pas de prefix car auth.py d√©finit d√©j√† router = APIRouter(prefix="/auth")
            tags=["auth"]
        )
        
        logger.info("‚úÖ Auth router ajout√© avec montage corrig√©")
        
    except ImportError as e:
        logger.error(f"‚åõ Import Error auth router: {e}")
    except Exception as e:
        logger.error(f"‚åõ Erreur inattendue auth router: {e}")
    
    # üöÄ Routers cache statistiques (SAFE)
    if STATS_CACHE_AVAILABLE:
        try:
            from app.api.v1.stats_fast import router as stats_fast_router
            temp_v1_router.include_router(stats_fast_router, tags=["statistics-fast"])
            logger.info("‚úÖ Stats Fast router ajout√© (endpoints ultra-rapides)")
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è Stats Fast router non disponible: {e}")
        
        try:
            from app.api.v1.stats_admin import router as stats_admin_router
            temp_v1_router.include_router(stats_admin_router, tags=["statistics-admin"])
            logger.info("‚úÖ Stats Admin router ajout√© (administration cache)")
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è Stats Admin router non disponible: {e}")
    
    # Autres routers
    for router_info in [
        ("health", "health"),
        ("system", "system"),
        ("admin", "admin"),
        ("invitations", "invitations"),
        ("auth_invitations", "auth-invitations"),
        ("logging", "logging"),
        ("conversations", "conversations"),
        ("billing", "billing"),
        ("billing_openai", "billing-openai"),
    ]:
        try:
            module_name, tag = router_info
            module = __import__(f"app.api.v1.{module_name}", fromlist=["router"])
            router = getattr(module, "router")
            
            if module_name == "conversations":
                temp_v1_router.include_router(router, prefix="/conversations", tags=[tag])
            elif module_name == "billing_openai":
                temp_v1_router.include_router(router, prefix="/billing", tags=[tag])
            else:
                temp_v1_router.include_router(router, tags=[tag])
                
            logger.info(f"‚úÖ {tag.capitalize()} router ajout√©")
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è {tag.capitalize()} router non disponible: {e}")
    
    # Monter le router temporaire
    app.include_router(temp_v1_router)
    logger.info("‚úÖ Router v1 temporaire mont√© avec succ√®s")

# === ENDPOINTS DE DEBUG RAG AM√âLIOR√âS ===
@app.get("/rag/debug", tags=["Debug"])
async def rag_debug():
    """üîç Debug des 3 RAG avec d√©tection am√©lior√©e"""
    try:
        env_vars = {
            "RAG_INDEX_DIR_GLOBAL": os.getenv("RAG_INDEX_DIR_GLOBAL"),
            "RAG_INDEX_DIR_BROILER": os.getenv("RAG_INDEX_DIR_BROILER"),
            "RAG_INDEX_DIR_LAYER": os.getenv("RAG_INDEX_DIR_LAYER"),
            "RAG_INDEX_GLOBAL": os.getenv("RAG_INDEX_GLOBAL"),
            "RAG_INDEX_BROILER": os.getenv("RAG_INDEX_BROILER"),
            "RAG_INDEX_LAYER": os.getenv("RAG_INDEX_LAYER"),
        }
        
        rag_status = {}
        for name, attr in [("global", "rag"), ("broiler", "rag_broiler"), ("layer", "rag_layer")]:
            embedder = getattr(app.state, attr, None)
            if embedder:
                try:
                    stats = embedder.get_index_stats() if hasattr(embedder, "get_index_stats") else {}
                    rag_status[name] = {
                        "loaded": True,
                        "functional": is_rag_functional(embedder),
                        "stats": stats
                    }
                except:
                    rag_status[name] = {"loaded": True, "functional": False, "error": "Stats unavailable"}
            else:
                rag_status[name] = {"loaded": False, "functional": False}
        
        return {
            "env_vars": env_vars,
            "rag_status": rag_status,
            "paths": get_rag_paths(),
            "detection_method": "improved_functional_testing"
        }
    except Exception as e:
        return {"error": str(e)}

# === ENDPOINTS PRINCIPAUX (RESTAUR√âS) ===
@app.get("/health/complete", tags=["Health"])
async def complete_health_check():
    """ü•º Check de sant√© complet du syst√®me"""
    try:
        health_status = {
            "timestamp": datetime.utcnow().isoformat(),
            "status": "healthy",
            "components": {}
        }
        
        # Check base de donn√©es et analytics
        try:
            from app.api.v1.logging import get_analytics_manager
            analytics = get_analytics_manager()
            health_status["components"]["analytics"] = {
                "status": "healthy",
                "type": "postgresql",
                "tables_created": True
            }
        except Exception as e:
            health_status["components"]["analytics"] = {
                "status": "unhealthy",
                "error": str(e)
            }
            health_status["status"] = "degraded"
        
        # Check syst√®me de facturation
        try:
            from app.api.v1.billing import get_billing_manager
            billing = get_billing_manager()
            health_status["components"]["billing"] = {
                "status": "healthy",
                "plans_loaded": len(billing.plans),
                "quota_enforcement": True
            }
        except Exception as e:
            health_status["components"]["billing"] = {
                "status": "unhealthy",
                "error": str(e)
            }
            health_status["status"] = "degraded"
        
        # üöÄ Check syst√®me de cache statistiques
        if STATS_CACHE_AVAILABLE:
            try:
                cache = get_stats_cache()
                if hasattr(cache, 'get_cache_stats'):
                    cache_stats = cache.get_cache_stats()
                else:
                    cache_stats = "disponible"
                
                scheduler_active = stats_scheduler_task is not None and not stats_scheduler_task.done()
                cache_success_rate = 100
                if cache_update_counter + cache_error_counter > 0:
                    cache_success_rate = (cache_update_counter / (cache_update_counter + cache_error_counter)) * 100
                
                health_status["components"]["statistics_cache"] = {
                    "status": "healthy" if cache_stats and scheduler_active else "degraded",
                    "scheduler_active": scheduler_active,
                    "cache_updates": cache_update_counter,
                    "cache_errors": cache_error_counter,
                    "success_rate": round(cache_success_rate, 1),
                    "cache_entries": cache_stats
                }
            except Exception as e:
                health_status["components"]["statistics_cache"] = {
                    "status": "unhealthy",
                    "error": str(e)
                }
                health_status["status"] = "degraded"
        else:
            health_status["components"]["statistics_cache"] = {
                "status": "not_available",
                "message": "Module cache statistiques non import√©"
            }
        
        # Check RAG
        total_rags = sum(1 for rag in [
            getattr(app.state, "rag", None),
            getattr(app.state, "rag_broiler", None), 
            getattr(app.state, "rag_layer", None)
        ] if rag and is_rag_functional(rag))
        
        health_status["components"]["rag"] = {
            "status": "healthy" if total_rags >= 2 else "degraded" if total_rags >= 1 else "unhealthy",
            "loaded_rags": total_rags,
            "total_rags": 3
        }
        
        # Check OpenAI
        openai_key = os.getenv("OPENAI_API_KEY")
        health_status["components"]["openai"] = {
            "status": "configured" if openai_key else "not_configured"
        }
        
        # Check Auth system
        try:
            jwt_secret = os.getenv("JWT_SECRET") or os.getenv("SUPABASE_JWT_SECRET")
            supabase_url = os.getenv("SUPABASE_URL") 
            supabase_key = os.getenv("SUPABASE_ANON_KEY")
            
            auth_status = "healthy"
            if not (jwt_secret and supabase_url and supabase_key):
                auth_status = "partially_configured"
            if not jwt_secret:
                auth_status = "not_configured"
                
            health_status["components"]["auth"] = {
                "status": auth_status,
                "has_jwt_secret": bool(jwt_secret),
                "has_supabase_config": bool(supabase_url and supabase_key),
                "routing_fixed": True
            }
        except Exception as e:
            health_status["components"]["auth"] = {
                "status": "error",
                "error": str(e)
            }
        
        # M√©triques syst√®me
        uptime_hours = (time.time() - start_time) / 3600
        health_status["metrics"] = {
            "uptime_hours": round(uptime_hours, 2),
            "total_requests": request_counter,
            "error_rate_percent": round((error_counter / max(request_counter, 1)) * 100, 2),
            "active_requests": active_requests,
            "cache_updates": cache_update_counter,
            "cache_errors": cache_error_counter,
            "scheduler_active": stats_scheduler_task is not None and not stats_scheduler_task.done() if STATS_CACHE_AVAILABLE else False
        }
        
        # D√©terminer le statut global
        component_statuses = [comp["status"] for comp in health_status["components"].values() 
                            if comp["status"] in ["healthy", "degraded", "unhealthy"]]
        
        if any(status == "unhealthy" for status in component_statuses):
            health_status["status"] = "unhealthy"
        elif any(status == "degraded" for status in component_statuses):
            health_status["status"] = "degraded"
        
        return health_status
        
    except Exception as e:
        logger.error(f"‚åõ Erreur health check: {e}")
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "status": "error",
            "error": str(e)
        }

@app.get("/metrics", tags=["Monitoring"])
async def system_metrics():
    """üìä M√©triques syst√®me pour monitoring externe"""
    try:
        uptime_seconds = time.time() - start_time
        
        base_metrics = {
            "uptime_seconds": uptime_seconds,
            "requests_total": request_counter,
            "errors_total": error_counter,
            "requests_active": active_requests,
            "error_rate": (error_counter / max(request_counter, 1)) * 100,
            "requests_per_second": request_counter / max(uptime_seconds, 1),
            "rag_status": {
                "global": bool(getattr(app.state, "rag", None)),
                "broiler": bool(getattr(app.state, "rag_broiler", None)),
                "layer": bool(getattr(app.state, "rag_layer", None))
            },
            "synthesis_enabled": synthesis_enabled,
            "auth_routing_fixed": True,
            "cors_middleware_fixed": True,
            "direct_auth_endpoints": True,
            "cors_credentials_fixed": True,
            "stats_admin_router_fixed": True,
        }
        
        # üöÄ Ajouter m√©triques cache si disponible
        if STATS_CACHE_AVAILABLE:
            base_metrics.update({
                "cache_system_enabled": True,
                "cache_updates_total": cache_update_counter,
                "cache_errors_total": cache_error_counter,
                "cache_success_rate": (cache_update_counter / max(cache_update_counter + cache_error_counter, 1)) * 100,
                "scheduler_active": stats_scheduler_task is not None and not stats_scheduler_task.done()
            })
        else:
            base_metrics["cache_system_enabled"] = False
        
        return base_metrics
        
    except Exception as e:
        return {"error": str(e)}

@app.get("/admin/stats", tags=["Admin"])
async def admin_statistics():
    """üìà Statistiques administrateur compl√®tes avec cache"""
    try:
        from app.api.v1.billing import get_billing_manager
        
        # Stats billing
        billing = get_billing_manager()
        
        # Stats analytics (approximatives)
        uptime_hours = (time.time() - start_time) / 3600
        
        base_stats = {
            "system_health": {
                "uptime_hours": round(uptime_hours, 2),
                "total_requests": request_counter,
                "error_rate": round((error_counter / max(request_counter, 1)) * 100, 2),
                "rag_status": {
                    "global": bool(getattr(app.state, "rag", None)),
                    "broiler": bool(getattr(app.state, "rag_broiler", None)),
                    "layer": bool(getattr(app.state, "rag_layer", None))
                }
            },
            "billing_stats": {
                "plans_available": len(billing.plans),
                "plan_names": list(billing.plans.keys())
            },
            "features_enabled": {
                "analytics": bool(os.getenv("DATABASE_URL")),
                "billing": True,
                "authentication": bool(os.getenv("JWT_SECRET") or os.getenv("SUPABASE_JWT_SECRET")),
                "openai_fallback": bool(os.getenv("OPENAI_API_KEY")),
                "auth_routing_fixed": True,
                "cors_middleware_fixed": True,
                "direct_auth_endpoints": True,
                "cors_credentials_fixed": True,
                "statistics_cache_system": STATS_CACHE_AVAILABLE,
                "stats_admin_router_enabled": True,
            }
        }
        
        # üöÄ Ajouter stats cache si disponible
        if STATS_CACHE_AVAILABLE:
            try:
                cache = get_stats_cache()
                if hasattr(cache, 'get_cache_stats'):
                    cache_stats = cache.get_cache_stats()
                else:
                    cache_stats = "disponible"
                
                base_stats["cache_system"] = {
                    "enabled": True,
                    "scheduler_active": stats_scheduler_task is not None and not stats_scheduler_task.done(),
                    "updates_total": cache_update_counter,
                    "errors_total": cache_error_counter,
                    "success_rate": round((cache_update_counter / max(cache_update_counter + cache_error_counter, 1)) * 100, 1),
                    "cache_statistics": cache_stats
                }
            except Exception as e:
                base_stats["cache_system"] = {
                    "enabled": True,
                    "status": "error",
                    "error": str(e)
                }
        else:
            base_stats["cache_system"] = {
                "enabled": False,
                "reason": "Module non import√©"
            }
        
        return base_stats
        
    except Exception as e:
        return {"error": str(e)}

# üöÄ Endpoints pour contr√¥le manuel du cache
@app.post("/admin/cache/force-update", tags=["Admin"])
async def admin_force_cache_update():
    """üîÑ Force une mise √† jour manuelle du cache (admin)"""
    if not STATS_CACHE_AVAILABLE:
        return {
            "status": "unavailable",
            "message": "Syst√®me de cache non disponible"
        }
    
    try:
        logger.info("üîÑ Force update cache demand√© via admin endpoint")
        result = await force_update_all()
        
        global cache_update_counter, cache_error_counter
        if result.get("status") == "completed":
            cache_update_counter += 1
        else:
            cache_error_counter += 1
        
        return {
            "status": "success",
            "message": "Mise √† jour cache forc√©e",
            "result": result,
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        cache_error_counter += 1
        logger.error(f"‚ùå Erreur force update admin: {e}")
        return {
            "status": "error",
            "message": f"Erreur mise √† jour: {e}",
            "timestamp": datetime.utcnow().isoformat()
        }

@app.get("/admin/cache/status", tags=["Admin"])
async def admin_cache_status():
    """üìä Statut d√©taill√© du syst√®me de cache (admin)"""
    if not STATS_CACHE_AVAILABLE:
        return {
            "status": "unavailable",
            "message": "Syst√®me de cache non disponible",
            "enabled": False
        }
    
    try:
        cache = get_stats_cache()
        updater = get_stats_updater()
        
        if hasattr(cache, 'get_cache_stats'):
            cache_stats = cache.get_cache_stats()
        else:
            cache_stats = "disponible"
            
        if hasattr(updater, 'get_update_status'):
            update_status = updater.get_update_status()
        else:
            update_status = "disponible"
        
        return {
            "status": "available",
            "enabled": True,
            "scheduler": {
                "active": stats_scheduler_task is not None and not stats_scheduler_task.done(),
                "task_done": stats_scheduler_task.done() if stats_scheduler_task else True
            },
            "counters": {
                "updates_successful": cache_update_counter,
                "updates_failed": cache_error_counter,
                "success_rate": round((cache_update_counter / max(cache_update_counter + cache_error_counter, 1)) * 100, 1)
            },
            "cache_statistics": cache_stats,
            "last_update": update_status,
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e),
            "enabled": True
        }

# === ENDPOINTS AUTH DIRECTS (RESTAUR√âS) ===
@app.post("/v1/auth/login")
async def auth_login_direct(request: Request):
    """Endpoint login direct - contournement du probl√®me de router"""
    try:
        from app.api.v1.auth import LoginRequest, create_access_token
        
        body = await request.json()
        login_data = LoginRequest(**body)
        
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_ANON_KEY")
        
        if not supabase_url or not supabase_key:
            return JSONResponse(
                status_code=500,
                content={"detail": "Authentication service unavailable"}
            )
        
        try:
            from supabase import create_client
            supabase = create_client(supabase_url, supabase_key)
            
            result = supabase.auth.sign_in_with_password({
                "email": login_data.email,
                "password": login_data.password
            })
            
        except Exception as e:
            return JSONResponse(
                status_code=401,
                content={"detail": "Invalid credentials", "error": str(e)}
            )
        
        user = result.user
        if user is None:
            return JSONResponse(
                status_code=401,
                content={"detail": "Invalid credentials"}
            )
        
        expires = timedelta(minutes=60)
        token = create_access_token(
            {"user_id": user.id, "email": login_data.email}, 
            expires
        )
        
        return {
            "access_token": token,
            "token_type": "bearer",
            "expires_at": (datetime.utcnow() + expires).isoformat(),
            "user": {
                "id": user.id,
                "email": user.email,
                "user_metadata": user.user_metadata
            }
        }
        
    except Exception as e:
        logger.error(f"‚åõ Erreur login direct: {e}")
        return JSONResponse(
            status_code=500,
            content={"detail": "Login failed", "error": str(e)}
        )

@app.get("/v1/auth/me")
async def auth_me_direct(request: Request):
    """Endpoint me direct"""
    try:
        from app.api.v1.auth import get_current_user
        from fastapi.security import HTTPAuthorizationCredentials
        
        auth_header = request.headers.get("Authorization")
        if not auth_header or not auth_header.startswith("Bearer "):
            return JSONResponse(
                status_code=401,
                content={"detail": "Missing or invalid authorization header"}
            )
        
        credentials = HTTPAuthorizationCredentials(
            scheme="Bearer",
            credentials=auth_header.replace("Bearer ", "")
        )
        
        user_info = await get_current_user(credentials)
        
        return {
            "user_id": user_info.get("user_id"),
            "email": user_info.get("email"),
            "user_type": user_info.get("user_type"),
            "full_name": user_info.get("full_name"),
            "is_admin": user_info.get("is_admin"),
            "preferences": user_info.get("preferences", {}),
            "profile_id": user_info.get("profile_id"),
            "direct_endpoint": True,
            "routing_corrected": True
        }
        
    except Exception as e:
        logger.error(f"‚åõ Erreur me direct: {e}")
        return JSONResponse(
            status_code=401,
            content={"detail": "Authentication failed", "error": str(e)}
        )

@app.get("/", tags=["Root"])
async def root():
    def rag_status() -> str:
        total_rags = sum(1 for rag in [
            getattr(app.state, "rag", None),
            getattr(app.state, "rag_broiler", None),
            getattr(app.state, "rag_layer", None)
        ] if rag and is_rag_functional(rag))

        if total_rags == 3:
            return "optimal_3_rags"
        elif total_rags == 1:
            return "suboptimal_1_rag"
        else:
            return f"partial_{total_rags}_rags"

    uptime_hours = (time.time() - start_time) / 3600
    
    cache_status = "not_available"
    if STATS_CACHE_AVAILABLE:
        if stats_scheduler_task and not stats_scheduler_task.done():
            cache_status = "active"
        else:
            cache_status = "available_but_inactive"
    
    return {
        "status": "running",
        "version": "4.1.0",
        "environment": os.getenv("ENV", "production"),
        "database": bool(getattr(app.state, "supabase", None)),
        "postgresql": bool(os.getenv("DATABASE_URL")),
        "rag": rag_status(),
        "synthesis_enabled": synthesis_enabled,
        "cors_fix": "applied_v3_credentials",
        "optimization": "three_rag_system_enabled",
        "auth_routing_fix": "applied",
        "catch_22_resolved": True,
        "cors_middleware_fixed": True,
        "direct_auth_endpoints": True,
        "cors_credentials_fixed": True,
        "statistics_cache_system": cache_status,
        "stats_admin_router_enabled": True,
        "improved_rag_detection": True,
        "uptime_hours": round(uptime_hours, 2),
        "requests_processed": request_counter,
        "cache_updates": cache_update_counter if STATS_CACHE_AVAILABLE else 0,
        "last_update": "2025-08-25T02:00:00Z",
        "deployment_version": "v4.1.0-complete-fusion"
    }

# === GESTIONNAIRES D'EXCEPTIONS AVEC CORS CORRIG√â ===
@app.exception_handler(HTTPException)
async def http_exc_handler(request: Request, exc: HTTPException):
    response = JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail, "timestamp": datetime.utcnow().isoformat() + "Z"},
        headers={"content-type": "application/json; charset=utf-8"},
    )

    origin = request.headers.get("Origin")
    allowed_origins = [
        "https://expert.intelia.com",
        "https://expert-app-cngws.ondigitalocean.app",
        "http://localhost:3000",
        "http://localhost:8080"
    ]
    
    if origin and origin in allowed_origins:
        response.headers["Access-Control-Allow-Origin"] = origin
        response.headers["Access-Control-Allow-Credentials"] = "true"
    else:
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Credentials"] = "false"
    
    response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS, PATCH"
    response.headers["Access-Control-Allow-Headers"] = "Content-Type, Authorization, X-Session-ID"

    return response

@app.exception_handler(Exception)
async def generic_exc_handler(request: Request, exc: Exception):
    logger.exception("Unhandled: %s", exc)

    response = JSONResponse(
        status_code=500,
        content={"detail": "Internal server error", "timestamp": datetime.utcnow().isoformat() + "Z"},
        headers={"content-type": "application/json; charset=utf-8"},
    )

    origin = request.headers.get("Origin")
    allowed_origins = [
        "https://expert.intelia.com",
        "https://expert-app-cngws.ondigitalocean.app",
        "http://localhost:3000",
        "http://localhost:8080"
    ]
    
    if origin and origin in allowed_origins:
        response.headers["Access-Control-Allow-Origin"] = origin
        response.headers["Access-Control-Allow-Credentials"] = "true"
    else:
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Credentials"] = "false"
    
    response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS, PATCH"
    response.headers["Access-Control-Allow-Headers"] = "Content-Type, Authorization, X-Session-ID"

    return response

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=os.getenv("HOST", "0.0.0.0"), port=int(os.getenv("PORT", "8000")))