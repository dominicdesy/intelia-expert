"""
app/api/v1/expert_services.py - SERVICES M√âTIER EXPERT SYSTEM

Logique m√©tier principale pour le syst√®me expert
VERSION MODIFI√âE : RAG-First obligatoire, suppression donn√©es cod√©es
"""

import os
import logging
import uuid
import time
import re
from datetime import datetime
from typing import Optional, Dict, Any, Tuple

from fastapi import HTTPException, Request

from .expert_models import (
    EnhancedQuestionRequest, EnhancedExpertResponse, FeedbackRequest,
    ValidationResult, ProcessingContext
)
from .expert_utils import (
    get_user_id_from_request, 
    build_enriched_question_from_clarification,
    get_enhanced_topics_by_language,
    save_conversation_auto_enhanced
)
from .expert_integrations import IntegrationsManager

logger = logging.getLogger(__name__)

class RAGContextEnhancer:
    """Am√©liore le contexte conversationnel pour optimiser les requ√™tes RAG"""
    
    def __init__(self):
        # Patterns pour d√©tecter les r√©f√©rences contextuelles
        self.pronoun_patterns = {
            "fr": [
                r'\b(son|sa|ses|leur|leurs)\s+(poids|√¢ge|croissance|d√©veloppement)',
                r'\b(ils|elles)\s+(p√®sent|grandissent|se d√©veloppent)',
                r'\b(qu\'?est-ce que|quel est)\s+(son|sa|ses|leur)',
                r'\b(combien)\s+(p√®sent-ils|font-ils|mesurent-ils)'
            ],
            "en": [
                r'\b(their|its)\s+(weight|age|growth|development)',
                r'\b(they)\s+(weigh|grow|develop)',
                r'\b(what is|how much is)\s+(their|its)',
                r'\b(how much do they)\s+(weigh|measure)'
            ],
            "es": [
                r'\b(su|sus)\s+(peso|edad|crecimiento|desarrollo)',
                r'\b(ellos|ellas)\s+(pesan|crecen|se desarrollan)',
                r'\b(cu√°l es|cu√°nto es)\s+(su|sus)',
                r'\b(cu√°nto)\s+(pesan|miden)'
            ]
        }
    
    def enhance_question_for_rag(
        self, 
        question: str, 
        conversation_context: str, 
        language: str = "fr"
    ) -> Tuple[str, Dict[str, any]]:
        """
        Am√©liore une question pour le RAG en utilisant le contexte conversationnel
        """
        
        enhancement_info = {
            "pronoun_detected": False,
            "context_entities_used": [],
            "question_enriched": False,
            "original_question": question
        }
        
        # 1. D√©tecter les pronoms/r√©f√©rences contextuelles
        has_pronouns = self._detect_contextual_references(question, language)
        if has_pronouns:
            enhancement_info["pronoun_detected"] = True
            logger.info(f"üîç [RAG Context] Pronoms d√©tect√©s dans: '{question}'")
        
        # 2. Extraire entit√©s du contexte
        context_entities = self._extract_context_entities(conversation_context)
        if context_entities:
            enhancement_info["context_entities_used"] = list(context_entities.keys())
            logger.info(f"üìä [RAG Context] Entit√©s contextuelles: {context_entities}")
        
        # 3. Enrichir la question si n√©cessaire
        enriched_question = question
        
        if has_pronouns and context_entities:
            enriched_question = self._build_enriched_question(
                question, context_entities, language
            )
            enhancement_info["question_enriched"] = True
            logger.info(f"‚ú® [RAG Context] Question enrichie: '{enriched_question}'")
        
        # 4. Ajouter contexte technique si pertinent
        if context_entities or has_pronouns:
            technical_context = self._build_technical_context(context_entities, language)
            if technical_context:
                enriched_question += f"\n\nContexte technique: {technical_context}"
        
        return enriched_question, enhancement_info
    
    def _detect_contextual_references(self, question: str, language: str) -> bool:
        """D√©tecte si la question contient des pronoms/r√©f√©rences contextuelles"""
        
        patterns = self.pronoun_patterns.get(language, self.pronoun_patterns["fr"])
        question_lower = question.lower()
        
        for pattern in patterns:
            if re.search(pattern, question_lower, re.IGNORECASE):
                logger.debug(f"üéØ [RAG Context] Pattern trouv√©: {pattern}")
                return True
        
        return False
    
    def _extract_context_entities(self, context: str) -> Dict[str, str]:
        """Extrait les entit√©s importantes du contexte conversationnel"""
        
        if not context:
            return {}
        
        entities = {}
        context_lower = context.lower()
        
        # Extraire race
        breed_patterns = [
            r'race[:\s]+([a-zA-Z0-9\s]+?)(?:\n|,|\.|\s|$)',
            r'breed[:\s]+([a-zA-Z0-9\s]+?)(?:\n|,|\.|\s|$)',
            r'(ross\s*308|cobb\s*500|hubbard|arbor\s*acres)',
            r'poulets?\s+(ross\s*308|cobb\s*500)',
            r'chickens?\s+(ross\s*308|cobb\s*500)'
        ]
        
        for pattern in breed_patterns:
            match = re.search(pattern, context_lower, re.IGNORECASE)
            if match:
                entities["breed"] = match.group(1).strip()
                break
        
        # Extraire √¢ge
        age_patterns = [
            r'√¢ge[:\s]+(\d+\s*(?:jour|semaine|day|week)s?)',
            r'age[:\s]+(\d+\s*(?:jour|semaine|day|week)s?)',
            r'(\d+)\s*(?:jour|day)s?',
            r'(\d+)\s*(?:semaine|week)s?'
        ]
        
        for pattern in age_patterns:
            match = re.search(pattern, context_lower, re.IGNORECASE)
            if match:
                entities["age"] = match.group(1).strip()
                break
        
        return entities
    
    def _build_enriched_question(
        self, 
        question: str, 
        context_entities: Dict[str, str], 
        language: str
    ) -> str:
        """Construit une question enrichie en rempla√ßant les pronoms par les entit√©s contextuelles"""
        
        enriched = question
        
        # Templates par langue
        templates = {
            "fr": {
                "breed_age": "Pour des {breed} de {age}",
                "breed_only": "Pour des {breed}",
                "age_only": "Pour des poulets de {age}"
            },
            "en": {
                "breed_age": "For {breed} chickens at {age}",
                "breed_only": "For {breed} chickens", 
                "age_only": "For chickens at {age}"
            },
            "es": {
                "breed_age": "Para pollos {breed} de {age}",
                "breed_only": "Para pollos {breed}",
                "age_only": "Para pollos de {age}"
            }
        }
        
        template_set = templates.get(language, templates["fr"])
        
        # Construire le pr√©fixe contextuel
        context_prefix = ""
        if "breed" in context_entities and "age" in context_entities:
            context_prefix = template_set["breed_age"].format(
                breed=context_entities["breed"],
                age=context_entities["age"]
            )
        elif "breed" in context_entities:
            context_prefix = template_set["breed_only"].format(
                breed=context_entities["breed"]
            )
        elif "age" in context_entities:
            context_prefix = template_set["age_only"].format(
                age=context_entities["age"]
            )
        
        if context_prefix:
            # Remplacer ou pr√©fixer selon la structure de la question
            if any(word in question.lower() for word in ["son", "sa", "ses", "leur", "leurs", "their", "its", "su", "sus"]):
                enriched = f"{context_prefix}, {question.lower()}"
            else:
                enriched = f"{context_prefix}: {question}"
        
        return enriched
    
    def _build_technical_context(self, entities: Dict[str, str], language: str) -> str:
        """Construit un contexte technique pour aider le RAG"""
        
        if not entities:
            return ""
        
        context_parts = []
        
        if "breed" in entities:
            context_parts.append(f"Race: {entities['breed']}")
        
        if "age" in entities:
            context_parts.append(f"√Çge: {entities['age']}")
        
        return " | ".join(context_parts)

class ExpertService:
    """Service principal pour le syst√®me expert"""
    
    def __init__(self):
        self.integrations = IntegrationsManager()
        self.rag_enhancer = RAGContextEnhancer()
        logger.info("‚úÖ [Expert Service] Service expert initialis√©")
    
    def get_current_user_dependency(self):
        """Retourne la d√©pendance pour l'authentification"""
        return self.integrations.get_current_user_dependency()
    
    async def process_expert_question(
        self,
        request_data: EnhancedQuestionRequest,
        request: Request,
        current_user: Optional[Dict[str, Any]],
        start_time: float
    ) -> EnhancedExpertResponse:
        """Traite une question expert avec toutes les fonctionnalit√©s am√©lior√©es"""
        
        processing_steps = []
        ai_enhancements_used = []
        
        # Initialisation
        processing_steps.append("initialization")
        
        # === AUTHENTIFICATION ===
        if current_user is None and self.integrations.auth_available:
            raise HTTPException(status_code=401, detail="Authentification requise")
        
        user_id = self._extract_user_id(current_user, request_data, request)
        user_email = current_user.get("email") if current_user else None
        request_ip = request.client.host if request.client else "unknown"
        
        processing_steps.append("authentication")
        
        # === GESTION CONVERSATION ID ===
        conversation_id = self._get_or_create_conversation_id(request_data)
        
        # === VALIDATION QUESTION ===
        question_text = request_data.text.strip()
        if not question_text:
            raise HTTPException(status_code=400, detail="Question text is required")
        
        processing_steps.append("question_validation")
        
        # === ENREGISTREMENT DANS M√âMOIRE INTELLIGENTE ===
        conversation_context = None
        if self.integrations.intelligent_memory_available:
            try:
                conversation_context = self.integrations.add_message_to_conversation(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    message=question_text,
                    role="user",
                    language=request_data.language,
                    message_type="clarification_response" if request_data.is_clarification_response else "question"
                )
                ai_enhancements_used.append("intelligent_memory")
                processing_steps.append("memory_storage")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [Expert Service] Erreur m√©moire: {e}")
        
        # === VALIDATION AGRICOLE ===
        validation_result = await self._validate_agricultural_question(
            question_text, request_data.language, user_id, request_ip, conversation_id
        )
        
        processing_steps.append("agricultural_validation")
        
        if not validation_result.is_valid:
            return self._create_rejection_response(
                question_text, validation_result, conversation_id, 
                user_email, request_data.language, start_time,
                processing_steps, ai_enhancements_used
            )
        
        # === SYST√àME DE CLARIFICATION ===
        clarification_result = await self._handle_clarification(
            request_data, question_text, user_id, conversation_id,
            processing_steps, ai_enhancements_used
        )
        
        if clarification_result:
            return clarification_result
        
        # === TRAITEMENT EXPERT AVEC RAG-FIRST ===
        expert_result = await self._process_expert_response(
            question_text, request_data, request, current_user,
            conversation_id, processing_steps, ai_enhancements_used
        )
        
        # === ENREGISTREMENT R√âPONSE ===
        if self.integrations.intelligent_memory_available and expert_result["answer"]:
            try:
                self.integrations.add_message_to_conversation(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    message=expert_result["answer"],
                    role="assistant",
                    language=request_data.language,
                    message_type="response"
                )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [Expert Service] Erreur enregistrement r√©ponse: {e}")
        
        processing_steps.append("response_storage")
        
        # === CONSTRUCTION R√âPONSE FINALE ===
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return self._build_final_response(
            question_text, expert_result["answer"], conversation_id,
            user_email, request_data.language, response_time_ms,
            expert_result, validation_result, conversation_context,
            processing_steps, ai_enhancements_used, request_data
        )
    
    async def process_feedback(self, feedback_data: FeedbackRequest) -> Dict[str, Any]:
        """Traite le feedback utilisateur"""
        
        feedback_updated = False
        
        if feedback_data.conversation_id and self.integrations.logging_available:
            try:
                # Convertir le rating en format num√©rique
                rating_numeric = {
                    "positive": 1,
                    "negative": -1,
                    "neutral": 0
                }.get(feedback_data.rating, 0)
                
                feedback_updated = await self.integrations.update_feedback(
                    feedback_data.conversation_id, rating_numeric
                )
                
            except Exception as e:
                logger.error(f"‚ùå [Expert Service] Erreur mise √† jour feedback: {e}")
        
        return {
            "success": True,
            "message": "Feedback enregistr√© avec succ√®s (Enhanced)",
            "rating": feedback_data.rating,
            "comment": feedback_data.comment,
            "conversation_id": feedback_data.conversation_id,
            "feedback_updated_in_db": feedback_updated,
            "enhanced_features_used": True,
            "timestamp": datetime.now().isoformat()
        }
    
    async def get_suggested_topics(self, language: str) -> Dict[str, Any]:
        """R√©cup√®re les topics sugg√©r√©s enrichis"""
        
        lang = language.lower() if language else "fr"
        if lang not in ["fr", "en", "es"]:
            lang = "fr"
        
        topics_by_language = get_enhanced_topics_by_language()
        topics = topics_by_language.get(lang, topics_by_language["fr"])
        
        return {
            "topics": topics,
            "language": lang,
            "count": len(topics),
            "enhanced_features": {
                "numerical_data_included": True,
                "context_aware": True,
                "breed_specific_examples": True
            },
            "system_status": {
                "validation_enabled": self.integrations.is_agricultural_validation_enabled(),
                "enhanced_clarification_enabled": self.integrations.is_enhanced_clarification_enabled(),
                "intelligent_memory_enabled": self.integrations.intelligent_memory_available,
                "ai_enhancements_enabled": self.integrations.intelligent_memory_available and self.integrations.enhanced_clarification_available
            },
            "note": "Topics enrichis avec donn√©es num√©riques et exemples sp√©cifiques"
        }
    
    # === M√âTHODES PRIV√âES ===
    
    def _extract_user_id(self, current_user: Optional[Dict], request_data: EnhancedQuestionRequest, request: Request) -> str:
        """Extrait l'user_id depuis diff√©rentes sources"""
        if current_user:
            return current_user.get("user_id") or request_data.user_id or "authenticated_user"
        return request_data.user_id or get_user_id_from_request(request)
    
    def _get_or_create_conversation_id(self, request_data: EnhancedQuestionRequest) -> str:
        """R√©cup√®re ou cr√©e un conversation_id"""
        if request_data.conversation_id and request_data.conversation_id.strip():
            conversation_id = request_data.conversation_id.strip()
            logger.info(f"üîÑ [Expert Service] CONTINUATION: {conversation_id}")
            return conversation_id
        else:
            conversation_id = str(uuid.uuid4())
            logger.info(f"üÜï [Expert Service] NOUVELLE: {conversation_id}")
            return conversation_id
    
    async def _validate_agricultural_question(
        self, question: str, language: str, user_id: str, 
        request_ip: str, conversation_id: str
    ) -> ValidationResult:
        """Valide la question dans le domaine agricole"""
        
        if not self.integrations.agricultural_validator_available:
            return ValidationResult(
                is_valid=False,
                rejection_message="Service temporairement indisponible. Veuillez r√©essayer plus tard.",
                confidence=0.0
            )
        
        # Enrichir avec contexte intelligent si disponible
        enriched_question = question
        if self.integrations.intelligent_memory_available:
            try:
                rag_context = self.integrations.get_context_for_rag(conversation_id)
                if rag_context:
                    enriched_question = f"{question}\n\nContexte conversationnel:\n{rag_context}"
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [Expert Service] Erreur enrichissement contexte: {e}")
        
        try:
            validation_result = self.integrations.validate_agricultural_question(
                question=enriched_question,
                language=language,
                user_id=user_id,
                request_ip=request_ip
            )
            
            return ValidationResult(
                is_valid=validation_result.is_valid,
                rejection_message=validation_result.reason or "Question hors domaine agricole",
                confidence=validation_result.confidence
            )
            
        except Exception as e:
            logger.error(f"‚ùå [Expert Service] Erreur validateur: {e}")
            
            rejection_messages = {
                "fr": "Erreur de validation. Veuillez reformuler votre question sur le domaine avicole.",
                "en": "Validation error. Please rephrase your question about the poultry domain.",
                "es": "Error de validaci√≥n. Por favor, reformule su pregunta sobre el dominio av√≠cola."
            }
            
            return ValidationResult(
                is_valid=False,
                rejection_message=rejection_messages.get(language, rejection_messages["fr"]),
                confidence=0.0
            )
    
    async def _handle_clarification(
        self, request_data: EnhancedQuestionRequest, question_text: str,
        user_id: str, conversation_id: str, processing_steps: list,
        ai_enhancements_used: list
    ) -> Optional[EnhancedExpertResponse]:
        """G√®re le syst√®me de clarification"""
        
        # V√©rifier si c'est une r√©ponse √† clarification avec retraitement
        if request_data.is_clarification_response and request_data.original_question:
            if self.integrations.enhanced_clarification_available:
                try:
                    # Construire question enrichie
                    enriched_question = build_enriched_question_from_clarification(
                        original_question=request_data.original_question,
                        clarification_response=question_text,
                        conversation_context=self.integrations.get_context_for_rag(conversation_id) if self.integrations.intelligent_memory_available else ""
                    )
                    
                    # V√©rifier retraitement possible
                    if hasattr(request_data, 'clarification_context') and request_data.clarification_context:
                        reprocess_result = await self.integrations.check_for_reprocessing_after_clarification(
                            conversation_id=conversation_id,
                            user_response=question_text,
                            original_clarification_result=request_data.clarification_context
                        )
                        
                        if reprocess_result and reprocess_result.should_reprocess:
                            logger.info("‚úÖ [Expert Service] Retraitement automatique activ√©")
                            ai_enhancements_used.append("automatic_reprocessing")
                            processing_steps.append("automatic_reprocessing")
                            return None  # Continue avec le traitement normal
                
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è [Expert Service] Erreur v√©rification retraitement: {e}")
        
        # Analyse de clarification normale
        if not request_data.is_clarification_response and not request_data.force_reprocess:
            if self.integrations.enhanced_clarification_available and self.integrations.is_enhanced_clarification_enabled():
                try:
                    clarification_context = {}
                    if self.integrations.intelligent_memory_available:
                        clarification_context = self.integrations.get_context_for_clarification(conversation_id)
                        ai_enhancements_used.append("intelligent_clarification_context")
                    
                    clarification_result = await self.integrations.analyze_question_for_clarification_enhanced(
                        question=question_text,
                        language=request_data.language,
                        user_id=user_id,
                        conversation_id=conversation_id,
                        conversation_context=clarification_context,
                        original_question=question_text
                    )
                    
                    processing_steps.append("enhanced_clarification_analysis")
                    
                    if clarification_result.needs_clarification:
                        logger.info(f"‚ùì [Expert Service] Clarification n√©cessaire")
                        
                        clarification_response = self.integrations.format_clarification_response_enhanced(
                            result=clarification_result,
                            language=request_data.language
                        )
                        
                        # Enregistrer clarification dans m√©moire
                        if self.integrations.intelligent_memory_available:
                            try:
                                self.integrations.add_message_to_conversation(
                                    conversation_id=conversation_id,
                                    user_id=user_id,
                                    message=clarification_response,
                                    role="assistant",
                                    language=request_data.language,
                                    message_type="clarification"
                                )
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è [Expert Service] Erreur enregistrement clarification: {e}")
                        
                        # Retourner la r√©ponse de clarification
                        return EnhancedExpertResponse(
                            question=str(question_text),
                            response=str(clarification_response),
                            conversation_id=conversation_id,
                            rag_used=False,
                            rag_score=None,
                            timestamp=datetime.now().isoformat(),
                            language=request_data.language,
                            response_time_ms=0,  # Sera calcul√© plus tard
                            mode="enhanced_clarification_needed",
                            user=None,
                            logged=True,
                            validation_passed=True,
                            clarification_result=clarification_result.to_dict(),
                            processing_steps=processing_steps,
                            ai_enhancements_used=ai_enhancements_used
                        )
                
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è [Expert Service] Erreur clarification: {e}")
        
        return None  # Pas de clarification n√©cessaire
    
    async def _process_expert_response(
        self, question_text: str, request_data: EnhancedQuestionRequest,
        request: Request, current_user: Optional[Dict], conversation_id: str,
        processing_steps: list, ai_enhancements_used: list
    ) -> Dict[str, Any]:
        """
        VERSION CORRIG√âE - RAG OBLIGATOIRE avec am√©lioration contextuelle
        """
        
        # === 1. R√âCUP√âRER CONTEXTE CONVERSATIONNEL ===
        conversation_context_str = ""
        if self.integrations.intelligent_memory_available:
            try:
                conversation_context_str = self.integrations.get_context_for_rag(conversation_id, max_chars=800)
                if conversation_context_str:
                    ai_enhancements_used.append("contextual_rag")
                    logger.info(f"üß† Contexte conversationnel r√©cup√©r√©: {conversation_context_str[:100]}...")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur r√©cup√©ration contexte: {e}")
        
        # === 2. AM√âLIORATION INTELLIGENTE DE LA QUESTION ===
        enriched_question, enhancement_info = self.rag_enhancer.enhance_question_for_rag(
            question=question_text,
            conversation_context=conversation_context_str,
            language=request_data.language
        )
        
        if enhancement_info["question_enriched"]:
            ai_enhancements_used.append("intelligent_question_enhancement")
            logger.info(f"‚ú® Question am√©lior√©e: {enriched_question[:150]}...")
        
        if enhancement_info["pronoun_detected"]:
            ai_enhancements_used.append("contextual_pronoun_resolution")
            logger.info(f"üéØ Pronoms contextuels r√©solus: {enhancement_info['context_entities_used']}")
        
        processing_steps.append("intelligent_question_enhancement")
        
        # === 3. V√âRIFICATION RAG DISPONIBLE ===
        app = request.app
        process_rag = getattr(app.state, 'process_question_with_rag', None)
        
        if not process_rag:
            logger.error("‚ùå Syst√®me RAG indisponible - Erreur critique")
            raise HTTPException(
                status_code=503, 
                detail={
                    "error": "Service RAG indisponible",
                    "message": "Le syst√®me expert n√©cessite l'acc√®s √† la base documentaire pour r√©pondre. Veuillez r√©essayer plus tard.",
                    "technical_details": "process_question_with_rag not available in app.state",
                    "suggestion": "V√©rifiez que le syst√®me RAG est correctement initialis√©"
                }
            )
        
        # === 4. APPEL RAG AVEC QUESTION AM√âLIOR√âE ===
        try:
            logger.info("üîç Appel RAG avec question intelligemment am√©lior√©e...")
            logger.info(f"üìù Question originale: {question_text}")
            logger.info(f"‚ú® Question enrichie: {enriched_question}")
            
            # Essayer d'abord avec le param√®tre context
            try:
                result = await process_rag(
                    question=enriched_question,
                    user=current_user,
                    language=request_data.language,
                    speed_mode=request_data.speed_mode,
                    conversation_id=conversation_id,
                    context=conversation_context_str
                )
                logger.info("‚úÖ RAG appel√© avec param√®tre context")
            except TypeError as te:
                logger.info(f"‚ÑπÔ∏è Param√®tre context non support√©: {te}")
                # Fallback sans param√®tre context (question reste enrichie)
                result = await process_rag(
                    question=enriched_question,
                    user=current_user,
                    language=request_data.language,
                    speed_mode=request_data.speed_mode,
                    conversation_id=conversation_id
                )
                logger.info("‚úÖ RAG appel√© sans param√®tre context")
            
            # === 5. TRAITEMENT R√âSULTAT RAG ===
            answer = str(result.get("response", ""))
            rag_score = result.get("score")
            original_mode = result.get("mode", "rag_processing")
            
            # Mode enrichi
            mode = f"enhanced_contextual_{original_mode}"
            
            processing_steps.append("mandatory_rag_with_context")
            
            # === 6. VALIDATION QUALIT√â R√âPONSE ===
            quality_check = self._validate_rag_response_quality(
                answer, enriched_question, enhancement_info
            )
            
            if not quality_check["valid"]:
                logger.warning(f"‚ö†Ô∏è Qualit√© RAG insuffisante: {quality_check['reason']}")
                ai_enhancements_used.append("quality_validation_failed")
                
                # On peut ajouter ici une logique de retry avec question reformul√©e
                # Mais JAMAIS de fallback vers donn√©es cod√©es !
            
            logger.info(f"‚úÖ RAG r√©ponse re√ßue: {len(answer)} caract√®res, score: {rag_score}")
            
            return {
                "answer": answer,
                "rag_used": True,  # Toujours vrai maintenant
                "rag_score": rag_score,
                "mode": mode,
                "context_used": bool(conversation_context_str),
                "question_enriched": enhancement_info["question_enriched"],
                "enhancement_info": enhancement_info,
                "quality_check": quality_check
            }
            
        except Exception as rag_error:
            logger.error(f"‚ùå Erreur critique RAG: {rag_error}")
            processing_steps.append("rag_error")
            
            # === 7. GESTION D'ERREUR - AUCUN FALLBACK DONN√âES COD√âES ===
            error_details = {
                "error": "Erreur RAG",
                "message": "Impossible d'interroger la base documentaire pour cette question.",
                "technical_details": str(rag_error),
                "question_original": question_text,
                "question_enriched": enriched_question,
                "context_available": bool(conversation_context_str),
                "suggestion": "Veuillez reformuler votre question ou r√©essayer plus tard."
            }
            
            raise HTTPException(
                status_code=503,
                detail=error_details
            )
    
    def _validate_rag_response_quality(
        self, 
        answer: str, 
        enriched_question: str, 
        enhancement_info: Dict
    ) -> Dict[str, any]:
        """Valide la qualit√© de la r√©ponse RAG"""
        
        if not answer or len(answer.strip()) < 20:
            return {
                "valid": False,
                "reason": "R√©ponse trop courte",
                "answer_length": len(answer) if answer else 0
            }
        
        # V√©rifier si la r√©ponse contient "je ne sais pas" ou √©quivalents
        negative_responses = [
            "je ne sais pas", "i don't know", "no s√©",
            "pas d'information", "no information", "sin informaci√≥n",
            "donn√©es non disponibles", "data not available", "datos no disponibles"
        ]
        
        answer_lower = answer.lower()
        for negative in negative_responses:
            if negative in answer_lower:
                return {
                    "valid": False,
                    "reason": f"R√©ponse n√©gative d√©tect√©e: {negative}",
                    "answer_length": len(answer)
                }
        
        # Si question concernait des donn√©es sp√©cifiques (poids, √¢ge), v√©rifier pr√©sence de chiffres
        if any(word in enriched_question.lower() for word in ["poids", "weight", "peso", "gramme", "gram", "kg"]):
            if not re.search(r'\d+', answer):
                return {
                    "valid": False,
                    "reason": "Question sur donn√©es num√©riques mais pas de chiffres dans la r√©ponse",
                    "answer_length": len(answer)
                }
        
        return {
            "valid": True,
            "reason": "R√©ponse valide",
            "answer_length": len(answer)
        }
    
    def _create_rejection_response(
        self, question_text: str, validation_result: ValidationResult,
        conversation_id: str, user_email: Optional[str], language: str,
        start_time: float, processing_steps: list, ai_enhancements_used: list
    ) -> EnhancedExpertResponse:
        """Cr√©e une r√©ponse de rejet"""
        
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return EnhancedExpertResponse(
            question=str(question_text),
            response=str(validation_result.rejection_message),
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode="enhanced_agricultural_validation_rejected",
            user=user_email,
            logged=True,
            validation_passed=False,
            validation_confidence=validation_result.confidence,
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used
        )
    
    def _build_final_response(
        self, question_text: str, answer: str, conversation_id: str,
        user_email: Optional[str], language: str, response_time_ms: int,
        expert_result: Dict, validation_result: ValidationResult,
        conversation_context: Any, processing_steps: list,
        ai_enhancements_used: list, request_data: EnhancedQuestionRequest
    ) -> EnhancedExpertResponse:
        """Construit la r√©ponse finale"""
        
        # M√©triques finales
        extracted_entities = None
        confidence_overall = None
        conversation_state = None
        
        if conversation_context and hasattr(conversation_context, 'consolidated_entities'):
            extracted_entities = conversation_context.consolidated_entities.to_dict()
            confidence_overall = conversation_context.consolidated_entities.confidence_overall
            conversation_state = conversation_context.conversation_urgency
        
        return EnhancedExpertResponse(
            question=str(question_text),
            response=str(answer),
            conversation_id=conversation_id,
            rag_used=expert_result["rag_used"],
            rag_score=expert_result["rag_score"],
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode=expert_result["mode"],
            user=user_email,
            logged=True,
            validation_passed=True,
            validation_confidence=validation_result.confidence,
            reprocessed_after_clarification=request_data.is_clarification_response,
            conversation_state=conversation_state,
            extracted_entities=extracted_entities,
            confidence_overall=confidence_overall,
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used
        )

# =============================================================================
# CONFIGURATION
# =============================================================================

logger.info("‚úÖ [Expert Service] Services m√©tier initialis√©s avec RAG-First obligatoire")