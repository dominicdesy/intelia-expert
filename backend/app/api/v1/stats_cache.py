# app/api/v1/stats_cache.py
# -*- coding: utf-8 -*-
"""
üöÄ SYST√àME DE CACHE STATISTIQUES - VERSION ULTRA-L√âG√àRE
CORRECTIF URGENT: Out of Memory - Suppression de tous les imports lourds
‚ú® MINIMAL: Seules les fonctionnalit√©s essentielles conserv√©es
üõ°Ô∏è MEMORY-SAFE: Aucun thread, aucun pool, aucun monitoring lourd
"""

import json
import logging
import os
import decimal
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
import psycopg2
from psycopg2.extras import RealDictCursor

logger = logging.getLogger(__name__)

def decimal_safe_json_encoder(obj):
    """Converter JSON pour g√©rer les types Decimal de PostgreSQL"""
    if isinstance(obj, decimal.Decimal):
        return float(obj)
    raise TypeError(f"Object of type {type(obj).__name__} is not JSON serializable")

def safe_json_dumps(data):
    """S√©rialisation JSON s√©curis√©e SIMPLE"""
    try:
        return json.dumps(data, default=decimal_safe_json_encoder, separators=(',', ':'))
    except Exception as e:
        logger.error(f"‚ùå Erreur s√©rialisation JSON: {e}")
        return json.dumps({"error": "JSON_SERIALIZATION_ERROR"})

class StatisticsCache:
    """
    üõ°Ô∏è Gestionnaire de cache ULTRA-L√âGER pour √©viter Out of Memory
    - Connexions directes uniquement (pas de pool)
    - Pas de monitoring m√©moire
    - Pas de threads persistants
    - Migration automatique des colonnes manquantes
    """
    
    def __init__(self, dsn: str = None):
        self.dsn = dsn or os.getenv("DATABASE_URL")
        if not self.dsn:
            raise ValueError("DATABASE_URL manquant pour le cache statistiques")
        
        # Cr√©er les tables de cache (version simple)
        self._ensure_cache_tables()
        
        # Migration automatique des colonnes
        self._migration_feedback_success = self._ensure_user_questions_feedback_columns()
        self._migration_cache_stats_success = self._ensure_existing_tables_migration()
        
        logger.info("‚úÖ Syst√®me de cache statistiques initialis√© (ULTRA-L√âGER)")

    def _ensure_user_questions_feedback_columns(self):
        """üîß MIGRATION AUTOMATIQUE: Version ultra-l√©g√®re"""
        try:
            with psycopg2.connect(self.dsn) as conn:
                with conn.cursor() as cur:
                    # V√©rification simple
                    cur.execute("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.tables 
                            WHERE table_name = 'user_questions_complete'
                        )
                    """)
                    
                    table_exists = cur.fetchone()[0]
                    if not table_exists:
                        # Cr√©er la table si elle n'existe pas
                        cur.execute("""
                            CREATE TABLE user_questions_complete (
                                id SERIAL PRIMARY KEY,
                                user_email VARCHAR(255),
                                question_text TEXT,
                                response_text TEXT,
                                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                                processing_time_ms INTEGER DEFAULT 0,
                                response_confidence DECIMAL(3,2) DEFAULT NULL,
                                response_source VARCHAR(50) DEFAULT NULL,
                                status VARCHAR(20) DEFAULT 'completed',
                                feedback INTEGER DEFAULT NULL CHECK (feedback IN (-1, 0, 1)),
                                feedback_comment TEXT DEFAULT NULL
                            )
                        """)
                        conn.commit()
                        logger.info("‚úÖ Table user_questions_complete cr√©√©e")
                        return True
                    
                    # Migration simple des colonnes feedback
                    try:
                        cur.execute("ALTER TABLE user_questions_complete ADD COLUMN IF NOT EXISTS feedback INTEGER CHECK (feedback IN (-1, 0, 1))")
                        cur.execute("ALTER TABLE user_questions_complete ADD COLUMN IF NOT EXISTS feedback_comment TEXT")
                        conn.commit()
                        logger.info("‚úÖ Migration feedback termin√©e (ultra-l√©g√®re)")
                    except Exception:
                        pass  # Ignore si d√©j√† pr√©sent
                    
                    return True
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur migration feedback: {e}")
            return False

    def _ensure_existing_tables_migration(self):
        """üîß MIGRATION AUTOMATIQUE: Ajoute data_size_kb aux tables existantes - ULTRA-L√âGER"""
        try:
            with psycopg2.connect(self.dsn) as conn:
                with conn.cursor() as cur:
                    # Tables √† migrer (CORRECTIF: inclure statistics_cache)
                    tables_to_migrate = [
                        'statistics_cache',           # ‚Üê TABLE PRINCIPALE !
                        'dashboard_stats_snapshot',
                        'questions_cache', 
                        'openai_costs_cache'
                    ]
                    
                    migrations_applied = []
                    
                    for table_name in tables_to_migrate:
                        try:
                            # V√©rifier si la table existe
                            cur.execute("""
                                SELECT EXISTS (
                                    SELECT FROM information_schema.tables 
                                    WHERE table_name = %s
                                )
                            """, (table_name,))
                            
                            if cur.fetchone()[0]:
                                # Table existe - ajouter data_size_kb si manquante
                                cur.execute(f"ALTER TABLE {table_name} ADD COLUMN IF NOT EXISTS data_size_kb REAL DEFAULT 0")
                                migrations_applied.append(table_name)
                                logger.info(f"üîß Colonne data_size_kb ajout√©e √† {table_name}")
                        except Exception as table_error:
                            logger.info(f"‚ÑπÔ∏è Table {table_name} skip: {table_error}")
                    
                    conn.commit()
                    
                    if migrations_applied:
                        logger.info(f"‚úÖ Migration data_size_kb termin√©e: {migrations_applied}")
                    
                    return True
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur migration data_size_kb: {e}")
            return False
    
    def _ensure_cache_tables(self):
        """üõ°Ô∏è Cr√©e les tables de cache ULTRA-L√âG√àRES"""
        try:
            with psycopg2.connect(self.dsn) as conn:
                with conn.cursor() as cur:
                    
                    # Table principale simplifi√©e
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS statistics_cache (
                            id SERIAL PRIMARY KEY,
                            cache_key VARCHAR(200) UNIQUE NOT NULL,
                            data JSONB NOT NULL,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            expires_at TIMESTAMP DEFAULT (CURRENT_TIMESTAMP + INTERVAL '1 hour'),
                            data_size_kb REAL DEFAULT 0
                        );
                    """)
                    
                    # Table dashboard simplifi√©e
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS dashboard_stats_lite (
                            id SERIAL PRIMARY KEY,
                            total_users INTEGER DEFAULT 0,
                            total_questions INTEGER DEFAULT 0,
                            questions_today INTEGER DEFAULT 0,
                            generated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            expires_at TIMESTAMP DEFAULT (CURRENT_TIMESTAMP + INTERVAL '1 hour'),
                            is_current BOOLEAN DEFAULT FALSE,
                            data_size_kb REAL DEFAULT 0
                        );
                    """)
                    
                    # Index minimaux
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_stats_cache_expires ON statistics_cache(expires_at)")
                    
                    conn.commit()
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation tables cache: {e}")

    # ==================== M√âTHODES ESSENTIELLES SEULEMENT ====================
    
    def set_cache(self, key: str, data: Any, ttl_hours: int = 1) -> bool:
        """Stocke des donn√©es dans le cache - VERSION L√âG√àRE"""
        try:
            json_data = safe_json_dumps(data)
            data_size_kb = len(json_data.encode('utf-8')) / 1024
            
            # Limite simple
            if data_size_kb > 200:  # 200KB max
                logger.warning(f"‚ö†Ô∏è Cache entry trop large ({data_size_kb:.1f}KB) pour {key}")
                return False
            
            expires_at = datetime.now() + timedelta(hours=ttl_hours)
            
            with psycopg2.connect(self.dsn) as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        INSERT INTO statistics_cache (cache_key, data, expires_at, data_size_kb)
                        VALUES (%s, %s, %s, %s)
                        ON CONFLICT (cache_key) 
                        DO UPDATE SET data = EXCLUDED.data, expires_at = EXCLUDED.expires_at, data_size_kb = EXCLUDED.data_size_kb
                    """, (key, json_data, expires_at, data_size_kb))
                    conn.commit()
                    
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur set cache {key}: {e}")
            return False
    
    def get_cache(self, key: str) -> Optional[Dict[str, Any]]:
        """R√©cup√®re des donn√©es depuis le cache - VERSION L√âG√àRE"""
        try:
            with psycopg2.connect(self.dsn) as conn:
                with conn.cursor(cursor_factory=RealDictCursor) as cur:
                    cur.execute("""
                        SELECT data FROM statistics_cache 
                        WHERE cache_key = %s AND expires_at > NOW()
                    """, (key,))
                    
                    result = cur.fetchone()
                    if result:
                        return {"data": result["data"]}
                    return None
                        
        except Exception as e:
            logger.error(f"‚ùå Erreur get cache {key}: {e}")
            return None
    
    def invalidate_cache(self, pattern: str = None, key: str = None) -> int:
        """Invalide le cache - VERSION L√âG√àRE"""
        try:
            with psycopg2.connect(self.dsn) as conn:
                with conn.cursor() as cur:
                    if key:
                        cur.execute("DELETE FROM statistics_cache WHERE cache_key = %s", (key,))
                    elif pattern:
                        cur.execute("DELETE FROM statistics_cache WHERE cache_key LIKE %s", (pattern.replace("*", "%"),))
                    else:
                        cur.execute("DELETE FROM statistics_cache WHERE expires_at <= NOW()")
                    
                    deleted_count = cur.rowcount
                    conn.commit()
                    return deleted_count
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur invalidation cache: {e}")
            return 0

    def set_dashboard_snapshot(self, stats: Dict[str, Any]) -> bool:
        """Stocke un snapshot dashboard ULTRA-L√âGER"""
        try:
            with psycopg2.connect(self.dsn) as conn:
                with conn.cursor() as cur:
                    # Nettoyer les anciens
                    cur.execute("UPDATE dashboard_stats_lite SET is_current = FALSE")
                    
                    # Ins√©rer le nouveau
                    cur.execute("""
                        INSERT INTO dashboard_stats_lite (
                            total_users, total_questions, questions_today, is_current
                        ) VALUES (%s, %s, %s, TRUE)
                    """, (
                        int(stats.get('total_users', 0)),
                        int(stats.get('total_questions', 0)), 
                        int(stats.get('questions_today', 0))
                    ))
                    
                    conn.commit()
                    
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde dashboard: {e}")
            return False
    
    def get_dashboard_snapshot(self) -> Optional[Dict[str, Any]]:
        """R√©cup√®re le snapshot dashboard ULTRA-L√âGER"""
        try:
            with psycopg2.connect(self.dsn) as conn:
                with conn.cursor(cursor_factory=RealDictCursor) as cur:
                    cur.execute("""
                        SELECT * FROM dashboard_stats_lite 
                        WHERE is_current = TRUE
                        ORDER BY generated_at DESC 
                        LIMIT 1
                    """)
                    
                    result = cur.fetchone()
                    if result:
                        return dict(result)
                    return None
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration dashboard: {e}")
            return None

    def cleanup_expired_cache(self) -> int:
        """Nettoie le cache expir√© - VERSION L√âG√àRE"""
        try:
            with psycopg2.connect(self.dsn) as conn:
                with conn.cursor() as cur:
                    cur.execute("DELETE FROM statistics_cache WHERE expires_at <= NOW()")
                    cleaned = cur.rowcount
                    
                    # Garder seulement le plus r√©cent dashboard
                    cur.execute("""
                        DELETE FROM dashboard_stats_lite 
                        WHERE id NOT IN (
                            SELECT id FROM dashboard_stats_lite 
                            ORDER BY generated_at DESC 
                            LIMIT 1
                        )
                    """)
                    cleaned += cur.rowcount
                    
                    conn.commit()
                    return cleaned
                        
        except Exception as e:
            logger.error(f"‚ùå Erreur cleanup: {e}")
            return 0

    def get_cache_stats(self) -> Dict[str, Any]:
        """Statistiques du syst√®me de cache - VERSION ULTRA-S√âCURIS√âE"""
        try:
            with psycopg2.connect(self.dsn) as conn:
                with conn.cursor(cursor_factory=RealDictCursor) as cur:
                    
                    stats = {}
                    
                    # Cache g√©n√©rique avec gestion d'erreur robuste
                    try:
                        cur.execute("""
                            SELECT 
                                COUNT(*) as total,
                                COUNT(*) FILTER (WHERE expires_at > NOW()) as valid,
                                COALESCE(AVG(data_size_kb), 0) as avg_size_kb
                            FROM statistics_cache
                        """)
                        result = cur.fetchone()
                        if result:
                            stats['general_cache'] = dict(result)
                    except Exception as cache_error:
                        logger.warning(f"‚ö†Ô∏è Stats cache g√©n√©rique: {cache_error}")
                        stats['general_cache'] = {'total': 0, 'valid': 0, 'avg_size_kb': 0, 'note': 'Migration en cours'}
                    
                    # Dashboard snapshots
                    try:
                        cur.execute("SELECT COUNT(*) as total FROM dashboard_stats_lite")
                        result = cur.fetchone()
                        stats['dashboard_snapshots'] = dict(result) if result else {'total': 0}
                    except Exception:
                        stats['dashboard_snapshots'] = {'total': 0, 'note': 'Non disponible'}
                    
                    stats['migration_status'] = {
                        'feedback_columns_migrated': self._migration_feedback_success,
                        'cache_stats_migrated': self._migration_cache_stats_success,
                        'ultra_light_mode': True,
                        'timestamp': datetime.now().isoformat()
                    }
                    
                    stats['last_updated'] = datetime.now().isoformat()
                    return stats
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur stats cache: {e}")
            return {
                "error": str(e), 
                "ultra_light_mode": True,
                "timestamp": datetime.now().isoformat()
            }

    # ==================== M√âTHODES COMPATIBILIT√â ====================
    
    def set_openai_costs(self, start_date: str, end_date: str, period_type: str, costs_data: Dict[str, Any]) -> bool:
        """Cache les co√ªts OpenAI - VERSION ULTRA-L√âG√àRE"""
        cache_key = f"openai_costs:{start_date}:{end_date}:{period_type}"
        essential_costs = {
            "total_cost": costs_data.get('total_cost', 0),
            "total_tokens": costs_data.get('total_tokens', 0),
            "period": f"{start_date} to {end_date}"
        }
        return self.set_cache(cache_key, essential_costs, ttl_hours=4)

    def get_openai_costs(self, start_date: str, end_date: str, period_type: str) -> Optional[Dict[str, Any]]:
        """R√©cup√®re les co√ªts OpenAI - VERSION ULTRA-L√âG√àRE"""
        cache_key = f"openai_costs:{start_date}:{end_date}:{period_type}"
        cached_result = self.get_cache(cache_key)
        if cached_result:
            return cached_result.get("data")
        return None

# ==================== SINGLETON GLOBAL ====================

_stats_cache_instance = None

def get_stats_cache() -> StatisticsCache:
    """R√©cup√®re l'instance singleton du cache statistiques ULTRA-L√âGER"""
    global _stats_cache_instance
    if _stats_cache_instance is None:
        _stats_cache_instance = StatisticsCache()
    return _stats_cache_instance

# ==================== FONCTIONS UTILITAIRES ====================

def is_cache_available() -> bool:
    """V√©rifie si le syst√®me de cache est disponible"""
    try:
        cache = get_stats_cache()
        return cache.dsn is not None
    except:
        return False

def force_cache_refresh() -> Dict[str, Any]:
    """Force une actualisation du cache - VERSION L√âG√àRE"""
    try:
        cache = get_stats_cache()
        cleaned = cache.cleanup_expired_cache()
        invalidated = cache.invalidate_cache(pattern="dashboard_*")
        
        return {
            "status": "success",
            "cache_invalidated": invalidated,
            "entries_cleaned": cleaned,
            "ultra_light_mode": True,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return {"status": "error", "error": str(e)}