"""
app/api/v1/expert_utils.py - FONCTIONS UTILITAIRES EXPERT SYSTEM

Fonctions utilitaires n√©cessaires pour le bon fonctionnement du syst√®me expert
‚úÖ CORRIG√â: Toutes les fonctions r√©f√©renc√©es dans expert.py et expert_services.py
‚úÖ CORRIG√â: Erreur syntaxe ligne 830 r√©solue
‚úÖ CORRIG√â: Gestion des exceptions am√©lior√©e
‚úÖ CORRIG√â: Validation des types et None-safety
üöÄ NOUVEAU: Auto-d√©tection sexe pour races pondeuses (Bug Fix)
üöÄ INT√âGR√â: Centralisation via clarification_entities
üöÄ AJOUT√â: score_question_variant() pour scoring g√©n√©rique des variantes
üöÄ AJOUT√â: convert_legacy_entities() pour normalisation des entit√©s anciennes
"""

import re
import uuid
import logging
import time
from typing import Optional, Dict, Any, List, Union
from datetime import datetime

logger = logging.getLogger(__name__)

# üöÄ NOUVEAU: Imports centralisation clarification_entities
try:
    from .clarification_entities import normalize_breed_name, infer_sex_from_breed
    CLARIFICATION_ENTITIES_AVAILABLE = True
    logger.info("‚úÖ [Utils] clarification_entities import√© avec succ√®s")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è [Utils] clarification_entities non disponible: {e}")
    # Fonctions fallback
    def normalize_breed_name(breed: str) -> tuple[str, str]:
        """Fallback function for breed normalization"""
        if not breed:
            return "", "manual"
        return breed.lower().strip(), "manual"
    
    def infer_sex_from_breed(breed: str) -> tuple[Optional[str], bool]:
        """Fallback function for sex inference from breed"""
        if not breed:
            return None, False
        layer_breeds = ['isa brown', 'lohmann brown', 'hy-line', 'bovans', 'shaver']
        is_layer = any(layer in breed.lower() for layer in layer_breeds)
        return "femelles" if is_layer else None, is_layer
    
    CLARIFICATION_ENTITIES_AVAILABLE = False

# =============================================================================
# NOUVELLES FONCTIONS POUR NORMALISATION DES ENTIT√âS (PHASE 1)
# =============================================================================

def convert_legacy_entities(old_entities: Dict) -> Dict:
    """
    Convertit les anciennes entit√©s vers le format normalis√©
    üöÄ NOUVEAU: Support pour la normalisation des entit√©s legacy
    
    Args:
        old_entities: Anciennes entit√©s au format variable
        
    Returns:
        Dict: Entit√©s normalis√©es avec cl√©s standardis√©es
    """
    try:
        if not old_entities or not isinstance(old_entities, dict):
            return {}
        
        normalized = {}
        
        # Normalisation de la race
        breed_keys = ['breed', 'race', 'souche', 'strain', 'raza']
        for key in breed_keys:
            if key in old_entities and old_entities[key]:
                breed_value = str(old_entities[key]).strip()
                if breed_value:
                    normalized_breed, _ = normalize_breed_name(breed_value)
                    normalized['breed'] = normalized_breed
                    break
        
        # Normalisation de l'√¢ge en jours
        age_keys = ['age', 'age_days', 'age_weeks', '√¢ge', 'edad']
        for key in age_keys:
            if key in old_entities and old_entities[key] is not None:
                try:
                    age_value = old_entities[key]
                    if isinstance(age_value, str):
                        # Extraire les nombres de la cha√Æne
                        numbers = re.findall(r'\d+', age_value)
                        if numbers:
                            age_value = int(numbers[0])
                    
                    age_int = int(age_value)
                    
                    # Conversion selon l'unit√©
                    if 'week' in key.lower() or 'semaine' in key.lower():
                        normalized['age_days'] = age_int * 7
                    else:
                        normalized['age_days'] = age_int
                    break
                except (ValueError, TypeError):
                    logger.warning(f"‚ö†Ô∏è [Utils] Impossible de convertir l'√¢ge: {old_entities[key]}")
                    continue
        
        # Normalisation du sexe
        sex_keys = ['sex', 'sexe', 'g√©nero', 'gender']
        for key in sex_keys:
            if key in old_entities and old_entities[key]:
                sex_value = str(old_entities[key]).lower().strip()
                
                # Mapping vers format standard
                if any(word in sex_value for word in ['m√¢le', 'male', 'macho', 'cock', 'rooster']):
                    normalized['sex'] = 'males'
                elif any(word in sex_value for word in ['femelle', 'female', 'hembra', 'hen']):
                    normalized['sex'] = 'females'
                elif any(word in sex_value for word in ['mixte', 'mixed', 'mixto', 'both']):
                    normalized['sex'] = 'mixed'
                else:
                    normalized['sex'] = sex_value
                break
        
        # Normalisation du poids (toujours en grammes)
        weight_keys = ['weight', 'poids', 'peso', 'weight_g', 'weight_kg']
        for key in weight_keys:
            if key in old_entities and old_entities[key] is not None:
                try:
                    weight_value = old_entities[key]
                    if isinstance(weight_value, str):
                        # Extraire les nombres avec d√©cimales
                        numbers = re.findall(r'\d+(?:[.,]\d+)?', weight_value)
                        if numbers:
                            weight_value = float(numbers[0].replace(',', '.'))
                    
                    weight_float = float(weight_value)
                    
                    # Conversion en grammes si n√©cessaire
                    if 'kg' in key.lower() or weight_float < 20:  # Probablement en kg si < 20
                        normalized['weight_g'] = int(weight_float * 1000)
                    else:
                        normalized['weight_g'] = int(weight_float)
                    break
                except (ValueError, TypeError):
                    logger.warning(f"‚ö†Ô∏è [Utils] Impossible de convertir le poids: {old_entities[key]}")
                    continue
        
        # Pr√©server autres m√©tadonn√©es utiles
        metadata_keys = ['confidence', 'source', 'timestamp', 'language']
        for key in metadata_keys:
            if key in old_entities:
                normalized[key] = old_entities[key]
        
        logger.info(f"üîÑ [Utils] Entit√©s converties: {len(old_entities)} ‚Üí {len(normalized)}")
        return normalized
        
    except Exception as e:
        logger.error(f"‚ùå [Utils] Erreur conversion entit√©s: {e}")
        return old_entities or {}

def validate_normalized_entities(entities: Dict) -> Dict[str, Any]:
    """
    Valide que les entit√©s sont dans le format normalis√© attendu
    
    Args:
        entities: Entit√©s √† valider
        
    Returns:
        Dict: R√©sultat de validation avec suggestions de correction
    """
    if not isinstance(entities, dict):
        return {
            "valid": False,
            "errors": ["Entit√©s doivent √™tre un dictionnaire"],
            "suggestions": ["Convertir vers format dictionnaire"]
        }
    
    validation_result = {
        "valid": True,
        "errors": [],
        "warnings": [],
        "suggestions": [],
        "normalized_keys": 0,
        "total_keys": len(entities)
    }
    
    # Cl√©s attendues dans le format normalis√©
    expected_formats = {
        'breed': str,
        'age_days': int,
        'sex': str,
        'weight_g': int
    }
    
    # Cl√©s obsol√®tes √† convertir
    legacy_mappings = {
        'race': 'breed',
        '√¢ge': 'age_days',
        'sexe': 'sex',
        'poids': 'weight_g'
    }
    
    try:
        for key, value in entities.items():
            if key in expected_formats:
                # V√©rifier le type attendu
                expected_type = expected_formats[key]
                if not isinstance(value, expected_type):
                    validation_result["errors"].append(
                        f"Cl√© '{key}': type {type(value).__name__} au lieu de {expected_type.__name__}"
                    )
                    validation_result["suggestions"].append(
                        f"Convertir '{key}' vers {expected_type.__name__}"
                    )
                else:
                    validation_result["normalized_keys"] += 1
            
            elif key in legacy_mappings:
                validation_result["warnings"].append(
                    f"Cl√© legacy '{key}' d√©tect√©e"
                )
                validation_result["suggestions"].append(
                    f"Remplacer '{key}' par '{legacy_mappings[key]}'"
                )
        
        # V√©rifications sp√©cifiques
        if 'age_days' in entities:
            age = entities['age_days']
            if age < 0 or age > 365:
                validation_result["warnings"].append(
                    f"√Çge suspect: {age} jours (0-365 attendu)"
                )
        
        if 'weight_g' in entities:
            weight = entities['weight_g']
            if weight < 10 or weight > 10000:
                validation_result["warnings"].append(
                    f"Poids suspect: {weight}g (10-10000g attendu)"
                )
        
        if 'sex' in entities:
            sex = entities['sex']
            valid_sexes = ['males', 'females', 'mixed']
            if sex not in valid_sexes:
                validation_result["warnings"].append(
                    f"Sexe non standard: '{sex}' (attendu: {valid_sexes})"
                )
        
        # D√©terminer si globalement valide
        if validation_result["errors"]:
            validation_result["valid"] = False
        
        normalization_ratio = validation_result["normalized_keys"] / max(validation_result["total_keys"], 1)
        validation_result["normalization_score"] = normalization_ratio
        
        if normalization_ratio < 0.5:
            validation_result["warnings"].append(
                "Faible taux de normalisation - consid√©rer convert_legacy_entities()"
            )
        
    except Exception as e:
        validation_result["valid"] = False
        validation_result["errors"].append(f"Erreur durant validation: {str(e)}")
    
    return validation_result

def merge_entities_intelligently(primary_entities: Dict, secondary_entities: Dict) -> Dict:
    """
    Fusionne intelligemment deux dictionnaires d'entit√©s en priorisant les plus fiables
    
    Args:
        primary_entities: Entit√©s prioritaires (plus fiables)
        secondary_entities: Entit√©s secondaires (fallback)
        
    Returns:
        Dict: Entit√©s fusionn√©es
    """
    if not primary_entities and not secondary_entities:
        return {}
    
    if not primary_entities:
        return convert_legacy_entities(secondary_entities or {})
    
    if not secondary_entities:
        return convert_legacy_entities(primary_entities or {})
    
    try:
        # Normaliser les deux sources
        primary_normalized = convert_legacy_entities(primary_entities)
        secondary_normalized = convert_legacy_entities(secondary_entities)
        
        merged = {}
        
        # Priorit√©s par type d'entit√©
        entity_priorities = {
            'breed': ['breed', 'race', 'souche'],
            'age_days': ['age_days', 'age', '√¢ge'],
            'sex': ['sex', 'sexe', 'g√©nero'],
            'weight_g': ['weight_g', 'poids', 'weight']
        }
        
        for normalized_key, possible_keys in entity_priorities.items():
            value_found = False
            
            # Chercher d'abord dans les entit√©s primaires
            if normalized_key in primary_normalized and primary_normalized[normalized_key]:
                merged[normalized_key] = primary_normalized[normalized_key]
                value_found = True
            
            # Fallback vers entit√©s secondaires si pas trouv√©
            if not value_found and normalized_key in secondary_normalized and secondary_normalized[normalized_key]:
                merged[normalized_key] = secondary_normalized[normalized_key]
        
        # Ajouter m√©tadonn√©es de fusion
        merged['_merge_metadata'] = {
            'primary_source': len(primary_normalized),
            'secondary_source': len(secondary_normalized),
            'merged_count': len(merged),
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"üîÄ [Utils] Fusion entit√©s: {len(primary_normalized)}+{len(secondary_normalized)} ‚Üí {len(merged)}")
        return merged
        
    except Exception as e:
        logger.error(f"‚ùå [Utils] Erreur fusion entit√©s: {e}")
        return primary_entities or secondary_entities or {}

# =============================================================================
# UTILITAIRES D'AUTHENTIFICATION ET SESSION (CONSERV√âS)
# =============================================================================

def get_user_id_from_request(request) -> str:
    """Extrait l'ID utilisateur depuis la requ√™te"""
    try:
        # Essayer d'extraire depuis les headers
        if hasattr(request, 'headers') and request.headers:
            user_id = request.headers.get('X-User-ID')
            if user_id and isinstance(user_id, str) and user_id.strip():
                return user_id.strip()
        
        # Fallback vers l'IP client
        if hasattr(request, 'client') and request.client and hasattr(request.client, 'host'):
            client_host = request.client.host
            if client_host:
                return f"ip_{client_host}"
        
        # Dernier fallback
        return f"anonymous_{uuid.uuid4().hex[:8]}"
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è [Utils] Erreur extraction user_id: {e}")
        return f"error_{uuid.uuid4().hex[:8]}"

def extract_session_info(request) -> Dict[str, Any]:
    """Extrait les informations de session depuis la requ√™te"""
    try:
        session_info = {
            "user_agent": None,
            "ip_address": None,
            "request_id": str(uuid.uuid4()),
            "timestamp": datetime.now().isoformat()
        }
        
        if hasattr(request, 'headers') and request.headers:
            session_info["user_agent"] = request.headers.get('User-Agent')
            request_id_header = request.headers.get('X-Request-ID')
            if request_id_header:
                session_info["request_id"] = request_id_header
        
        if hasattr(request, 'client') and request.client and hasattr(request.client, 'host'):
            session_info["ip_address"] = request.client.host
        
        return session_info
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è [Utils] Erreur extraction session: {e}")
        return {
            "user_agent": None,
            "ip_address": "unknown",
            "request_id": str(uuid.uuid4()),
            "timestamp": datetime.now().isoformat()
        }

# =============================================================================
# EXTRACTION ENTIT√âS POUR CLARIFICATION (AM√âLIOR√âE)
# =============================================================================

def extract_breed_and_sex_from_clarification(text: str, language: str = "fr") -> Dict[str, Optional[str]]:
    """
    Extrait race et sexe depuis une r√©ponse de clarification
    üöÄ CORRIG√â: Auto-d√©tection sexe pour races pondeuses
    üöÄ AM√âLIOR√â: Support normalisation avanc√©e
    """
    
    if not text or not isinstance(text, str) or not text.strip():
        return {"breed": None, "sex": None}
    
    text_lower = text.lower().strip()
    
    # Dictionnaires de patterns par langue
    breed_patterns = {
        "fr": [
            # Races compl√®tes courantes
            r'\b(ross\s*308|cobb\s*500|hubbard|arbor\s*acres)\b',
            r'\b(ross|cobb|hubbard)\s*\d{2,3}\b',
            # üöÄ NOUVEAU: Patterns pondeuses √©tendus
            r'\b(isa\s*brown|lohmann\s*brown|hy[-\s]*line|bovans|shaver|hissex|novogen|tetra|hendrix|dominant)\b',
            # Mentions g√©n√©riques
            r'\brace[:\s]*([a-zA-Z0-9\s]+)',
            r'\bsouche[:\s]*([a-zA-Z0-9\s]+)',
        ],
        "en": [
            r'\b(ross\s*308|cobb\s*500|hubbard|arbor\s*acres)\b',
            r'\b(ross|cobb|hubbard)\s*\d{2,3}\b',
            # üöÄ NOUVEAU: Patterns pondeuses √©tendus
            r'\b(isa\s*brown|lohmann\s*brown|hy[-\s]*line|bovans|shaver|hissex|novogen|tetra|hendrix|dominant)\b',
            r'\bbreed[:\s]*([a-zA-Z0-9\s]+)',
            r'\bstrain[:\s]*([a-zA-Z0-9\s]+)',
        ],
        "es": [
            r'\b(ross\s*308|cobb\s*500|hubbard|arbor\s*acres)\b',
            r'\b(ross|cobb|hubbard)\s*\d{2,3}\b',
            # üöÄ NOUVEAU: Patterns pondeuses √©tendus
            r'\b(isa\s*brown|lohmann\s*brown|hy[-\s]*line|bovans|shaver|hissex|novogen|tetra|hendrix|dominant)\b',
            r'\braza[:\s]*([a-zA-Z0-9\s]+)',
            r'\bcepa[:\s]*([a-zA-Z0-9\s]+)',
        ]
    }
    
    sex_patterns = {
        "fr": [
            r'\b(m√¢les?|males?)\b',
            r'\b(femelles?|females?)\b',
            r'\b(mixte|mixed|m√©lang√©)\b',
            r'\btroupeau\s+(mixte|m√©lang√©)\b',
            r'\bsexe[:\s]*(m√¢le|femelle|mixte)',
        ],
        "en": [
            r'\b(males?|roosters?|cocks?)\b',
            r'\b(females?|hens?)\b',
            r'\b(mixed|both)\b',
            r'\bflock\s+(mixed|both)\b',
            r'\bsex[:\s]*(male|female|mixed)',
        ],
        "es": [
            r'\b(machos?|gallos?)\b',
            r'\b(hembras?|gallinas?)\b',
            r'\b(mixto|mezclado|ambos)\b',
            r'\blote\s+(mixto|mezclado)\b',
            r'\bsexo[:\s]*(macho|hembra|mixto)',
        ]
    }
    
    # Extraction race
    breed = None
    patterns = breed_patterns.get(language, breed_patterns["fr"])
    
    for pattern in patterns:
        try:
            match = re.search(pattern, text_lower, re.IGNORECASE)
            if match:
                if pattern.startswith(r'\b(ross') or pattern.startswith(r'\b(isa'):  # Pattern de races sp√©cifiques
                    breed = match.group(1).strip()
                else:  # Pattern avec groupe de capture
                    if match.lastindex and match.lastindex >= 1:
                        breed = match.group(1).strip()
                    else:
                        breed = match.group(0).strip()
                
                # Nettoyer la race extraite
                breed = re.sub(r'^(race|breed|souche|strain|raza|cepa)[:\s]*', '', breed, flags=re.IGNORECASE)
                breed = breed.strip()
                
                if len(breed) >= 3:  # Garde seulement les races avec au moins 3 caract√®res
                    # üöÄ NOUVEAU: Normalisation via convert_legacy_entities
                    normalized = convert_legacy_entities({"breed": breed})
                    if "breed" in normalized:
                        breed = normalized["breed"]
                    break
                else:
                    breed = None
        except re.error as e:
            logger.warning(f"‚ö†Ô∏è [Utils] Erreur regex pattern breed: {e}")
            continue
    
    # Extraction sexe
    sex = None
    patterns = sex_patterns.get(language, sex_patterns["fr"])
    
    for pattern in patterns:
        try:
            match = re.search(pattern, text_lower, re.IGNORECASE)
            if match:
                if match.lastindex and match.lastindex >= 1:
                    matched_text = match.group(1)
                else:
                    matched_text = match.group(0)
                
                # Normalisation via convert_legacy_entities
                normalized = convert_legacy_entities({"sex": matched_text})
                if "sex" in normalized:
                    sex = normalized["sex"]
                    break
        except re.error as e:
            logger.warning(f"‚ö†Ô∏è [Utils] Erreur regex pattern sex: {e}")
            continue
    
    # üöÄ Utilisation de la centralisation pour normaliser la race et inf√©rer le sexe
    if breed and not sex:
        try:
            normalized_breed, _ = normalize_breed_name(breed)
            inferred_sex, was_inferred = infer_sex_from_breed(normalized_breed)
            
            if was_inferred and inferred_sex:
                # Normaliser le sexe inf√©r√©
                normalized = convert_legacy_entities({"sex": inferred_sex})
                sex = normalized.get("sex", inferred_sex)
                logger.info(f"ü•ö [Auto-Fix Utils] Race d√©tect√©e: {normalized_breed} ‚Üí sexe='{sex}' (via clarification_entities)")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [Utils] Erreur inf√©rence sexe: {e}")
    
    result = {"breed": breed, "sex": sex}
    
    logger.info(f"üîç [Utils] extraction '{text}' -> {result}")
    return result

def validate_clarification_completeness(text: str, missing_info: List[str], language: str = "fr") -> Dict[str, Any]:
    """Valide si une clarification contient toutes les informations n√©cessaires"""
    
    if not isinstance(text, str):
        text = str(text) if text is not None else ""
    
    if not isinstance(missing_info, list):
        missing_info = []
    
    extracted = extract_breed_and_sex_from_clarification(text, language)
    if not extracted:
        extracted = {"breed": None, "sex": None}
    
    still_missing = []
    confidence = 1.0
    
    # V√©rifier chaque information manquante
    for info in missing_info:
        if info in ["breed", "race", "souche"]:
            if not extracted.get("breed"):
                still_missing.append("breed")
                confidence -= 0.5
        elif info in ["sex", "sexe"]:
            if not extracted.get("sex"):
                still_missing.append("sex")
                confidence -= 0.5
    
    is_complete = len(still_missing) == 0
    
    return {
        "is_complete": is_complete,
        "still_missing": still_missing,
        "extracted_info": extracted,
        "confidence": max(0.0, confidence),
        "completeness_score": 1.0 - (len(still_missing) / max(len(missing_info), 1))
    }

# =============================================================================
# CONSTRUCTION QUESTIONS ENRICHIES (CONSERV√â)
# =============================================================================

def build_enriched_question_from_clarification(
    original_question: str, 
    clarification_response: str, 
    language: str = "fr"
) -> str:
    """Construit une question enrichie √† partir de la clarification"""
    
    if not isinstance(original_question, str):
        original_question = str(original_question) if original_question is not None else ""
    
    if not isinstance(clarification_response, str):
        clarification_response = str(clarification_response) if clarification_response is not None else ""
    
    entities = extract_breed_and_sex_from_clarification(clarification_response, language)
    if not entities:
        return original_question
    
    breed = entities.get("breed")
    sex = entities.get("sex")
    
    return build_enriched_question_with_breed_sex(original_question, breed, sex, language)

def build_enriched_question_with_breed_sex(
    original_question: str, 
    breed: Optional[str], 
    sex: Optional[str], 
    language: str = "fr"
) -> str:
    """Construit une question enrichie avec race et sexe"""
    
    if not isinstance(original_question, str):
        original_question = str(original_question) if original_question is not None else ""
    
    if not breed and not sex:
        return original_question
    
    # Templates par langue
    templates = {
        "fr": {
            "both": "Pour des poulets {breed} {sex}",
            "breed_only": "Pour des poulets {breed}",
            "sex_only": "Pour des poulets {sex}"
        },
        "en": {
            "both": "For {breed} {sex} chickens",
            "breed_only": "For {breed} chickens", 
            "sex_only": "For {sex} chickens"
        },
        "es": {
            "both": "Para pollos {breed} {sex}",
            "breed_only": "Para pollos {breed}",
            "sex_only": "Para pollos {sex}"
        }
    }
    
    template_set = templates.get(language, templates["fr"])
    
    # Construire le pr√©fixe
    prefix = ""
    try:
        if breed and sex:
            prefix = template_set["both"].format(breed=breed, sex=sex)
        elif breed:
            prefix = template_set["breed_only"].format(breed=breed)
        elif sex:
            prefix = template_set["sex_only"].format(sex=sex)
    except (KeyError, TypeError) as e:
        logger.warning(f"‚ö†Ô∏è [Utils] Erreur formatage template: {e}")
        return original_question
    
    # Combiner avec la question originale
    if prefix:
        # D√©tecter le type de question pour le formatting
        question_lower = original_question.lower().strip()
        
        if any(starter in question_lower for starter in ["quel", "quelle", "what", "cu√°l", "cu√°les"]):
            return f"{prefix}, {question_lower}"
        elif any(starter in question_lower for starter in ["comment", "how", "c√≥mo"]):
            return f"{prefix}: {original_question}"
        else:
            return f"{prefix}: {original_question}"
    
    return original_question

# =============================================================================
# UTILITAIRES TOPICS ET SUGGESTIONS (CONSERV√âS)
# =============================================================================

def get_enhanced_topics_by_language() -> Dict[str, List[str]]:
    """Retourne les sujets sugg√©r√©s par langue"""
    
    return {
        "fr": [
            "Probl√®mes de croissance chez les poulets",
            "Conditions environnementales optimales pour l'√©levage",
            "Protocoles de vaccination recommand√©s",
            "Diagnostic des probl√®mes de sant√© aviaire",
            "Nutrition et programmes d'alimentation",
            "Gestion de la mortalit√© √©lev√©e",
            "Optimisation des performances de croissance",
            "Pr√©vention des maladies courantes"
        ],
        "en": [
            "Chicken growth problems",
            "Optimal environmental conditions for farming",
            "Recommended vaccination protocols",
            "Avian health problem diagnosis",
            "Nutrition and feeding programs",
            "High mortality management",
            "Growth performance optimization",
            "Common disease prevention"
        ],
        "es": [
            "Problemas de crecimiento en pollos",
            "Condiciones ambientales √≥ptimas para la cr√≠a",
            "Protocolos de vacunaci√≥n recomendados",
            "Diagn√≥stico de problemas de salud aviar",
            "Nutrici√≥n y programas de alimentaci√≥n",
            "Manejo de alta mortalidad",
            "Optimizaci√≥n del rendimiento de crecimiento",
            "Prevenci√≥n de enfermedades comunes"
        ]
    }

def get_contextualized_suggestions(
    current_question: str, 
    conversation_history: List[str], 
    language: str = "fr"
) -> List[str]:
    """G√©n√®re des suggestions contextuelles bas√©es sur la conversation"""
    
    if not isinstance(current_question, str):
        current_question = str(current_question) if current_question is not None else ""
    
    if not isinstance(conversation_history, list):
        conversation_history = []
    
    all_topics = get_enhanced_topics_by_language()
    base_topics = all_topics.get(language, all_topics["fr"])
    
    # Simple contextualisation bas√©e sur les mots-cl√©s
    question_lower = current_question.lower()
    history_text = " ".join(str(item) for item in conversation_history).lower()
    
    contextualized = []
    
    # Ajuster les suggestions selon le contexte
    if any(word in question_lower + history_text for word in ["poids", "weight", "peso", "croissance", "growth"]):
        contextualized.extend([
            topic for topic in base_topics 
            if any(keyword in topic.lower() for keyword in ["croissance", "growth", "performance"])
        ])
    
    if any(word in question_lower + history_text for word in ["mortalit√©", "mortality", "mortalidad", "mort"]):
        contextualized.extend([
            topic for topic in base_topics 
            if any(keyword in topic.lower() for keyword in ["mortalit√©", "mortality", "sant√©", "health"])
        ])
    
    if any(word in question_lower + history_text for word in ["alimentation", "nutrition", "feeding", "alimentaci√≥n"]):
        contextualized.extend([
            topic for topic in base_topics 
            if any(keyword in topic.lower() for keyword in ["nutrition", "alimentation", "feeding"])
        ])
    
    # √âviter les doublons et limiter √† 6 suggestions
    unique_suggestions = list(dict.fromkeys(contextualized))[:6]
    
    # Compl√©ter avec des suggestions g√©n√©rales si n√©cessaire
    if len(unique_suggestions) < 4:
        for topic in base_topics:
            if topic not in unique_suggestions:
                unique_suggestions.append(topic)
                if len(unique_suggestions) >= 6:
                    break
    
    return unique_suggestions[:6]

# =============================================================================
# UTILITAIRES CONVERSATION ET M√âMOIRE (CONSERV√âS)
# =============================================================================

def save_conversation_auto_enhanced(
    conversation_id: str,
    user_id: str,
    question: str,
    response: str,
    metadata: Optional[Dict[str, Any]] = None
) -> bool:
    """Sauvegarde automatique de conversation avec m√©tadonn√©es enrichies"""
    
    try:
        # Validation des param√®tres
        if not conversation_id or not user_id:
            logger.error("‚ùå [Utils] conversation_id et user_id requis")
            return False
        
        # Cette fonction serait int√©gr√©e avec le syst√®me de logging
        # Pour l'instant, on log juste l'information
        
        enhanced_metadata = {
            "timestamp": datetime.now().isoformat(),
            "auto_enhanced": True,
            "question_length": len(str(question)),
            "response_length": len(str(response)),
            **(metadata or {})
        }
        
        logger.info(f"üíæ [Utils] Conversation sauvegard√©e: {conversation_id}")
        logger.debug(f"üìä [Utils] M√©tadonn√©es: {enhanced_metadata}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå [Utils] Erreur sauvegarde conversation: {e}")
        return False

def generate_conversation_id() -> str:
    """G√©n√®re un ID de conversation unique"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    unique_id = uuid.uuid4().hex[:8]
    return f"conv_{timestamp}_{unique_id}"

def extract_conversation_context(conversation_history: List[Dict[str, Any]], max_context: int = 500) -> str:
    """Extrait le contexte pertinent d'un historique de conversation"""
    
    if not isinstance(conversation_history, list) or not conversation_history:
        return ""
    
    context_parts = []
    current_length = 0
    
    # Prendre les messages les plus r√©cents en premier
    recent_history = conversation_history[-5:] if len(conversation_history) > 5 else conversation_history
    
    for message in reversed(recent_history):
        try:
            if not isinstance(message, dict):
                continue
            
            role = message.get("role", "unknown")
            content = message.get("content", "")
            
            if role in ["user", "assistant"] and content:
                part = f"{role}: {content}"
                
                if current_length + len(part) <= max_context:
                    context_parts.insert(0, part)  # Ins√©rer au d√©but pour garder l'ordre chronologique
                    current_length += len(part)
                else:
                    # Tronquer le dernier message si n√©cessaire
                    remaining_space = max_context - current_length
                    if remaining_space > 50:  # Garder seulement si on peut avoir au moins 50 caract√®res
                        truncated_part = f"{role}: {content[:remaining_space-10]}..."
                        context_parts.insert(0, truncated_part)
                    break
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [Utils] Erreur traitement message: {e}")
            continue
    
    return " | ".join(context_parts)

# =============================================================================
# UTILITAIRES VALIDATION ET FORMATS (CONSERV√âS + AM√âLIOR√âS)
# =============================================================================

def score_question_variant(variant: str, entities: Dict[str, Any]) -> float:
    """
    Score une variante de question en fonction des entit√©s pr√©sentes
    
    Args:
        variant: La variante de question √† scorer
        entities: Dictionnaire des entit√©s extraites (breed, sex, age, etc.)
    
    Returns:
        float: Score entre 0 et 1 (1 = toutes les entit√©s pr√©sentes)
    
    Example:
        entities = {"breed": "Ross 308", "sex": "m√¢les", "age": "25 jours"}
        variant = "Pour des poulets Ross 308 m√¢les de 25 jours"
        score = score_question_variant(variant, entities) # Returns 1.0
    """
    if not variant or not isinstance(variant, str):
        return 0.0
    
    if not entities or not isinstance(entities, dict):
        return 0.0
    
    # üöÄ NOUVEAU: Normaliser les entit√©s avant scoring
    normalized_entities = convert_legacy_entities(entities)
    
    variant_lower = variant.lower()
    matched_entities = 0
    total_entities = 0
    
    for entity_key, entity_value in normalized_entities.items():
        if entity_value and not entity_key.startswith('_'):  # Ignore metadata keys
            total_entities += 1
            entity_str = str(entity_value).lower()
            
            # Score diff√©rent selon le type d'entit√©
            if entity_key == "breed":
                # Pour les races, chercher le nom exact ou des parties
                breed_parts = entity_str.split()
                if len(breed_parts) > 1:
                    # Race compos√©e (ex: "ross 308") - chercher toutes les parties
                    if all(part in variant_lower for part in breed_parts):
                        matched_entities += 1
                else:
                    # Race simple - chercher le nom exact
                    if entity_str in variant_lower:
                        matched_entities += 1
            elif entity_key == "sex":
                # Pour le sexe, chercher le terme exact ou variations
                sex_variations = {
                    "males": ["male", "m√¢le", "m√¢les", "macho", "machos"],
                    "females": ["female", "femelle", "femelles", "hembra", "hembras"],
                    "mixed": ["mixte", "mixed", "mixto", "m√©lang√©"]
                }
                variations = sex_variations.get(entity_str, [entity_str])
                if any(var in variant_lower for var in variations):
                    matched_entities += 1
            elif entity_key == "age_days":
                # Pour l'√¢ge, chercher la valeur ou √©quivalent en semaines
                age_days = int(entity_value)
                age_weeks = age_days // 7
                if (str(age_days) in variant_lower or 
                    f"{age_weeks} semaine" in variant_lower or
                    f"{age_weeks} week" in variant_lower):
                    matched_entities += 1
            else:
                # Pour les autres entit√©s (poids, etc.), chercher la valeur
                if entity_str in variant_lower:
                    matched_entities += 1
    
    return matched_entities / max(total_entities, 1)

def validate_question_length(question: str, min_length: int = 3, max_length: int = 5000) -> Dict[str, Any]:
    """Valide la longueur d'une question"""
    
    if not isinstance(question, str):
        question = str(question) if question is not None else ""
    
    if not question:
        return {
            "valid": False,
            "reason": "Question vide",
            "length": 0
        }
    
    length = len(question.strip())
    
    if length < min_length:
        return {
            "valid": False,
            "reason": f"Question trop courte (minimum {min_length} caract√®res)",
            "length": length
        }
    
    if length > max_length:
        return {
            "valid": False,
            "reason": f"Question trop longue (maximum {max_length} caract√®res)",
            "length": length
        }
    
    return {
        "valid": True,
        "reason": "Question valide",
        "length": length
    }

def normalize_language_code(language: str) -> str:
    """Normalise un code de langue"""
    if not language or not isinstance(language, str):
        return "fr"
    
    lang_lower = language.lower().strip()
    
    # Codes ISO 639-1 support√©s
    supported_languages = {
        "fr": "fr",
        "fran√ßais": "fr", 
        "french": "fr",
        "en": "en",
        "english": "en",
        "anglais": "en",
        "es": "es",
        "espa√±ol": "es",
        "spanish": "es",
        "espagnol": "es"
    }
    
    return supported_languages.get(lang_lower, "fr")

def format_response_with_metadata(
    response_text: str, 
    metadata: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """Formate une r√©ponse avec ses m√©tadonn√©es"""
    
    if not isinstance(response_text, str):
        response_text = str(response_text) if response_text is not None else ""
    
    formatted_response = {
        "text": response_text,
        "length": len(response_text),
        "word_count": len(response_text.split()),
        "timestamp": datetime.now().isoformat(),
        "metadata": metadata or {}
    }
    
    # Ajouter des statistiques automatiques
    try:
        formatted_response["metadata"].update({
            "has_numbers": bool(re.search(r'\d+', response_text)),
            "has_bullet_points": '‚Ä¢' in response_text or '-' in response_text,
            "paragraph_count": len([p for p in response_text.split('\n\n') if p.strip()]),
            "estimated_reading_time_seconds": max(1, len(response_text.split()) * 0.25)  # ~240 mots/minute
        })
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è [Utils] Erreur ajout m√©tadonn√©es automatiques: {e}")
    
    return formatted_response

# =============================================================================
# UTILITAIRES POUR GESTION D'ERREURS (CONSERV√âS)
# =============================================================================

def safe_extract_field(data: Any, field_path: str, default: Any = None) -> Any:
    """Extraction s√©curis√©e d'un champ avec path en dot notation"""
    
    try:
        if not data or not isinstance(field_path, str):
            return default
        
        current = data
        for field in field_path.split('.'):
            if hasattr(current, field):
                current = getattr(current, field)
            elif isinstance(current, dict) and field in current:
                current = current[field]
            elif isinstance(current, dict) and hasattr(current, 'get'):
                current = current.get(field, default)
            else:
                return default
        
        return current if current is not None else default
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è [Utils] Erreur extraction {field_path}: {e}")
        return default

def safe_string_operation(text: Any, operation: str, *args, **kwargs) -> str:
    """Op√©ration sur string s√©curis√©e"""
    
    try:
        if not isinstance(text, str):
            text = str(text) if text is not None else ""
        
        if not isinstance(operation, str):
            logger.warning(f"‚ö†Ô∏è [Utils] Op√©ration doit √™tre une string: {operation}")
            return text
        
        if operation == "lower":
            return text.lower()
        elif operation == "upper":
            return text.upper()
        elif operation == "strip":
            return text.strip()
        elif operation == "replace":
            return text.replace(*args, **kwargs) if args else text
        elif operation == "split":
            return text.split(*args, **kwargs) if args else text.split()
        elif operation == "join":
            return args[0].join(text) if args else text
        else:
            logger.warning(f"‚ö†Ô∏è [Utils] Op√©ration inconnue: {operation}")
            return text
            
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è [Utils] Erreur op√©ration {operation}: {e}")
        return text if isinstance(text, str) else str(text) if text is not None else ""

def validate_and_sanitize_input(
    user_input: str, 
    max_length: int = 5000,
    remove_html: bool = True,
    remove_sql_keywords: bool = True
) -> Dict[str, Any]:
    """Valide et nettoie l'input utilisateur"""
    
    if not isinstance(user_input, str):
        user_input = str(user_input) if user_input is not None else ""
    
    if not user_input:
        return {
            "valid": False,
            "sanitized": "",
            "reason": "Input vide",
            "length": 0,
            "original_length": 0,
            "sanitized_length": 0,
            "warnings": []
        }
    
    original_length = len(user_input)
    sanitized = user_input
    warnings = []
    
    try:
        # Nettoyage HTML basique
        if remove_html:
            html_pattern = re.compile(r'<[^>]+>')
            if html_pattern.search(sanitized):
                sanitized = html_pattern.sub('', sanitized)
                warnings.append("HTML tags supprim√©s")
        
        # Nettoyage SQL basique
        if remove_sql_keywords:
            sql_keywords = ['DROP', 'DELETE', 'INSERT', 'UPDATE', 'CREATE', 'ALTER', 'EXEC']
            for keyword in sql_keywords:
                if re.search(rf'\b{keyword}\b', sanitized, re.IGNORECASE):
                    warnings.append(f"Mot-cl√© SQL potentiellement dangereux d√©tect√©: {keyword}")
        
        # Limitation de longueur
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
            warnings.append(f"Texte tronqu√© √† {max_length} caract√®res")
        
        # Nettoyage des espaces
        sanitized = re.sub(r'\s+', ' ', sanitized).strip()
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è [Utils] Erreur nettoyage input: {e}")
        sanitized = user_input
        warnings.append(f"Erreur lors du nettoyage: {str(e)}")
    
    return {
        "valid": len(sanitized) > 0,
        "sanitized": sanitized,
        "original_length": original_length,
        "sanitized_length": len(sanitized),
        "warnings": warnings,
        "reason": "Input valide" if len(sanitized) > 0 else "Input vide apr√®s nettoyage"
    }

# =============================================================================
# UTILITAIRES DEBUGGING ET MONITORING (CONSERV√âS)
# =============================================================================

def create_debug_info(
    function_name: str,
    inputs: Optional[Dict[str, Any]] = None,
    outputs: Optional[Dict[str, Any]] = None,
    execution_time_ms: Optional[float] = None,
    errors: Optional[List[str]] = None
) -> Dict[str, Any]:
    """Cr√©e des informations de debug structur√©es"""
    
    debug_info = {
        "function": function_name,
        "timestamp": datetime.now().isoformat(),
        "execution_time_ms": execution_time_ms,
        "success": not bool(errors),
        "errors": errors or [],
        "inputs": inputs or {},
        "outputs": outputs or {}
    }
    
    # Ajouter des statistiques si disponibles
    if inputs:
        debug_info["input_stats"] = {
            "input_count": len(inputs),
            "input_keys": list(inputs.keys())
        }
    
    if outputs:
        debug_info["output_stats"] = {
            "output_count": len(outputs),
            "output_keys": list(outputs.keys())
        }
    
    return debug_info

def log_performance_metrics(
    operation: str,
    start_time: float,
    end_time: Optional[float] = None,
    additional_metrics: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """Log des m√©triques de performance"""
    
    if end_time is None:
        end_time = time.time()
    
    duration_ms = int((end_time - start_time) * 1000)
    
    metrics = {
        "operation": operation,
        "duration_ms": duration_ms,
        "timestamp": datetime.now().isoformat(),
        "performance_category": _categorize_performance(duration_ms),
        **(additional_metrics or {})
    }
    
    logger.info(f"üìä [Performance] {operation}: {duration_ms}ms ({metrics['performance_category']})")
    
    return metrics

def _categorize_performance(duration_ms: int) -> str:
    """Cat√©gorise la performance selon la dur√©e"""
    if duration_ms < 100:
        return "excellent"
    elif duration_ms < 500:
        return "good"
    elif duration_ms < 1000:
        return "acceptable"
    elif duration_ms < 3000:
        return "slow"
    else:
        return "very_slow"

# =============================================================================
# UTILITAIRES SP√âCIAUX POUR INT√âGRATIONS (CONSERV√âS)
# =============================================================================

def create_fallback_response(
    original_question: str,
    error_message: str = "Service temporairement indisponible",
    language: str = "fr"
) -> Dict[str, Any]:
    """Cr√©e une r√©ponse de fallback standardis√©e"""
    
    if not isinstance(original_question, str):
        original_question = str(original_question) if original_question is not None else ""
    
    fallback_messages = {
        "fr": f"Je m'excuse, le service est temporairement indisponible. Votre question '{original_question}' a √©t√© re√ßue. Veuillez r√©essayer dans quelques minutes.",
        "en": f"I apologize, the service is temporarily unavailable. Your question '{original_question}' was received. Please try again in a few minutes.",
        "es": f"Me disculpo, el servicio no est√° disponible temporalmente. Su pregunta '{original_question}' fue recibida. Por favor intente de nuevo en unos minutos."
    }
    
    return {
        "response": fallback_messages.get(language, fallback_messages["fr"]),
        "is_fallback": True,
        "original_question": original_question,
        "error_message": error_message,
        "language": language,
        "timestamp": datetime.now().isoformat(),
        "suggested_action": "retry_later"
    }

def extract_key_entities_simple(text: str, language: str = "fr") -> Dict[str, List[Union[int, float]]]:
    """Extraction simple d'entit√©s cl√©s sans d√©pendances externes"""
    
    if not isinstance(text, str):
        text = str(text) if text is not None else ""
    
    entities = {
        "numbers": [],
        "breeds": [],
        "ages": [],
        "weights": [],
        "temperatures": [],
        "percentages": []
    }
    
    text_lower = text.lower()
    
    try:
        # Extraction nombres
        numbers = re.findall(r'\b\d+(?:[.,]\d+)?\b', text)
        entities["numbers"] = [float(n.replace(',', '.')) for n in numbers if n]
        
        # Extraction races communes
        breed_patterns = [
            r'\b(ross\s*308|cobb\s*500|hubbard|arbor\s*acres)\b',
            r'\b(ross|cobb)\s*\d{2,3}\b'
        ]
        
        for pattern in breed_patterns:
            matches = re.findall(pattern, text_lower, re.IGNORECASE)
            entities["breeds"].extend([str(match) for match in matches if match])
        
        # Extraction √¢ges
        age_patterns = [
            r'(\d+)\s*(?:jour|day|d√≠a)s?',
            r'(\d+)\s*(?:semaine|week|semana)s?'
        ]
        
        for pattern in age_patterns:
            matches = re.findall(pattern, text_lower, re.IGNORECASE)
            entities["ages"].extend([int(m) for m in matches if m.isdigit()])
        
        # Extraction poids
        weight_patterns = [
            r'(\d+(?:[.,]\d+)?)\s*(?:g|gr|gram|gramme)s?',
            r'(\d+(?:[.,]\d+)?)\s*(?:kg|kilo)s?'
        ]
        
        for pattern in weight_patterns:
            matches = re.findall(pattern, text_lower, re.IGNORECASE)
            entities["weights"].extend([float(m.replace(',', '.')) for m in matches if m])
        
        # Extraction temp√©ratures
        temp_patterns = [
            r'(\d+(?:[.,]\d+)?)\s*(?:¬∞c|celsius|degr√©)s?',
            r'(\d+(?:[.,]\d+)?)\s*(?:¬∞f|fahrenheit)s?'
        ]
        
        for pattern in temp_patterns:
            matches = re.findall(pattern, text_lower, re.IGNORECASE)
            entities["temperatures"].extend([float(m.replace(',', '.')) for m in matches if m])
        
        # Extraction pourcentages
        percentage_pattern = r'(\d+(?:[.,]\d+)?)\s*%'
        matches = re.findall(percentage_pattern, text_lower)
        entities["percentages"] = [float(m.replace(',', '.')) for m in matches if m]
        
        # Nettoyer les listes vides et d√©duplication
        entities = {k: list(set(v)) for k, v in entities.items() if v}
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è [Utils] Erreur extraction entit√©s: {e}")
    
    return entities

# =============================================================================
# CONFIGURATION ET LOGGING
# =============================================================================

logger.info("‚úÖ [Expert Utils] Fonctions utilitaires charg√©es avec succ√®s")
logger.info("üîß [Expert Utils] Fonctions disponibles:")
logger.info("   - get_user_id_from_request: Extraction ID utilisateur")
logger.info("   - extract_breed_and_sex_from_clarification: Extraction entit√©s clarification")
logger.info("   - validate_clarification_completeness: Validation compl√©tude clarification")
logger.info("   - build_enriched_question_*: Construction questions enrichies")
logger.info("   - get_enhanced_topics_by_language: Topics sugg√©r√©s multilingues")
logger.info("   - save_conversation_auto_enhanced: Sauvegarde conversation")
logger.info("   - score_question_variant: Scoring variantes de questions")
logger.info("   - validate_question_length: Validation longueur questions")
logger.info("   - validate_and_sanitize_input: Validation et nettoyage input")
logger.info("   - create_debug_info: Informations debug structur√©es") 
logger.info("   - log_performance_metrics: M√©triques de performance")
logger.info("   - create_fallback_response: R√©ponses de fallback")
logger.info("   - extract_key_entities_simple: Extraction entit√©s simple")
logger.info("üöÄ [Expert Utils] CORRIG√â: Auto-d√©tection sexe pondeuses activ√©e!")
logger.info("üöÄ [Expert Utils] INT√âGR√â: Centralisation via clarification_entities")
logger.info("üöÄ [Expert Utils] NOUVEAU: score_question_variant() - Scoring g√©n√©rique des variantes")
logger.info("üöÄ [Expert Utils] NOUVEAU: convert_legacy_entities() - Normalisation entit√©s anciennes")
logger.info("üöÄ [Expert Utils] NOUVEAU: validate_normalized_entities() - Validation format normalis√©")
logger.info("üöÄ [Expert Utils] NOUVEAU: merge_entities_intelligently() - Fusion intelligente entit√©s")
logger.info("‚úÖ [Expert Utils] CORRECTIONS APPLIQU√âES:")
logger.info("   - Type annotations am√©lior√©es")
logger.info("   - Gestion des exceptions renforc√©e")
logger.info("   - Validation des param√®tres None-safe")
logger.info("   - Gestion des erreurs regex")
logger.info("   - Validation des types d'entr√©e")
logger.info("   - Support normalisation entit√©s legacy")
logger.info("   - Validation format normalis√©")
logger.info("   - Fusion intelligente entit√©s multiples")
if CLARIFICATION_ENTITIES_AVAILABLE:
    logger.info("   ‚úÖ clarification_entities: normalize_breed_name, infer_sex_from_breed")
else:
    logger.info("   ‚ö†Ô∏è clarification_entities: Mode fallback actif")
logger.info("‚ú® [Expert Utils] Toutes les d√©pendances expert.py et expert_services.py satisfaites!")
logger.info("üéØ [Expert Utils] PHASE 1 NORMALISATION: Fonctions ajout√©es selon sp√©cifications am√©liorations!")