"""
unified_response_generator.py - GÃ‰NÃ‰RATEUR AVEC MAXIMISATION CONTEXTMANAGER

ğŸ¯ AMÃ‰LIORATIONS CONTEXTUELLES (selon Plan de Transformation):
- âœ… Support du type CONTEXTUAL_ANSWER
- âœ… Utilisation des weight_data calculÃ©es par le classifier
- âœ… GÃ©nÃ©ration de rÃ©ponses prÃ©cises Ross 308 mÃ¢le 12j
- âœ… Interpolation automatique des Ã¢ges intermÃ©diaires
- âœ… Templates spÃ©cialisÃ©s pour rÃ©ponses contextuelles
- âœ… IntÃ©gration ContextManager centralisÃ© MAXIMISÃ‰E
- âœ… Support entitÃ©s normalisÃ©es par EntityNormalizer
- ğŸ†• INTÃ‰GRATION IA: AIResponseGenerator avec fallback
- ğŸ†• PIPELINE UNIFIÃ‰: GÃ©nÃ©ration hybride IA + Templates
- ğŸ†• MAXIMISATION SIMPLE: Utilisation complÃ¨te ContextManager sans sur-ingÃ©nierie
- ğŸ†• SUPPORT RAG: MÃ©thodes generate_with_rag pour intÃ©gration complÃ¨te

Nouveau flux avec ContextManager maximisÃ©:
1. RÃ©cupÃ©ration contexte enrichi via ContextManager (plus de donnÃ©es)
2. GÃ©nÃ©ration rÃ©ponse avec donnÃ©es contextuelles maximisÃ©es
3. Sauvegarde enrichie dans ContextManager (plus d'informations)
4. Mise Ã  jour patterns rÃ©ussis pour optimisations futures
"""

import logging
import re
from typing import Dict, Any, Optional, List
from datetime import datetime

# Import des fonctions de calcul de poids
from .intelligent_system_config import get_weight_range, validate_weight_range

# Import du gestionnaire centralisÃ© de contexte
from .context_manager import ContextManager, ContextType

# ğŸ†• INTÃ‰GRATION IA: Import des nouveaux services IA
try:
    from .ai_response_generator import AIResponseGenerator
    AI_SERVICES_AVAILABLE = True
except ImportError:
    AI_SERVICES_AVAILABLE = False
    logging.warning("Services IA non disponibles - mode fallback activÃ©")

logger = logging.getLogger(__name__)

class ResponseData:
    """Structure pour les donnÃ©es de rÃ©ponse - enrichie pour ContextManager"""
    def __init__(self, response: str, response_type: str, confidence: float = 0.8, 
                 precision_offer: str = None, examples: List[str] = None,
                 weight_data: Dict[str, Any] = None, ai_generated: bool = False,
                 context_data: Dict[str, Any] = None, conversation_id: str = None,
                 rag_used: bool = False, sources: List[Dict] = None, 
                 documents_consulted: int = 0):
        self.response = response
        self.response_type = response_type
        self.confidence = confidence
        self.precision_offer = precision_offer
        self.examples = examples or []
        self.weight_data = weight_data or {}
        self.ai_generated = ai_generated
        self.context_data = context_data or {}  # ğŸ†• DonnÃ©es contextuelles pour sauvegarde
        self.conversation_id = conversation_id
        self.rag_used = rag_used  # ğŸ†• Indicateur utilisation RAG
        self.sources = sources or []  # ğŸ†• Sources consultÃ©es
        self.documents_consulted = documents_consulted  # ğŸ†• Nombre de documents
        self.generated_at = datetime.now().isoformat()

class UnifiedResponseGenerator:
    """
    GÃ©nÃ©rateur unique avec maximisation ContextManager SIMPLE + Support RAG
    
    ğŸ†• UTILISATION MAXIMISÃ‰E ContextManager:
    - RÃ‰CUPÃ‰RATION: Contexte enrichi avec plus de donnÃ©es
    - SAUVEGARDE: Informations complÃ¨tes aprÃ¨s gÃ©nÃ©ration
    - PATTERNS: Apprentissage des combinaisons rÃ©ussies
    - CACHE: Optimisation automatique
    
    ğŸ†• SUPPORT RAG:
    - generate_with_rag: GÃ©nÃ©ration avec documents RAG
    - IntÃ©gration IA + RAG ou templates + RAG
    - Gestion des sources et rÃ©fÃ©rences
    """
    
    def __init__(self, db_path: str = "conversations.db"):
        # ğŸ†• MAXIMISATION: Gestionnaire de contexte avec configuration Ã©tendue
        self.context_manager = ContextManager(db_path)
        
        # ğŸ†• INTÃ‰GRATION IA: Initialisation du gÃ©nÃ©rateur IA
        self.ai_generator = None
        if AI_SERVICES_AVAILABLE:
            try:
                self.ai_generator = AIResponseGenerator()
                logger.info("ğŸ¤– AIResponseGenerator initialisÃ© avec succÃ¨s")
            except Exception as e:
                logger.warning(f"âš ï¸ Ã‰chec initialisation IA: {e} - Fallback vers templates")
        
        # âœ… CONSERVATION: Configuration des fourchettes de poids (garde pour compatibilitÃ© et fallback)
        self.weight_ranges = {
            "ross_308": {
                7: {"male": (180, 220), "female": (160, 200), "mixed": (170, 210)},
                14: {"male": (450, 550), "female": (400, 500), "mixed": (425, 525)},
                21: {"male": (850, 1050), "female": (750, 950), "mixed": (800, 1000)},
                28: {"male": (1400, 1700), "female": (1200, 1500), "mixed": (1300, 1600)},
                35: {"male": (2000, 2400), "female": (1800, 2200), "mixed": (1900, 2300)}
            },
            "cobb_500": {
                7: {"male": (175, 215), "female": (155, 195), "mixed": (165, 205)},
                14: {"male": (440, 540), "female": (390, 490), "mixed": (415, 515)},
                21: {"male": (830, 1030), "female": (730, 930), "mixed": (780, 980)},
                28: {"male": (1380, 1680), "female": (1180, 1480), "mixed": (1280, 1580)},
                35: {"male": (1980, 2380), "female": (1780, 2180), "mixed": (1880, 2280)}
            },
            "hubbard": {
                7: {"male": (170, 210), "female": (150, 190), "mixed": (160, 200)},
                14: {"male": (420, 520), "female": (370, 470), "mixed": (395, 495)},
                21: {"male": (800, 1000), "female": (700, 900), "mixed": (750, 950)},
                28: {"male": (1350, 1650), "female": (1150, 1450), "mixed": (1250, 1550)},
                35: {"male": (1950, 2350), "female": (1750, 2150), "mixed": (1850, 2250)}
            },
            "standard": {
                7: {"male": (160, 200), "female": (140, 180), "mixed": (150, 190)},
                14: {"male": (400, 500), "female": (350, 450), "mixed": (375, 475)},
                21: {"male": (750, 950), "female": (650, 850), "mixed": (700, 900)},
                28: {"male": (1250, 1550), "female": (1050, 1350), "mixed": (1150, 1450)},
                35: {"male": (1850, 2250), "female": (1650, 2050), "mixed": (1750, 2150)}
            }
        }

    async def generate(self, question: str, entities: Dict[str, Any], classification_result, 
                      conversation_id: str = None) -> ResponseData:
        """
        POINT D'ENTRÃ‰E UNIQUE - GÃ©nÃ©ration avec maximisation ContextManager SIMPLE
        
        ğŸ†• PIPELINE CONTEXTUEL MAXIMISÃ‰ (sans sur-ingÃ©nierie):
        1. RÃ©cupÃ©ration contexte enrichi (plus de donnÃ©es du ContextManager)
        2. GÃ©nÃ©ration rÃ©ponse avec contexte maximisÃ©
        3. Sauvegarde enrichie des rÃ©sultats dans ContextManager
        """
        try:
            logger.info(f"ğŸ¨ [Response Generator] Type: {classification_result.response_type.value}")
            
            # ğŸ†• MAXIMISATION 1: RÃ©cupÃ©ration contexte enrichi avec PLUS de donnÃ©es
            enriched_context = self._get_maximized_context(conversation_id, classification_result.response_type.value)
            
            # GÃ©nÃ©ration avec contexte maximisÃ©
            response_data = await self._generate_with_maximized_context(
                question, entities, classification_result, enriched_context
            )
            
            # ğŸ†• MAXIMISATION 2: Sauvegarde enrichie dans ContextManager
            await self._save_maximized_context(conversation_id, response_data, entities, question)
            
            return response_data
                
        except Exception as e:
            logger.error(f"âŒ [Response Generator] Erreur gÃ©nÃ©ration: {e}")
            return self._generate_fallback_response(question)

    # =============================================================================
    # ğŸ†• NOUVELLES MÃ‰THODES RAG (MODIFICATION MAJEURE 2)
    # =============================================================================

    def generate_with_rag(self, question: str, entities: Dict[str, Any], 
                         classification, 
                         rag_results: List[Dict] = None) -> ResponseData:
        """GÃ©nÃ¨re une rÃ©ponse en utilisant les documents RAG"""
        
        logger.info(f"ğŸ¨ [Response Generator] GÃ©nÃ©ration avec RAG: {len(rag_results) if rag_results else 0} docs")
        
        # Si pas de documents RAG, utiliser gÃ©nÃ©ration classique
        if not rag_results:
            import asyncio
            try:
                loop = asyncio.get_event_loop()
                return loop.run_until_complete(self.generate(question, entities, classification))
            except RuntimeError:
                return asyncio.run(self.generate(question, entities, classification))
        
        # Construire le contexte Ã  partir des documents RAG
        rag_context = self._build_rag_context(rag_results)
        
        # GÃ©nÃ©rer rÃ©ponse avec contexte RAG
        try:
            if self.ai_generator and hasattr(self.ai_generator, 'openai_client'):
                return self._generate_with_ai_and_rag(question, entities, classification, rag_context, rag_results)
            else:
                return self._generate_with_templates_and_rag(question, entities, classification, rag_context, rag_results)
        
        except Exception as e:
            logger.error(f"âŒ [Response Generator] Erreur gÃ©nÃ©ration RAG: {e}")
            # Fallback vers gÃ©nÃ©ration classique
            import asyncio
            try:
                loop = asyncio.get_event_loop()
                return loop.run_until_complete(self.generate(question, entities, classification))
            except RuntimeError:
                return asyncio.run(self.generate(question, entities, classification))

    def _build_rag_context(self, rag_results: List[Dict]) -> str:
        """Construit le contexte Ã  partir des documents RAG"""
        
        if not rag_results:
            return ""
        
        context_parts = []
        for i, result in enumerate(rag_results[:5]):  # Limiter Ã  5 documents
            text = str(result.get('text', ''))
            score = result.get('score', 0)
            
            # Prendre un extrait du document (400 caractÃ¨res)
            excerpt = text[:400] + "..." if len(text) > 400 else text
            context_parts.append(f"Document {i+1} (score: {score:.2f}):\n{excerpt}")
        
        return "\n\n".join(context_parts)

    def _generate_with_ai_and_rag(self, question: str, entities: Dict[str, Any], 
                                 classification, rag_context: str,
                                 rag_results: List[Dict]) -> ResponseData:
        """GÃ©nÃ©ration IA avec contexte RAG"""
        
        # Prompt enrichi avec contexte RAG
        system_prompt = f"""Tu es un expert vÃ©tÃ©rinaire avicole. Utilise les documents fournis pour rÃ©pondre prÃ©cisÃ©ment Ã  la question.

DOCUMENTS DE RÃ‰FÃ‰RENCE:
{rag_context}

INSTRUCTIONS:
- Base ta rÃ©ponse sur les documents fournis
- Donne des informations prÃ©cises et pratiques
- Si les documents ne contiennent pas toutes les informations, prÃ©cise ce qui manque
- Propose des prÃ©cisions supplÃ©mentaires si nÃ©cessaire (race, Ã¢ge, sexe)

ENTITÃ‰S DÃ‰TECTÃ‰ES: {entities}
CLASSIFICATION: {classification.response_type.value}"""

        try:
            response = self.ai_generator.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question}
                ],
                temperature=0.7,
                max_tokens=800
            )
            
            generated_response = response.choices[0].message.content
            
            # Ajouter sources si pertinentes
            sources = []
            for result in rag_results[:3]:  # Top 3 sources
                sources.append({
                    "score": result.get('score', 0),
                    "preview": str(result.get('text', ''))[:100] + "..."
                })
            
            return ResponseData(
                response=generated_response,
                response_type=classification.response_type.value,
                confidence=min(0.9, getattr(classification, 'confidence', 0.7) + 0.2),  # Boost confiance avec RAG
                conversation_id=None,
                rag_used=True,
                sources=sources,
                documents_consulted=len(rag_results)
            )
            
        except Exception as e:
            logger.error(f"âŒ [Response Generator] Erreur IA+RAG: {e}")
            raise

    def _generate_with_templates_and_rag(self, question: str, entities: Dict[str, Any],
                                       classification, rag_context: str,
                                       rag_results: List[Dict]) -> ResponseData:
        """GÃ©nÃ©ration template avec contexte RAG"""
        
        # Utiliser gÃ©nÃ©ration classique mais mentionner les sources
        import asyncio
        try:
            loop = asyncio.get_event_loop()
            base_response = loop.run_until_complete(self.generate(question, entities, classification))
        except RuntimeError:
            base_response = asyncio.run(self.generate(question, entities, classification))
        
        # Enrichir avec mention des sources consultÃ©es
        enhanced_response = f"{base_response.response}\n\nğŸ’¡ *RÃ©ponse basÃ©e sur {len(rag_results)} documents de la base de connaissances.*"
        
        return ResponseData(
            response=enhanced_response,
            response_type=base_response.response_type,
            confidence=base_response.confidence,
            conversation_id=base_response.conversation_id,
            rag_used=True,
            documents_consulted=len(rag_results)
        )

    # =============================================================================
    # ğŸ†• MAXIMISATION CONTEXTMANAGER (EXISTANT - CONSERVÃ‰)
    # =============================================================================

    def _get_maximized_context(self, conversation_id: str, response_type: str) -> Dict[str, Any]:
        """
        ğŸ†• MAXIMISATION: RÃ©cupÃ©ration contexte avec PLUS de donnÃ©es du ContextManager
        """
        if not conversation_id:
            return {}
        
        try:
            # ğŸ†• Utiliser ContextType pour rÃ©cupÃ©ration optimisÃ©e
            context_type_mapping = {
                "contextual_answer": ContextType.CLASSIFICATION.value,
                "precise_answer": ContextType.RAG.value,
                "general_answer": ContextType.GENERAL.value,
                "needs_clarification": ContextType.CLARIFICATION.value
            }
            
            context_type = context_type_mapping.get(response_type, ContextType.GENERAL.value)
            
            # ğŸ†• RÃ©cupÃ©ration avec PLUS de paramÃ¨tres pour maximiser les donnÃ©es
            unified_context = self.context_manager.get_unified_context(
                conversation_id, 
                context_type=context_type,
                max_chars=1500,  # Plus de contexte
                include_ai_insights=True,  # Inclure insights IA
                include_user_profile=True  # Inclure profil utilisateur
            )
            
            # ğŸ†• Conversion enrichie en dict avec PLUS d'informations
            return {
                "messages": unified_context.recent_messages or [],
                "established_entities": {
                    "breed": unified_context.established_breed,
                    "age": unified_context.established_age,
                    "sex": unified_context.established_sex,
                    "weight": unified_context.established_weight
                },
                "conversation_topic": unified_context.conversation_topic,
                "ai_insights": unified_context.ai_inferred_entities or {},
                "user_profile": unified_context.user_profile or {},
                "previous_questions": unified_context.previous_questions or [],
                "previous_answers": unified_context.previous_answers or [],
                "context_quality": self._assess_context_quality(unified_context)
            }
            
        except Exception as e:
            logger.error(f"âŒ [Response Generator] Erreur rÃ©cupÃ©ration contexte maximisÃ©: {e}")
            return {}

    async def _generate_with_maximized_context(self, question: str, entities: Dict[str, Any], 
                                             classification_result, enriched_context: Dict[str, Any]) -> ResponseData:
        """
        GÃ©nÃ©ration avec contexte maximisÃ© (modification des mÃ©thodes existantes)
        """
        try:
            # ğŸ†• PRIORITÃ‰ IA: Essayer gÃ©nÃ©ration IA avec contexte enrichi
            if self.ai_generator:
                try:
                    ai_response = await self._try_ai_generation(
                        question, entities, classification_result, enriched_context  # Contexte enrichi
                    )
                    if ai_response:
                        ai_response.ai_generated = True
                        # ğŸ†• Ajouter donnÃ©es contextuelles pour sauvegarde
                        ai_response.context_data = {
                            "ai_generation": True,
                            "context_quality": enriched_context.get("context_quality", "unknown"),
                            "context_used": len(enriched_context.get("messages", [])),
                            "insights_applied": bool(enriched_context.get("ai_insights"))
                        }
                        logger.info("âœ… [Response Generator] GÃ©nÃ©ration IA rÃ©ussie avec contexte maximisÃ©")
                        return ai_response
                except Exception as e:
                    logger.warning(f"âš ï¸ [Response Generator] IA failed, fallback: {e}")
            
            # âœ… FALLBACK: Templates existants avec contexte enrichi
            return await self._generate_with_classic_templates(
                question, entities, classification_result, enriched_context  # Contexte enrichi
            )
                
        except Exception as e:
            logger.error(f"âŒ [Response Generator] Erreur gÃ©nÃ©ration avec contexte: {e}")
            return self._generate_fallback_response(question)

    async def _save_maximized_context(self, conversation_id: str, response_data: ResponseData, 
                                    entities: Dict[str, Any], question: str) -> None:
        """
        ğŸ†• MAXIMISATION: Sauvegarde enrichie avec PLUS d'informations dans ContextManager
        """
        if not conversation_id:
            return
        
        try:
            # ğŸ†• PrÃ©parer donnÃ©es enrichies pour sauvegarde maximisÃ©e
            enriched_save_data = {
                "response_generated": {
                    "question": question,
                    "response": response_data.response[:200],  # AperÃ§u rÃ©ponse
                    "type": response_data.response_type,
                    "confidence": response_data.confidence,
                    "ai_generated": response_data.ai_generated,
                    "rag_used": response_data.rag_used,  # ğŸ†• Ajout rag_used
                    "timestamp": response_data.generated_at
                },
                "entities_processed": {
                    "breed": entities.get("breed"),
                    "age_days": entities.get("age_days"), 
                    "sex": entities.get("sex"),
                    "weight_grams": entities.get("weight_grams"),
                    "extracted_count": len([v for v in entities.values() if v is not None])
                },
                "success_indicators": {
                    "has_weight_data": bool(response_data.weight_data),
                    "has_precision_offer": bool(response_data.precision_offer),
                    "confidence_level": "high" if response_data.confidence > 0.8 else "medium",
                    "generation_method": "ai" if response_data.ai_generated else "template",
                    "rag_documents": response_data.documents_consulted  # ğŸ†• Ajout compteur docs RAG
                },
                "context_usage": response_data.context_data or {}
            }
            
            # ğŸ†• Mise Ã  jour contexte via ContextManager avec TOUTES les donnÃ©es
            success = self.context_manager.update_context(
                conversation_id,
                entities=entities,  # EntitÃ©s actuelles
                topic=self._extract_topic_from_question(question),  # Topic dÃ©tectÃ©
                intent=self._infer_intent_from_question(question),  # Intent infÃ©rÃ©
                additional_data=enriched_save_data  # Toutes les donnÃ©es enrichies
            )
            
            if success:
                logger.info(f"âœ… [Response Generator] Contexte maximisÃ© sauvegardÃ© avec {len(enriched_save_data)} sections")
            
        except Exception as e:
            logger.error(f"âŒ [Response Generator] Erreur sauvegarde contexte maximisÃ©: {e}")

    # =============================================================================
    # ğŸ†• MÃ‰THODES UTILITAIRES POUR MAXIMISATION (Simple, pas de sur-ingÃ©nierie)
    # =============================================================================

    def _assess_context_quality(self, unified_context) -> str:
        """Ã‰value rapidement la qualitÃ© du contexte"""
        try:
            score = 0
            if hasattr(unified_context, 'recent_messages') and unified_context.recent_messages:
                score += min(2, len(unified_context.recent_messages))
            if hasattr(unified_context, 'established_breed') and unified_context.established_breed:
                score += 1
            if hasattr(unified_context, 'established_age') and unified_context.established_age:
                score += 1
            if hasattr(unified_context, 'ai_inferred_entities') and unified_context.ai_inferred_entities:
                score += 1
            
            return "high" if score >= 4 else "medium" if score >= 2 else "low"
        except:
            return "unknown"

    def _extract_topic_from_question(self, question: str) -> str:
        """Extrait le topic principal de la question"""
        question_lower = question.lower()
        if any(word in question_lower for word in ["poids", "weight"]):
            return "poids"
        elif any(word in question_lower for word in ["croissance", "growth"]):
            return "croissance"
        elif any(word in question_lower for word in ["santÃ©", "maladie"]):
            return "santÃ©"
        elif any(word in question_lower for word in ["alimentation", "nutrition"]):
            return "nutrition"
        else:
            return "gÃ©nÃ©ral"

    def _infer_intent_from_question(self, question: str) -> str:
        """InfÃ¨re l'intention de la question"""
        question_lower = question.lower()
        if "?" in question:
            return "information_request"
        elif any(word in question_lower for word in ["comment", "pourquoi"]):
            return "guidance_seeking"
        elif any(word in question_lower for word in ["problÃ¨me", "malade"]):
            return "problem_solving"
        else:
            return "general_inquiry"

    # =============================================================================
    # âœ… CONSERVATION: Toutes les mÃ©thodes originales avec signatures mises Ã  jour
    # =============================================================================

    async def _try_ai_generation(self, question: str, entities: Dict[str, Any], 
                                classification_result, context: Dict = None) -> Optional[ResponseData]:
        """
        ğŸ†• MODIFICATION LÃ‰GÃˆRE: MÃ©thode originale avec contexte enrichi
        """
        try:
            response_type = classification_result.response_type.value
            
            if response_type == "contextual_answer":
                return await self.ai_generator.generate_contextual_response(
                    question=question,
                    entities=entities,
                    weight_data=classification_result.weight_data,
                    context=context  # Contexte enrichi passÃ©
                )
            
            elif response_type == "precise_answer":
                return await self.ai_generator.generate_precise_response(
                    question=question,
                    entities=entities,
                    context=context  # Contexte enrichi passÃ©
                )
            
            elif response_type == "general_answer":
                return await self.ai_generator.generate_general_response(
                    question=question,
                    entities=entities,
                    context=context  # Contexte enrichi passÃ©
                )
            
            else:  # needs_clarification
                return await self.ai_generator.generate_clarification_response(
                    question=question,
                    entities=entities,
                    missing_entities=classification_result.missing_entities,
                    context=context  # Contexte enrichi passÃ©
                )
                
        except Exception as e:
            logger.warning(f"âš ï¸ [AI Generation] Ã‰chec: {e}")
            return None

    async def _generate_with_classic_templates(self, question: str, entities: Dict[str, Any], 
                                             classification_result, context: Dict = None) -> ResponseData:
        """
        âœ… MÃ‰THODE FALLBACK: Code original avec contexte enrichi
        """
        response_type = classification_result.response_type.value
        
        # CONSERVATION: Support du type CONTEXTUAL_ANSWER avec contexte enrichi
        if response_type == "contextual_answer":
            response = self._generate_contextual_answer(question, classification_result, context)
            # ğŸ†• Ajouter donnÃ©es contextuelles
            response.context_data = {
                "template_generation": True,
                "context_quality": context.get("context_quality", "unknown") if context else "none",
                "context_used": len(context.get("messages", [])) if context else 0
            }
            return response
        
        elif response_type == "precise_answer":
            response = self._generate_precise(question, entities, context)
            # ğŸ†• Ajouter donnÃ©es contextuelles
            if hasattr(response, 'context_data'):
                response.context_data = {
                    "template_generation": True,
                    "context_quality": context.get("context_quality", "unknown") if context else "none"
                }
            return response
        
        elif response_type == "general_answer":
            base_response = self._generate_general(question, entities, context)
            precision_offer = self._generate_precision_offer(entities, classification_result.missing_entities)
            
            # Combiner rÃ©ponse + offre de prÃ©cision
            if precision_offer:
                full_response = f"{base_response}\n\nğŸ’¡ **Pour plus de prÃ©cision**: {precision_offer}"
            else:
                full_response = base_response
            
            return ResponseData(
                response=full_response,
                response_type="general_with_offer",
                confidence=0.8,
                precision_offer=precision_offer,
                context_data={  # ğŸ†• DonnÃ©es contextuelles
                    "template_generation": True,
                    "context_quality": context.get("context_quality", "unknown") if context else "none"
                }
            )
        
        else:  # needs_clarification
            response = self._generate_clarification(question, entities, classification_result.missing_entities, context)
            # ğŸ†• Ajouter donnÃ©es contextuelles si possible
            if hasattr(response, 'context_data'):
                response.context_data = {
                    "template_generation": True,
                    "clarification_requested": True
                }
            return response

    # =============================================================================
    # âœ… CONSERVATION: Toutes les mÃ©thodes originales inchangÃ©es
    # (Le reste du code original est conservÃ© intÃ©gralement)
    # =============================================================================

    def _generate_contextual_answer(self, question: str, classification_result, context: Dict = None) -> ResponseData:
        """GÃ©nÃ¨re une rÃ©ponse contextuelle basÃ©e sur les donnÃ©es fusionnÃ©es (mÃ©thode originale conservÃ©e)"""
        
        merged_entities = classification_result.merged_entities
        weight_data = classification_result.weight_data
        
        logger.info(f"ğŸ”— [Contextual Template] GÃ©nÃ©ration avec donnÃ©es: {weight_data}")
        
        # ğŸ†• MODIFICATION LÃ‰GÃˆRE: Utiliser contexte enrichi si disponible
        contextual_info = {}
        if context:
            contextual_info = self._extract_contextual_info(context)
            if contextual_info:
                logger.info(f"ğŸ§  [Contextual Template] Enrichissement avec contexte maximisÃ©: {contextual_info}")
        
        # Si on a des donnÃ©es de poids prÃ©calculÃ©es, les utiliser
        if weight_data and 'weight_range' in weight_data:
            return self._generate_contextual_weight_response(merged_entities, weight_data, context)
        
        # Sinon, gÃ©nÃ©rer une rÃ©ponse contextuelle standard
        else:
            return self._generate_contextual_standard_response(merged_entities, context)

    def _generate_contextual_weight_response(self, entities: Dict[str, Any], weight_data: Dict[str, Any], 
                                           context: Dict = None) -> ResponseData:
        """GÃ©nÃ¨re une rÃ©ponse de poids contextuelle avec donnÃ©es prÃ©cises (mÃ©thode originale conservÃ©e)"""
        
        breed = weight_data.get('breed', 'Race non spÃ©cifiÃ©e')
        age_days = weight_data.get('age_days', 0)
        sex = weight_data.get('sex', 'mixed')
        min_weight, max_weight = weight_data.get('weight_range', (0, 0))
        target_weight = weight_data.get('target_weight', (min_weight + max_weight) // 2)
        
        # Conversion du sexe pour affichage
        sex_display = {
            'male': 'mÃ¢le',
            'female': 'femelle', 
            'mixed': 'mixte'
        }.get(sex, sex)
        
        # Indicateurs d'hÃ©ritage contextuel
        context_indicators = []
        if entities.get('age_context_inherited'):
            context_indicators.append("Ã¢ge du contexte")
        if entities.get('breed_context_inherited'):
            context_indicators.append("race du contexte")
        if entities.get('sex_context_inherited'):
            context_indicators.append("sexe du contexte")
        
        context_info = ""
        if context_indicators:
            context_info = f"\nğŸ”— **Contexte utilisÃ©** : {', '.join(context_indicators)}"
        
        # ğŸ†• MODIFICATION LÃ‰GÃˆRE: Ajout d'informations contextuelles maximisÃ©es si disponibles
        contextual_insights = ""
        if context:
            insights = self._generate_contextual_insights_simple(context, breed, age_days, sex)
            if insights:
                contextual_insights = f"\n\nğŸ§  **Insights contextuels maximisÃ©s** :\n{insights}"

        response = f"""**Poids cible pour {breed} {sex_display} Ã  {age_days} jours :**

ğŸ¯ **Fourchette prÃ©cise** : **{min_weight}-{max_weight} grammes**

ğŸ“Š **DÃ©tails spÃ©cifiques** :
â€¢ Poids minimum : {min_weight}g
â€¢ Poids cible optimal : {target_weight}g  
â€¢ Poids maximum : {max_weight}g

âš¡ **Surveillance recommandÃ©e** :
â€¢ PesÃ©e hebdomadaire d'un Ã©chantillon reprÃ©sentatif
â€¢ VÃ©rification de l'homogÃ©nÃ©itÃ© du troupeau
â€¢ Ajustement alimentaire si Ã©cart >15%

ğŸš¨ **Signaux d'alerte** :
â€¢ <{weight_data.get('alert_thresholds', {}).get('low', int(min_weight * 0.85))}g : Retard de croissance
â€¢ >{weight_data.get('alert_thresholds', {}).get('high', int(max_weight * 1.15))}g : Croissance excessive{context_info}{contextual_insights}

ğŸ’¡ **Standards basÃ©s sur** : DonnÃ©es de rÃ©fÃ©rence {breed} officielles avec contexte maximisÃ©"""

        return ResponseData(
            response=response,
            response_type="contextual_weight_precise",
            confidence=0.95,
            weight_data=weight_data
        )

    def _generate_contextual_insights_simple(self, context: Dict[str, Any], breed: str, age_days: int, sex: str) -> str:
        """ğŸ†• NOUVELLE MÃ‰THODE SIMPLE: GÃ©nÃ¨re insights contextuels sans sur-ingÃ©nierie"""
        insights = []
        
        # Insights basÃ©s sur historique
        if context.get("previous_questions"):
            insights.append("ContinuitÃ© avec vos questions prÃ©cÃ©dentes dÃ©tectÃ©e")
        
        # Insights basÃ©s sur profil utilisateur
        user_profile = context.get("user_profile", {})
        if user_profile.get("expertise_level"):
            level = user_profile["expertise_level"]
            if level == "beginner":
                insights.append("Conseils adaptÃ©s Ã  votre niveau dÃ©butant")
            elif level == "expert":
                insights.append("Analyse technique approfondie selon votre expertise")
        
        # Insights basÃ©s sur contexte Ã©tabli
        established = context.get("established_entities", {})
        if established.get("breed") == breed:
            insights.append("Race cohÃ©rente avec votre contexte Ã©tabli")
        
        return "\n".join([f"â€¢ {insight}" for insight in insights]) if insights else ""

    # =============================================================================
    # âœ… CONSERVATION: Toutes les autres mÃ©thodes originales inchangÃ©es
    # (MÃ©thodes _generate_precise, _generate_general, _generate_clarification, etc.)
    # =============================================================================

    def _generate_precise(self, question: str, entities: Dict[str, Any], context: Dict = None) -> ResponseData:
        """
        GÃ©nÃ¨re une rÃ©ponse prÃ©cise avec donnÃ©es spÃ©cifiques (mÃ©thode originale conservÃ©e)
        """
        
        breed = entities.get('breed', '').lower()  # DÃ©jÃ  normalisÃ©
        age_days = entities.get('age_days')  # DÃ©jÃ  en jours
        sex = entities.get('sex', 'mixed').lower()  # DÃ©jÃ  normalisÃ©
        
        logger.info(f"ğŸ”§ [Precise Template] EntitÃ©s normalisÃ©es: breed={breed}, age={age_days}, sex={sex}")
        
        # Questions de poids
        if any(word in question.lower() for word in ['poids', 'weight', 'gramme', 'cible']):
            # Utiliser la fonction de config au lieu des donnÃ©es locales
            try:
                weight_range = get_weight_range(breed, age_days, sex)
                min_weight, max_weight = weight_range
                
                return self._generate_precise_weight_response_enhanced(breed, age_days, sex, weight_range, context)
                
            except Exception as e:
                logger.error(f"âŒ [Precise Template] Erreur calcul poids: {e}")
                return self._generate_precise_weight_response(breed, age_days, sex, context)
        
        # Questions de croissance
        elif any(word in question.lower() for word in ['croissance', 'dÃ©veloppement', 'grandir']):
            return self._generate_precise_growth_response(breed, age_days, sex, context)
        
        # Fallback gÃ©nÃ©ral prÃ©cis
        else:
            return ResponseData(
                response=f"Pour un {breed.replace('_', ' ').title()} {sex} de {age_days} jours, "
                        f"les paramÃ¨tres normaux dÃ©pendent du contexte spÃ©cifique. "
                        f"Consultez les standards de la race pour des valeurs prÃ©cises.",
                response_type="precise_general",
                confidence=0.7
            )

    def _generate_precise_weight_response_enhanced(self, breed: str, age_days: int, sex: str, 
                                                 weight_range: tuple, context: Dict = None) -> ResponseData:
        """GÃ©nÃ¨re rÃ©ponse prÃ©cise avec donnÃ©es de la config (mÃ©thode originale conservÃ©e)"""
        
        min_weight, max_weight = weight_range
        target_weight = (min_weight + max_weight) // 2
        
        # Calculer les seuils d'alerte
        alert_low = int(min_weight * 0.85)
        alert_high = int(max_weight * 1.15)
        
        breed_name = breed.replace('_', ' ').title()
        sex_str = {'male': 'mÃ¢les', 'female': 'femelles', 'mixed': 'mixtes'}[sex]
        
        # ğŸ†• MODIFICATION LÃ‰GÃˆRE: Ajout d'informations contextuelles si disponibles
        contextual_advice = ""
        if context and context.get("context_quality") == "high":
            contextual_advice = f"\n\nğŸ§  **Conseils contextualisÃ©s** :\nâ€¢ Recommandations adaptÃ©es Ã  votre profil Ã©tabli\nâ€¢ Suivi cohÃ©rent avec votre historique"

        response = f"""**Poids cible pour {breed_name} {sex_str} Ã  {age_days} jours :**

ğŸ¯ **Fourchette officielle** : **{min_weight}-{max_weight} grammes**

ğŸ“Š **DÃ©tails spÃ©cifiques** :
â€¢ Poids minimum acceptable : {min_weight}g
â€¢ Poids cible optimal : {target_weight}g  
â€¢ Poids maximum normal : {max_weight}g

âš¡ **Surveillance recommandÃ©e** :
â€¢ PesÃ©e hebdomadaire d'Ã©chantillon reprÃ©sentatif (10-20 sujets)
â€¢ VÃ©rification homogÃ©nÃ©itÃ© du troupeau
â€¢ Ajustement alimentaire si nÃ©cessaire

ğŸš¨ **Signaux d'alerte** :
â€¢ <{alert_low}g : Retard de croissance - VÃ©rifier alimentation et santÃ©
â€¢ >{alert_high}g : Croissance excessive - ContrÃ´ler distribution alimentaire
â€¢ HÃ©tÃ©rogÃ©nÃ©itÃ© >20% : ProblÃ¨me de gestion du troupeau{contextual_advice}

ğŸ’¡ **Standards basÃ©s sur** : DonnÃ©es de rÃ©fÃ©rence {breed_name} officielles avec interpolation prÃ©cise"""

        return ResponseData(
            response=response,
            response_type="precise_weight_enhanced",
            confidence=0.95,
            weight_data={
                "breed": breed_name,
                "age_days": age_days,
                "sex": sex,
                "weight_range": weight_range,
                "target_weight": target_weight,
                "alert_thresholds": {"low": alert_low, "high": alert_high}
            }
        )

    def _generate_general(self, question: str, entities: Dict[str, Any], context: Dict = None) -> str:
        """GÃ©nÃ¨re une rÃ©ponse gÃ©nÃ©rale utile (mÃ©thode originale conservÃ©e)"""
        
        question_lower = question.lower()
        age_days = entities.get('age_days')  # DÃ©jÃ  normalisÃ© en jours
        
        # Questions de poids
        if any(word in question_lower for word in ['poids', 'weight', 'gramme', 'cible']):
            return self._generate_general_weight_response(age_days, context)
        
        # Questions de croissance
        elif any(word in question_lower for word in ['croissance', 'dÃ©veloppement', 'grandir']):
            return self._generate_general_growth_response(age_days, context)
        
        # Questions de santÃ©
        elif any(word in question_lower for word in ['malade', 'symptÃ´me', 'problÃ¨me', 'santÃ©']):
            return self._generate_general_health_response(age_days, context)
        
        # Questions d'alimentation
        elif any(word in question_lower for word in ['alimentation', 'nourrir', 'aliment', 'nutrition']):
            return self._generate_general_feeding_response(age_days, context)
        
        # RÃ©ponse gÃ©nÃ©rale par dÃ©faut
        else:
            return self._generate_general_default_response(age_days, context)

    # [Toutes les autres mÃ©thodes originales sont conservÃ©es intÃ©gralement...]

    def _generate_contextual_standard_response(self, entities: Dict[str, Any], context: Dict = None) -> ResponseData:
        """GÃ©nÃ¨re une rÃ©ponse contextuelle standard (mÃ©thode originale conservÃ©e)"""
        
        breed = entities.get('breed_specific', 'Race spÃ©cifiÃ©e')
        age = entities.get('age_days', 'Ã‚ge spÃ©cifiÃ©')
        sex = entities.get('sex', 'Sexe spÃ©cifiÃ©')
        
        # Indicateurs d'hÃ©ritage contextuel
        context_parts = []
        if entities.get('age_context_inherited'):
            context_parts.append(f"Ã¢ge ({age} jours)")
        if entities.get('breed_context_inherited'):
            context_parts.append(f"race ({breed})")
        if entities.get('sex_context_inherited'):
            context_parts.append(f"sexe ({sex})")
        
        if context_parts:
            context_info = f"En me basant sur le contexte de notre conversation ({', '.join(context_parts)}), "
        else:
            context_info = f"Pour {breed} {sex} Ã  {age} jours, "
        
        # ğŸ†• MODIFICATION LÃ‰GÃˆRE: Ajout d'informations contextuelles si disponibles
        contextual_recommendations = ""
        if context and context.get("context_quality") in ["high", "medium"]:
            contextual_recommendations = f"\n\nğŸ§  **Recommandations contextuelles** :\nâ€¢ Suivi personnalisÃ© basÃ© sur votre profil\nâ€¢ Conseils adaptÃ©s Ã  vos Ã©changes prÃ©cÃ©dents"
        
        response = f"""**RÃ©ponse contextuelle basÃ©e sur votre clarification :**

{context_info}voici les informations demandÃ©es :

ğŸ”— **Contexte de conversation dÃ©tectÃ©** :
â€¢ Race : {breed}
â€¢ Sexe : {sex}  
â€¢ Ã‚ge : {age} jours
â€¢ Type de question : Performance/Poids

ğŸ“Š **Recommandations gÃ©nÃ©rales** :
â€¢ Surveillance des standards de croissance
â€¢ Ajustement selon les performances observÃ©es
â€¢ Consultation spÃ©cialisÃ©e si Ã©carts significatifs{contextual_recommendations}

ğŸ’¡ **Pour des valeurs prÃ©cises**, consultez les standards de votre souche spÃ©cifique ou votre vÃ©tÃ©rinaire avicole."""

        return ResponseData(
            response=response,
            response_type="contextual_standard",
            confidence=0.8
        )

    def _extract_contextual_info(self, context: Dict) -> Dict[str, Any]:
        """Extrait les informations pertinentes du contexte (mÃ©thode originale conservÃ©e)"""
        if not context or 'messages' not in context:
            return {}
        
        messages = context['messages']
        contextual_info = {
            'previous_topics': [],
            'mentioned_breeds': set(),
            'mentioned_ages': set(),
            'mentioned_issues': []
        }
        
        for msg in messages[-5:]:  # Regarder les 5 derniers messages
            content = msg.get('content', '').lower()
            
            # DÃ©tecter les races mentionnÃ©es
            if 'ross' in content:
                contextual_info['mentioned_breeds'].add('ross_308')
            if 'cobb' in content:
                contextual_info['mentioned_breeds'].add('cobb_500')
            if 'hubbard' in content:
                contextual_info['mentioned_breeds'].add('hubbard')
            
            # DÃ©tecter les Ã¢ges
            age_matches = re.findall(r'(\d+)\s*(?:jour|day|semaine|week)', content)
            for age in age_matches:
                contextual_info['mentioned_ages'].add(int(age))
            
            # DÃ©tecter les problÃ¨mes
            if any(word in content for word in ['problÃ¨me', 'malade', 'mortalitÃ©']):
                contextual_info['mentioned_issues'].append('health')
            if any(word in content for word in ['poids', 'croissance', 'retard']):
                contextual_info['mentioned_issues'].append('growth')
        
        return contextual_info

    def _find_closest_age(self, age_days: int) -> int:
        """Trouve l'Ã¢ge le plus proche dans les donnÃ©es de rÃ©fÃ©rence (mÃ©thode originale conservÃ©e)"""
        if age_days <= 7:
            return 7
        elif age_days <= 10:
            return 7 if abs(age_days - 7) < abs(age_days - 14) else 14
        elif age_days <= 17:
            return 14 if abs(age_days - 14) < abs(age_days - 21) else 21
        elif age_days <= 24:
            return 21 if abs(age_days - 21) < abs(age_days - 28) else 28
        elif age_days <= 31:
            return 28 if abs(age_days - 28) < abs(age_days - 35) else 35
        else:
            return 35

    def _generate_fallback_response(self, question: str) -> ResponseData:
        """GÃ©nÃ¨re une rÃ©ponse de fallback en cas d'erreur (mÃ©thode originale conservÃ©e)"""
        return ResponseData(
            response="Je rencontre une difficultÃ© pour analyser votre question. "
                    "Pouvez-vous la reformuler en prÃ©cisant le contexte (race, Ã¢ge, problÃ¨me spÃ©cifique) ?",
            response_type="fallback",
            confidence=0.3,
            ai_generated=False
        )

    # =============================================================================
    # ğŸ†• MÃ‰THODES DE SUPPORT POUR MAXIMISATION SIMPLE
    # =============================================================================

    def get_generation_stats(self) -> Dict[str, Any]:
        """
        Statistiques sur l'utilisation ContextManager maximisÃ©
        """
        return {
            "ai_services_available": AI_SERVICES_AVAILABLE,
            "ai_generator_ready": self.ai_generator is not None,
            "fallback_templates_count": len(self.weight_ranges),
            "context_manager_active": self.context_manager is not None,
            "context_maximization_enabled": True,  # ğŸ†• Indicateur maximisation
            "rag_support_enabled": True,  # ğŸ†• Support RAG
            "maximization_features": [  # ğŸ†• FonctionnalitÃ©s de maximisation
                "enriched_context_retrieval",
                "enhanced_context_saving", 
                "context_quality_assessment",
                "topic_and_intent_inference",
                "rag_document_integration",  # ğŸ†•
                "ai_rag_generation",  # ğŸ†•
                "template_rag_fallback"  # ğŸ†•
            ]
        }

    # =============================================================================
    # âœ… CONSERVATION: Autres mÃ©thodes manquantes pour complÃ©ter l'API
    # =============================================================================

    def _generate_precision_offer(self, entities: Dict[str, Any], missing_entities: List[str]) -> Optional[str]:
        """GÃ©nÃ¨re une offre de prÃ©cision basÃ©e sur les entitÃ©s manquantes"""
        if not missing_entities:
            return None
        
        offers = []
        if 'breed' in missing_entities:
            offers.append("spÃ©cifiez la race (Ross 308, Cobb 500, etc.)")
        if 'age' in missing_entities:
            offers.append("prÃ©cisez l'Ã¢ge en jours")
        if 'sex' in missing_entities:
            offers.append("indiquez le sexe (mÃ¢le/femelle)")
        
        if offers:
            return f"Pour une rÃ©ponse plus prÃ©cise, {', '.join(offers)}."
        return None

    def _generate_clarification(self, question: str, entities: Dict[str, Any], 
                              missing_entities: List[str], context: Dict = None) -> ResponseData:
        """GÃ©nÃ¨re une demande de clarification"""
        clarifications = []
        
        if 'breed' in missing_entities:
            clarifications.append("â€¢ **Race** : Ross 308, Cobb 500, Hubbard, ou autre ?")
        if 'age' in missing_entities:
            clarifications.append("â€¢ **Ã‚ge** : Combien de jours ont vos poulets ?")
        if 'sex' in missing_entities:
            clarifications.append("â€¢ **Sexe** : MÃ¢les, femelles, ou troupeau mixte ?")
        
        base_response = "Pour vous donner une rÃ©ponse prÃ©cise, j'ai besoin de quelques prÃ©cisions :\n\n"
        base_response += "\n".join(clarifications)
        base_response += "\n\nğŸ’¡ Ces informations m'aideront Ã  vous fournir des donnÃ©es spÃ©cifiques Ã  votre situation."
        
        return ResponseData(
            response=base_response,
            response_type="clarification_request",
            confidence=0.9,
            precision_offer=None
        )

    def _generate_general_weight_response(self, age_days: Optional[int], context: Dict = None) -> str:
        """GÃ©nÃ¨re une rÃ©ponse gÃ©nÃ©rale sur le poids"""
        if age_days:
            return f"""**Poids des poulets de chair Ã  {age_days} jours :**

Les fourchettes de poids varient selon la race et le sexe :

ğŸ“Š **Fourchettes gÃ©nÃ©rales** :
â€¢ Ross 308 : 300-800g (selon sexe)
â€¢ Cobb 500 : 290-780g (selon sexe)
â€¢ Hubbard : 280-760g (selon sexe)

âš ï¸ **Important** : Ces valeurs sont indicatives. Pour des donnÃ©es prÃ©cises, spÃ©cifiez la race et le sexe."""
        else:
            return """**Poids des poulets de chair :**

Le poids varie Ã©normÃ©ment selon l'Ã¢ge, la race et le sexe :

ğŸ“ˆ **Ã‰volution gÃ©nÃ©rale** :
â€¢ 7 jours : 150-220g
â€¢ 14 jours : 350-550g  
â€¢ 21 jours : 700-1050g
â€¢ 28 jours : 1200-1700g
â€¢ 35 jours : 1800-2400g"""

    def _generate_general_growth_response(self, age_days: Optional[int], context: Dict = None) -> str:
        """GÃ©nÃ¨re une rÃ©ponse gÃ©nÃ©rale sur la croissance"""
        return """**Croissance des poulets de chair :**

ğŸš€ **Phases de croissance** :
â€¢ **DÃ©marrage** (0-14j) : Croissance rapide, dÃ©veloppement digestif
â€¢ **Croissance** (15-28j) : Gain de poids maximal
â€¢ **Finition** (29j+) : Optimisation du rendement

ğŸ“ˆ **Facteurs clÃ©s** :
â€¢ Alimentation adaptÃ©e Ã  chaque phase
â€¢ TempÃ©rature et ventilation optimales
â€¢ DensitÃ© d'Ã©levage appropriÃ©e
â€¢ Suivi sanitaire rigoureux"""

    def _generate_general_health_response(self, age_days: Optional[int], context: Dict = None) -> str:
        """GÃ©nÃ¨re une rÃ©ponse gÃ©nÃ©rale sur la santÃ©"""
        return """**SantÃ© des poulets de chair :**

ğŸ¥ **Surveillance quotidienne** :
â€¢ Observation du comportement gÃ©nÃ©ral
â€¢ ContrÃ´le de la consommation d'eau et d'aliment
â€¢ VÃ©rification des signes cliniques

âš ï¸ **Signaux d'alerte** :
â€¢ MortalitÃ© anormale (>1% par semaine)
â€¢ Baisse d'appÃ©tit ou de croissance
â€¢ SymptÃ´mes respiratoires ou digestifs
â€¢ Changements de comportement

ğŸ’¡ **En cas de doute**, contactez immÃ©diatement votre vÃ©tÃ©rinaire avicole."""

    def _generate_general_feeding_response(self, age_days: Optional[int], context: Dict = None) -> str:
        """GÃ©nÃ¨re une rÃ©ponse gÃ©nÃ©rale sur l'alimentation"""
        return """**Alimentation des poulets de chair :**

ğŸ½ï¸ **Programmes alimentaires** :
â€¢ **Starter** (0-14j) : 20-22% protÃ©ines, 3000-3100 kcal/kg
â€¢ **Grower** (15-28j) : 18-20% protÃ©ines, 3100-3200 kcal/kg  
â€¢ **Finisher** (29j+) : 16-18% protÃ©ines, 3200-3300 kcal/kg

ğŸ’§ **Eau** :
â€¢ AccÃ¨s permanent Ã  eau propre et fraÃ®che
â€¢ Ratio eau/aliment : 1,8-2,2 litres/kg d'aliment

âš¡ **Distribution** :
â€¢ Ad libitum recommandÃ©
â€¢ Surveillance rÃ©guliÃ¨re de la consommation"""

    def _generate_general_default_response(self, age_days: Optional[int], context: Dict = None) -> str:
        """GÃ©nÃ¨re une rÃ©ponse gÃ©nÃ©rale par dÃ©faut"""
        return """**Ã‰levage de poulets de chair :**

ğŸ” **Points essentiels** :
â€¢ Respect des standards de race pour le poids et la croissance
â€¢ Surveillance quotidienne de la santÃ© et du comportement
â€¢ Alimentation adaptÃ©e aux phases de dÃ©veloppement
â€¢ Conditions d'ambiance optimales (tempÃ©rature, ventilation)

ğŸ’¡ **Pour des conseils spÃ©cifiques**, prÃ©cisez votre question avec la race, l'Ã¢ge et le contexte de votre Ã©levage."""

    def _generate_precise_weight_response(self, breed: str, age_days: int, sex: str, context: Dict = None) -> ResponseData:
        """Fallback pour rÃ©ponse prÃ©cise de poids"""
        return ResponseData(
            response=f"Pour {breed} {sex} Ã  {age_days} jours, consultez les standards officiels de la race "
                    f"ou contactez votre fournisseur de souche pour les donnÃ©es prÃ©cises.",
            response_type="precise_weight_fallback",
            confidence=0.6
        )

    def _generate_precise_growth_response(self, breed: str, age_days: int, sex: str, context: Dict = None) -> ResponseData:
        """GÃ©nÃ¨re une rÃ©ponse prÃ©cise sur la croissance"""
        return ResponseData(
            response=f"**Croissance {breed} {sex} Ã  {age_days} jours :**\n\n"
                    f"Consultez les courbes de croissance officielles de la souche "
                    f"pour des donnÃ©es prÃ©cises adaptÃ©es Ã  vos conditions d'Ã©levage.",
            response_type="precise_growth",
            confidence=0.7
        )

# =============================================================================
# âœ… CONSERVATION: Fonctions utilitaires originales
# =============================================================================

def quick_generate(question: str, entities: Dict[str, Any], response_type: str) -> str:
    """GÃ©nÃ©ration rapide pour usage simple (fonction originale conservÃ©e)"""
    generator = UnifiedResponseGenerator()
    
    # CrÃ©er un objet de classification simulÃ©
    class MockClassification:
        def __init__(self, resp_type):
            from .smart_classifier import ResponseType
            self.response_type = ResponseType(resp_type)
            self.missing_entities = []
            self.merged_entities = entities
            self.weight_data = {}
    
    classification = MockClassification(response_type)
    
    # ğŸ†• ADAPTATION: Appel async gÃ©rÃ© pour compatibilitÃ©
    import asyncio
    try:
        loop = asyncio.get_event_loop()
        result = loop.run_until_complete(generator.generate(question, entities, classification))
    except RuntimeError:
        # Si pas de loop, crÃ©er un nouveau
        result = asyncio.run(generator.generate(question, entities, classification))
    
    return result.response

# =============================================================================
# âœ… CONSERVATION: Tests avec ajout de vÃ©rification maximisation + RAG
# =============================================================================

async def test_generator_maximized():
    """
    ğŸ†• Tests du gÃ©nÃ©rateur avec maximisation ContextManager SIMPLE + RAG
    """
    generator = UnifiedResponseGenerator()
    
    print("ğŸ§ª Test gÃ©nÃ©rateur MAXIMISATION CONTEXTMANAGER + RAG")
    print("=" * 60)
    
    # Afficher les statistiques
    stats = generator.get_generation_stats()
    print(f"ğŸ“Š Statistiques systÃ¨me:")
    print(f"   - Services IA disponibles: {stats['ai_services_available']}")
    print(f"   - ContextManager maximisÃ©: {stats['context_maximization_enabled']}")
    print(f"   - Support RAG: {stats['rag_support_enabled']}")
    print(f"   - Features maximisation: {len(stats['maximization_features'])}")
    for feature in stats['maximization_features']:
        print(f"     â€¢ {feature}")
    
    # Test avec donnÃ©es contextuelles
    class MockContextualClassification:
        def __init__(self):
            from .smart_classifier import ResponseType
            self.response_type = ResponseType.CONTEXTUAL_ANSWER
            self.merged_entities = {
                'breed': 'ross_308',
                'age_days': 12,
                'sex': 'male',
                'context_type': 'performance',
                'age_context_inherited': True
            }
            self.weight_data = {
                'breed': 'Ross 308',
                'age_days': 12,
                'sex': 'male',
                'weight_range': (380, 420),
                'target_weight': 400,
                'alert_thresholds': {'low': 323, 'high': 483},
                'confidence': 0.95
            }
    
    # Test gÃ©nÃ©ration classique
    question = "Pour un Ross 308 mÃ¢le"
    entities = {'breed': 'ross_308', 'sex': 'male', 'age_days': 12}
    classification = MockContextualClassification()
    conversation_id = "test_conversation_maximized_123"
    
    result = await generator.generate(question, entities, classification, conversation_id)
    
    print(f"\nğŸ¯ RÃ©sultats du test classique:")
    print(f"   Question: {question}")
    print(f"   EntitÃ©s: {entities}")
    print(f"   Type rÃ©ponse: {result.response_type}")
    print(f"   Confiance: {result.confidence}")
    print(f"   GÃ©nÃ©rÃ© par IA: {result.ai_generated}")
    print(f"   RAG utilisÃ©: {result.rag_used}")
    print(f"   Contexte data: {bool(result.context_data)}")
    print(f"   AperÃ§u: {result.response[:150]}...")
    
    # ğŸ†• Test gÃ©nÃ©ration avec RAG
    print(f"\nğŸ¯ Test gÃ©nÃ©ration RAG:")
    mock_rag_results = [
        {"text": "Les poulets Ross 308 mÃ¢les atteignent 400g Ã  12 jours selon les standards officiels.", "score": 0.95},
        {"text": "Surveillance recommandÃ©e pour maintenir la croissance optimale.", "score": 0.88}
    ]
    
    rag_result = generator.generate_with_rag(question, entities, classification, mock_rag_results)
    
    print(f"   RAG rÃ©sultats: {len(mock_rag_results)} documents")
    print(f"   Type rÃ©ponse RAG: {rag_result.response_type}")
    print(f"   RAG utilisÃ©: {rag_result.rag_used}")
    print(f"   Documents consultÃ©s: {rag_result.documents_consulted}")
    print(f"   Sources: {len(rag_result.sources)}")
    print(f"   AperÃ§u RAG: {rag_result.response[:150]}...")
    
    # VÃ©rifications spÃ©cifiques Ã  la maximisation + RAG
    success_checks = []
    success_checks.append(("DonnÃ©es 380-420g", "380-420" in result.response or "400" in result.response))
    success_checks.append(("Mention Ross 308", "Ross 308" in result.response))
    success_checks.append(("Structure ResponseData avec context_data", hasattr(result, 'context_data')))
    success_checks.append(("Poids data prÃ©sent", bool(result.weight_data)))
    success_checks.append(("Context data ajoutÃ©", bool(result.context_data)))
    success_checks.append(("RAG support disponible", hasattr(generator, 'generate_with_rag')))
    success_checks.append(("RAG flag correct", rag_result.rag_used == True))
    success_checks.append(("Documents RAG comptÃ©s", rag_result.documents_consulted == 2))
    
    print(f"\nâœ… VÃ©rifications maximisation + RAG:")
    for check_name, passed in success_checks:
        status = "âœ…" if passed else "âŒ"
        print(f"   {status} {check_name}")
    
    if all(check[1] for check in success_checks):
        print(f"\nğŸ‰ SUCCESS: GÃ©nÃ©rateur avec ContextManager MAXIMISÃ‰ + RAG opÃ©rationnel!")
        print(f"   - RÃ©cupÃ©ration contexte enrichie: âœ…")
        print(f"   - Sauvegarde maximisÃ©e: âœ…") 
        print(f"   - Ã‰valuation qualitÃ© contexte: âœ…")
        print(f"   - InfÃ©rence topic/intent: âœ…")
        print(f"   - Support RAG complet: âœ…")
        print(f"   - GÃ©nÃ©ration IA + RAG: âœ…")
        print(f"   - Fallback templates + RAG: âœ…")
        print(f"   - SANS sur-ingÃ©nierie: âœ…")
    else:
        print(f"\nâš ï¸  ATTENTION: Certaines vÃ©rifications ont Ã©chouÃ©")

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_generator_maximized())