"""
app/api/v1/expert.py - Version avec int√©gration RAG compl√®te
"""
import os
import sys
import logging
import uuid
import time
from datetime import datetime
from typing import Optional, List, Dict, Any

from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field

# OpenAI import s√©curis√©
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# Import du syst√®me RAG depuis main
try:
    # Remonter au r√©pertoire backend pour importer depuis main
    backend_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    if backend_dir not in sys.path:
        sys.path.insert(0, backend_dir)
    
    from main import process_question_with_rag, rag_embedder, get_rag_status
    RAG_AVAILABLE = True
except ImportError as e:
    RAG_AVAILABLE = False
    process_question_with_rag = None
    rag_embedder = None
    get_rag_status = lambda: "not_available"
    logging.warning(f"‚ö†Ô∏è RAG system not available in expert router: {e}")

router = APIRouter(tags=["expert"])
logger = logging.getLogger(__name__)

# =============================================================================
# MOD√àLES PYDANTIC
# =============================================================================

class QuestionRequest(BaseModel):
    """Request model pour questions expert"""
    text: str = Field(..., min_length=1, max_length=2000, description="Question text")
    language: Optional[str] = Field("fr", description="Response language (fr, en, es)")
    speed_mode: Optional[str] = Field("balanced", description="Speed mode: fast, balanced, quality")
    context: Optional[str] = Field(None, description="Additional context")

class ExpertResponse(BaseModel):
    """Response model avec support RAG"""
    question: str
    response: str
    conversation_id: str
    rag_used: bool
    timestamp: str
    language: str
    response_time_ms: int
    mode: str
    sources: Optional[List[Dict[str, Any]]] = []
    processing_time: Optional[float] = None
    note: Optional[str] = None

class FeedbackRequest(BaseModel):
    """Feedback request model"""
    conversation_id: Optional[str] = None
    rating: str = Field(..., description="Rating: positive, negative, neutral")
    comment: Optional[str] = Field(None, max_length=500, description="Optional comment")

class TopicsResponse(BaseModel):
    """Topics response model"""
    topics: List[str]
    language: str
    count: int

# =============================================================================
# PROMPTS MULTI-LANGUES
# =============================================================================

EXPERT_PROMPTS = {
    "fr": """Tu es un expert v√©t√©rinaire sp√©cialis√© en sant√© et nutrition animale, particuli√®rement pour les poulets de chair Ross 308. 
R√©ponds de mani√®re pr√©cise et pratique en fran√ßais, en donnant des conseils bas√©s sur les meilleures pratiques du secteur.""",
    
    "en": """You are a veterinary expert specialized in animal health and nutrition, particularly for Ross 308 broiler chickens.
Answer precisely and practically in English, providing advice based on industry best practices.""",
    
    "es": """Eres un experto veterinario especializado en salud y nutrici√≥n animal, particularmente para pollos de engorde Ross 308.
Responde de manera precisa y pr√°ctica en espa√±ol, dando consejos basados en las mejores pr√°cticas del sector."""
}

# =============================================================================
# FONCTIONS HELPER
# =============================================================================

def get_expert_prompt(language: str) -> str:
    """Get expert system prompt for language"""
    return EXPERT_PROMPTS.get(language, EXPERT_PROMPTS["fr"])

def get_fallback_response(question: str, language: str = "fr") -> str:
    """R√©ponse de fallback si ni RAG ni OpenAI disponibles"""
    fallback_responses = {
        "fr": f"Je suis un expert v√©t√©rinaire. Pour votre question sur '{question[:50]}...', je recommande de surveiller les param√®tres environnementaux et de maintenir de bonnes pratiques d'hygi√®ne. Veuillez r√©essayer plus tard pour une r√©ponse plus d√©taill√©e.",
        "en": f"I am a veterinary expert. For your question about '{question[:50]}...', I recommend monitoring environmental parameters and maintaining good hygiene practices. Please try again later for a more detailed response.",
        "es": f"Soy un experto veterinario. Para su pregunta sobre '{question[:50]}...', recomiendo monitorear los par√°metros ambientales y mantener buenas pr√°cticas de higiene. Intente nuevamente m√°s tarde para una respuesta m√°s detallada."
    }
    return fallback_responses.get(language, fallback_responses["fr"])

async def process_question_direct_openai(question: str, language: str = "fr", speed_mode: str = "balanced") -> Dict[str, Any]:
    """Fallback direct OpenAI sans RAG"""
    if not OPENAI_AVAILABLE:
        return {
            "response": get_fallback_response(question, language),
            "mode": "static_fallback",
            "note": "Service temporairement indisponible"
        }
    
    try:
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            logger.warning("OpenAI API key not found")
            return {
                "response": get_fallback_response(question, language),
                "mode": "static_fallback",
                "note": "Configuration manquante"
            }
        
        openai.api_key = api_key
        system_prompt = get_expert_prompt(language)
        
        # Configuration par mode
        model_config = {
            "fast": {"model": "gpt-3.5-turbo", "max_tokens": 300},
            "balanced": {"model": "gpt-3.5-turbo", "max_tokens": 500},
            "quality": {"model": "gpt-4o-mini", "max_tokens": 800}
        }
        
        config = model_config.get(speed_mode, model_config["balanced"])
        
        response = openai.chat.completions.create(
            model=config["model"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            temperature=0.7,
            max_tokens=config["max_tokens"],
            timeout=15
        )
        
        return {
            "response": response.choices[0].message.content,
            "mode": "direct_openai",
            "note": "R√©ponse sans recherche documentaire (RAG non disponible)"
        }
        
    except Exception as e:
        logger.error(f"OpenAI error: {e}")
        return {
            "response": get_fallback_response(question, language),
            "mode": "error_fallback",
            "note": f"Erreur technique: {str(e)}"
        }

# =============================================================================
# ENDPOINTS AVEC INT√âGRATION RAG
# =============================================================================

@router.post("/ask-public", response_model=ExpertResponse)
async def ask_expert_public(request: QuestionRequest):
    """Ask question sans authentification - AVEC RAG"""
    start_time = time.time()
    
    try:
        question_text = request.text.strip()
        
        if not question_text:
            raise HTTPException(status_code=400, detail="Question text is required")
        
        conversation_id = str(uuid.uuid4())
        logger.info(f"üåê Question publique - ID: {conversation_id}")
        
        # Utiliser le syst√®me RAG si disponible
        if RAG_AVAILABLE and process_question_with_rag:
            try:
                logger.info(f"üîç Utilisation du syst√®me RAG pour la question: {question_text[:50]}...")
                
                # Appeler la fonction RAG depuis main.py
                result = await process_question_with_rag(
                    question=question_text,
                    user=None,  # Pas d'utilisateur pour l'endpoint public
                    language=request.language or "fr",
                    speed_mode=request.speed_mode or "balanced"
                )
                
                # Extraire les informations du r√©sultat
                answer = result.get("response", "")
                rag_used = result.get("mode", "").startswith("rag")
                sources = result.get("sources", [])
                note = result.get("note", "")
                
                logger.info(f"‚úÖ RAG utilis√©: {rag_used}, Sources trouv√©es: {len(sources)}")
                
            except Exception as rag_error:
                logger.error(f"‚ùå Erreur RAG, fallback OpenAI: {rag_error}")
                # Fallback sur OpenAI direct
                fallback_result = await process_question_direct_openai(
                    question_text, 
                    request.language or "fr",
                    request.speed_mode or "balanced"
                )
                answer = fallback_result["response"]
                rag_used = False
                sources = []
                note = fallback_result.get("note", "")
        else:
            # RAG non disponible, utiliser OpenAI direct
            logger.info("‚ö†Ô∏è RAG non disponible, utilisation OpenAI direct")
            fallback_result = await process_question_direct_openai(
                question_text,
                request.language or "fr", 
                request.speed_mode or "balanced"
            )
            answer = fallback_result["response"]
            rag_used = False
            sources = []
            note = fallback_result.get("note", "")
        
        response_time_ms = int((time.time() - start_time) * 1000)
        
        response_data = ExpertResponse(
            question=question_text,
            response=answer,
            conversation_id=conversation_id,
            rag_used=rag_used,
            timestamp=datetime.now().isoformat(),
            language=request.language or "fr",
            response_time_ms=response_time_ms,
            mode="expert_router_v1_public",
            sources=sources,
            processing_time=round(time.time() - start_time, 2),
            note=note
        )
        
        logger.info(f"‚úÖ R√©ponse publique - ID: {conversation_id} - RAG: {rag_used}")
        return response_data
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur ask expert public: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur interne: {str(e)}")

@router.post("/ask", response_model=ExpertResponse)
async def ask_expert(request: QuestionRequest):
    """Ask question avec authentification - AVEC RAG"""
    # M√™me logique que ask_expert_public mais pourrait avoir des fonctionnalit√©s suppl√©mentaires
    # Pour l'instant, on r√©utilise la m√™me logique
    return await ask_expert_public(request)

@router.post("/feedback")
async def submit_feedback(request: FeedbackRequest):
    """Submit feedback on response"""
    try:
        logger.info(f"üìä Feedback re√ßu: {request.rating} pour conversation {request.conversation_id}")
        
        # Ici on pourrait sauvegarder le feedback en base de donn√©es
        
        return {
            "success": True,
            "message": "Feedback enregistr√© avec succ√®s",
            "timestamp": datetime.now().isoformat(),
            "conversation_id": request.conversation_id,
            "source": "expert_router_v1"
        }
    
    except Exception as e:
        logger.error(f"‚ùå Erreur feedback: {e}")
        raise HTTPException(status_code=500, detail="Erreur enregistrement feedback")

@router.get("/topics", response_model=TopicsResponse)
async def get_suggested_topics(language: str = "fr"):
    """Get suggested topics - peut √™tre enrichi avec les documents RAG"""
    try:
        topics_by_language = {
            "fr": [
                "Protocoles Compass pour l'analyse de performance",
                "Probl√®mes de croissance poulets Ross 308",
                "Temp√©rature optimale pour √©levage",
                "Patterns de diagnostic du poids",
                "Mortalit√© √©lev√©e - diagnostic",
                "Ventilation et qualit√© d'air",
                "Protocoles de vaccination",
                "Indices de conversion alimentaire"
            ],
            "en": [
                "Compass Performance Analysis Protocol",
                "Ross 308 growth problems",
                "Weight Performance Diagnostic Patterns",
                "Optimal temperature for farming",
                "High mortality - diagnosis", 
                "Ventilation and air quality",
                "Vaccination protocols",
                "Feed conversion ratios"
            ],
            "es": [
                "Protocolos Compass an√°lisis rendimiento",
                "Problemas crecimiento pollos Ross 308",
                "Patrones diagn√≥stico peso",
                "Temperatura √≥ptima crianza",
                "Mortalidad alta - diagn√≥stico",
                "Ventilaci√≥n y calidad aire",
                "Protocolos vacunaci√≥n",
                "√çndices conversi√≥n alimentaria"
            ]
        }
        
        topics = topics_by_language.get(language, topics_by_language["fr"])
        
        return TopicsResponse(
            topics=topics,
            language=language,
            count=len(topics)
        )
    
    except Exception as e:
        logger.error(f"‚ùå Erreur topics: {e}")
        raise HTTPException(status_code=500, detail="Erreur r√©cup√©ration topics")

@router.get("/history")
async def get_conversation_history():
    """Get conversation history"""
    return {
        "status": "expert_router_v1",
        "message": "Historique conversations via router expert",
        "note": "Version avec int√©gration RAG",
        "rag_status": get_rag_status() if RAG_AVAILABLE else "not_available"
    }

# =============================================================================
# ENDPOINTS DE DEBUG
# =============================================================================

@router.get("/debug/status")
async def debug_status():
    """Debug endpoint pour v√©rifier le statut du service"""
    rag_status = "not_available"
    rag_documents = 0
    
    if RAG_AVAILABLE:
        rag_status = get_rag_status()
        if rag_embedder and hasattr(rag_embedder, 'documents'):
            rag_documents = len(rag_embedder.documents) if rag_embedder.documents else 0
    
    return {
        "expert_service_available": OPENAI_AVAILABLE,
        "rag_available": RAG_AVAILABLE,
        "rag_status": rag_status,
        "rag_documents": rag_documents,
        "module_path": __name__,
        "timestamp": datetime.now().isoformat(),
        "version": "expert_router_v1_with_rag"
    }

@router.get("/debug/test-question")
async def debug_test_question():
    """Test endpoint avec question qui devrait d√©clencher le RAG"""
    test_request = QuestionRequest(
        text="What are the Compass Performance Analysis Protocol diagnostic patterns for Ross 308?",
        language="en",
        speed_mode="quality"
    )
    
    try:
        response = await ask_expert_public(test_request)
        return {
            "test_status": "success",
            "response_preview": response.response[:200] + "...",
            "response_time_ms": response.response_time_ms,
            "mode": response.mode,
            "rag_used": response.rag_used,
            "sources_count": len(response.sources) if response.sources else 0,
            "rag_available": RAG_AVAILABLE,
            "rag_status": get_rag_status() if RAG_AVAILABLE else "not_available"
        }
    except Exception as e:
        return {
            "test_status": "error",
            "error": str(e),
            "rag_available": RAG_AVAILABLE,
            "timestamp": datetime.now().isoformat()
        }

@router.get("/debug/routes")
async def debug_routes():
    """Debug endpoint pour lister les routes disponibles"""
    return {
        "available_routes": [
            "/api/v1/expert/ask-public",
            "/api/v1/expert/ask", 
            "/api/v1/expert/feedback",
            "/api/v1/expert/topics",
            "/api/v1/expert/history",
            "/api/v1/expert/debug/status",
            "/api/v1/expert/debug/test-question",
            "/api/v1/expert/debug/routes"
        ],
        "features": {
            "rag_integration": RAG_AVAILABLE,
            "openai_fallback": OPENAI_AVAILABLE,
            "multi_language": True,
            "speed_modes": ["fast", "balanced", "quality"]
        },
        "timestamp": datetime.now().isoformat()
    }

# =============================================================================
# CONFIGURATION AU D√âMARRAGE
# =============================================================================

# Configuration OpenAI
openai_api_key = os.getenv('OPENAI_API_KEY')
if openai_api_key and OPENAI_AVAILABLE:
    openai.api_key = openai_api_key
    logger.info("‚úÖ OpenAI configur√© avec succ√®s dans expert router v1")
else:
    logger.warning("‚ö†Ô∏è OpenAI API key manquante ou module indisponible dans expert router v1")

# Log du statut RAG
if RAG_AVAILABLE:
    logger.info("‚úÖ Syst√®me RAG disponible dans expert router")
else:
    logger.warning("‚ö†Ô∏è Syst√®me RAG non disponible dans expert router - fallback sur OpenAI direct")