"""
app/api/expert.py - Version corrig√©e sans conflits
"""
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import logging
import uuid
import time
import openai
from datetime import datetime

# Configuration OpenAI - direct import
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

router = APIRouter(tags=["expert"])
logger = logging.getLogger(__name__)

# =============================================================================
# MOD√àLES PYDANTIC COMPATIBLES AVEC MAIN.PY
# =============================================================================

class QuestionRequest(BaseModel):
    """Request model unifi√© - Compatible avec main.py et frontend"""
    text: str = Field(..., description="Question text", min_length=1, max_length=2000)
    language: Optional[str] = Field("fr", description="Response language (fr, en, es)")
    user_id: Optional[str] = Field(None, description="User ID pour logging")
    speed_mode: Optional[str] = Field("balanced", description="Speed mode")
    context: Optional[str] = Field(None, description="Additional context")

class FeedbackRequest(BaseModel):
    """Feedback request model"""
    question: str = Field(..., description="Original question")
    response: str = Field(..., description="AI response received")
    rating: str = Field(..., description="User rating (positive, negative, neutral)")
    comment: Optional[str] = Field(None, description="Additional feedback comment")

class ExpertResponse(BaseModel):
    """Response model compatible avec main.py"""
    question: str
    response: str
    conversation_id: str
    rag_used: bool
    timestamp: str
    language: str
    response_time_ms: int
    confidence_score: Optional[float] = None
    mode: Optional[str] = "expert_router"
    note: Optional[str] = None
    sources: Optional[List[Dict[str, Any]]] = []

class TopicsResponse(BaseModel):
    """Topics response model"""
    topics: List[str]
    language: str
    count: int

class DebugResponse(BaseModel):
    """Debug response model"""
    expert_service_available: bool
    expert_service_object: bool
    module_path: str
    timestamp: str

# =============================================================================
# PROMPTS MULTI-LANGUES
# =============================================================================

EXPERT_PROMPTS = {
    "fr": {
        "system": """Tu es un expert v√©t√©rinaire sp√©cialis√© en sant√© et nutrition animale, particuli√®rement pour les poulets de chair Ross 308. 
R√©ponds de mani√®re pr√©cise et pratique en fran√ßais, en donnant des conseils bas√©s sur les meilleures pratiques du secteur.""",
        "fallback": "Je suis un expert v√©t√©rinaire. Voici une r√©ponse bas√©e sur les meilleures pratiques en √©levage de volaille."
    },
    "en": {
        "system": """You are a veterinary expert specialized in animal health and nutrition, particularly for Ross 308 broiler chickens.
Answer precisely and practically in English, providing advice based on industry best practices.""",
        "fallback": "I am a veterinary expert. Here's a response based on poultry farming best practices."
    },
    "es": {
        "system": """Eres un experto veterinario especializado en salud y nutrici√≥n animal, particularmente para pollos de engorde Ross 308.
Responde de manera precisa y pr√°ctica en espa√±ol, dando consejos basados en las mejores pr√°cticas del sector.""",
        "fallback": "Soy un experto veterinario. Aqu√≠ tienes una respuesta basada en las mejores pr√°cticas av√≠colas."
    }
}

# =============================================================================
# FONCTIONS HELPER
# =============================================================================

def get_expert_prompt(language: str) -> str:
    """Get expert system prompt for language"""
    return EXPERT_PROMPTS.get(language, EXPERT_PROMPTS["fr"])["system"]

def get_fallback_response(question: str, language: str = "fr") -> dict:
    """R√©ponse de fallback si OpenAI n'est pas disponible"""
    fallback_base = EXPERT_PROMPTS.get(language, EXPERT_PROMPTS["fr"])["fallback"]
    
    # R√©ponses sp√©cifiques selon les mots-cl√©s
    question_lower = question.lower()
    
    if any(word in question_lower for word in ["temp√©rature", "temperature", "temp"]):
        specific_responses = {
            "fr": f"{fallback_base} Pour les Ross 308, maintenez 32-35¬∞C la premi√®re semaine, puis r√©duisez progressivement de 2-3¬∞C par semaine.",
            "en": f"{fallback_base} For Ross 308, maintain 32-35¬∞C in the first week, then gradually reduce by 2-3¬∞C per week.",
            "es": f"{fallback_base} Para Ross 308, mant√©n 32-35¬∞C la primera semana, luego reduce gradualmente 2-3¬∞C por semana."
        }
    elif any(word in question_lower for word in ["croissance", "growth", "crecimiento"]):
        specific_responses = {
            "fr": f"{fallback_base} Les Ross 308 doivent atteindre environ 2.5kg √† 35 jours avec un bon indice de conversion.",
            "en": f"{fallback_base} Ross 308 should reach approximately 2.5kg at 35 days with good feed conversion ratio.",
            "es": f"{fallback_base} Los Ross 308 deben alcanzar aproximadamente 2.5kg a los 35 d√≠as con buen √≠ndice de conversi√≥n."
        }
    elif any(word in question_lower for word in ["mortalit√©", "mortality", "mortalidad"]):
        specific_responses = {
            "fr": f"{fallback_base} Une mortalit√© √©lev√©e peut indiquer des probl√®mes sanitaires, de ventilation ou de temp√©rature. Consultez un v√©t√©rinaire.",
            "en": f"{fallback_base} High mortality may indicate health, ventilation, or temperature issues. Consult a veterinarian.",
            "es": f"{fallback_base} La alta mortalidad puede indicar problemas sanitarios, de ventilaci√≥n o temperatura. Consulte un veterinario."
        }
    else:
        specific_responses = {
            "fr": f"{fallback_base} Pour votre question sur '{question[:50]}...', je recommande de surveiller les param√®tres environnementaux et de maintenir de bonnes pratiques d'hygi√®ne.",
            "en": f"{fallback_base} For your question about '{question[:50]}...', I recommend monitoring environmental parameters and maintaining good hygiene practices.",
            "es": f"{fallback_base} Para su pregunta sobre '{question[:50]}...', recomiendo monitorear los par√°metros ambientales y mantener buenas pr√°cticas de higiene."
        }
    
    response_text = specific_responses.get(language, specific_responses["fr"])
    
    return {
        "success": True,
        "response": response_text,
        "rag_used": False,
        "timestamp": datetime.now().isoformat(),
        "confidence_score": 0.7
    }

async def process_question_openai(question: str, language: str = "fr") -> dict:
    """Process question using OpenAI directly"""
    if not OPENAI_AVAILABLE:
        return get_fallback_response(question, language)
    
    try:
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            logger.warning("OpenAI API key not found")
            return get_fallback_response(question, language)
        
        openai.api_key = api_key
        system_prompt = get_expert_prompt(language)
        
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            temperature=0.7,
            max_tokens=500,
            timeout=15
        )
        
        answer = response.choices[0].message.content
        
        return {
            "success": True,
            "response": answer,
            "rag_used": False,
            "timestamp": datetime.now().isoformat(),
            "confidence_score": 0.8
        }
        
    except Exception as e:
        logger.error(f"OpenAI error: {e}")
        return get_fallback_response(question, language)

# =============================================================================
# ENDPOINTS NON-CONFLICTUELS
# =============================================================================

@router.post("/ask-router", response_model=ExpertResponse)
async def ask_expert_router(request: QuestionRequest):
    """Ask question to AI expert - Router version (non-conflictuel)"""
    start_time = time.time()
    
    try:
        question_text = request.text.strip()
        
        if not question_text:
            raise HTTPException(status_code=400, detail="Question text is required")
        
        conversation_id = str(uuid.uuid4())
        logger.info(f"üîç Question router - ID: {conversation_id}")
        logger.info(f"üìù Question: {question_text[:100]}...")
        
        # Process avec OpenAI
        result = await process_question_openai(question_text, request.language or "fr")
        
        response_time_ms = int((time.time() - start_time) * 1000)
        
        response_data = {
            "question": question_text,
            "response": result.get("response", "R√©ponse non disponible"),
            "conversation_id": conversation_id,
            "rag_used": result.get("rag_used", False),
            "timestamp": result.get("timestamp", datetime.now().isoformat()),
            "language": request.language or "fr",
            "response_time_ms": response_time_ms,
            "confidence_score": result.get("confidence_score"),
            "mode": "expert_router",
            "note": "Trait√© par le router expert",
            "sources": []
        }
        
        logger.info(f"‚úÖ R√©ponse router g√©n√©r√©e - ID: {conversation_id}, Temps: {response_time_ms}ms")
        
        return ExpertResponse(**response_data)
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur ask expert router: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur interne: {str(e)}")

@router.post("/ask-public-router", response_model=ExpertResponse)
async def ask_expert_public_router(request: QuestionRequest):
    """Ask question sans authentification - Router version (non-conflictuel)"""
    start_time = time.time()
    
    try:
        question_text = request.text.strip()
        
        if not question_text:
            raise HTTPException(status_code=400, detail="Question text is required")
        
        conversation_id = str(uuid.uuid4())
        logger.info(f"üåê Question publique router - ID: {conversation_id}")
        
        # Process avec OpenAI
        result = await process_question_openai(question_text, request.language or "fr")
        
        response_time_ms = int((time.time() - start_time) * 1000)
        
        response_data = {
            "question": question_text,
            "response": result.get("response", "R√©ponse non disponible"),
            "conversation_id": conversation_id,
            "rag_used": result.get("rag_used", False),
            "timestamp": result.get("timestamp", datetime.now().isoformat()),
            "language": request.language or "fr",
            "response_time_ms": response_time_ms,
            "confidence_score": result.get("confidence_score"),
            "mode": "public_router",
            "note": "Acc√®s public via router - fonctionnalit√©s limit√©es",
            "sources": []
        }
        
        logger.info(f"‚úÖ R√©ponse publique router - ID: {conversation_id}")
        
        return ExpertResponse(**response_data)
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur ask expert public router: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur interne: {str(e)}")

@router.post("/feedback-router")
async def submit_feedback_router(request: FeedbackRequest):
    """Submit feedback on response - Router version"""
    try:
        logger.info(f"üìä Feedback router re√ßu: {request.rating}")
        
        return {
            "success": True,
            "message": "Feedback enregistr√© avec succ√®s (router)",
            "timestamp": datetime.now().isoformat(),
            "source": "expert_router"
        }
    
    except Exception as e:
        logger.error(f"‚ùå Erreur feedback router: {e}")
        raise HTTPException(status_code=500, detail="Erreur enregistrement feedback")

@router.get("/topics-router", response_model=TopicsResponse)
async def get_suggested_topics_router(language: str = "fr"):
    """Get suggested topics - Router version"""
    try:
        topics_by_language = {
            "fr": [
                "Probl√®mes de croissance poulets Ross 308",
                "Temp√©rature optimale pour √©levage",
                "Indices de conversion alimentaire",
                "Mortalit√© √©lev√©e - diagnostic",
                "Ventilation et qualit√© d'air",
                "Protocoles de vaccination",
                "Nutrition sp√©cialis√©e",
                "Gestion sanitaire"
            ],
            "en": [
                "Ross 308 growth problems",
                "Optimal temperature for farming",
                "Feed conversion ratios",
                "High mortality - diagnosis", 
                "Ventilation and air quality",
                "Vaccination protocols",
                "Specialized nutrition",
                "Health management"
            ],
            "es": [
                "Problemas crecimiento pollos Ross 308",
                "Temperatura √≥ptima crianza",
                "√çndices conversi√≥n alimentaria",
                "Mortalidad alta - diagn√≥stico",
                "Ventilaci√≥n y calidad aire",
                "Protocolos vacunaci√≥n",
                "Nutrici√≥n especializada",
                "Gesti√≥n sanitaria"
            ]
        }
        
        topics = topics_by_language.get(language, topics_by_language["fr"])
        
        return TopicsResponse(
            topics=topics,
            language=language,
            count=len(topics)
        )
    
    except Exception as e:
        logger.error(f"‚ùå Erreur topics router: {e}")
        raise HTTPException(status_code=500, detail="Erreur r√©cup√©ration topics")

@router.get("/history-router")
async def get_conversation_history_router():
    """Get conversation history - Router version"""
    return {
        "status": "router_version",
        "message": "Historique conversations via router expert",
        "redirect": "/api/v1/expert/history-router",
        "note": "Version router - fonctionnalit√©s limit√©es"
    }

# =============================================================================
# ENDPOINTS DE DEBUG ET STATUS
# =============================================================================

@router.get("/debug/status", response_model=DebugResponse)
async def debug_status():
    """Debug endpoint pour v√©rifier le statut du service"""
    return DebugResponse(
        expert_service_available=OPENAI_AVAILABLE,
        expert_service_object=OPENAI_AVAILABLE,
        module_path=__name__,
        timestamp=datetime.now().isoformat()
    )

@router.get("/debug/test-question")
async def debug_test_question():
    """Test endpoint avec question simple"""
    test_request = QuestionRequest(
        text="Test de fonctionnement du syst√®me Ross 308",
        language="fr"
    )
    
    try:
        response = await ask_expert_public_router(test_request)
        return {
            "test_status": "success",
            "response_preview": response.response[:100] + "...",
            "response_time_ms": response.response_time_ms,
            "mode": response.mode
        }
    except Exception as e:
        return {
            "test_status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@router.get("/debug/routes")
async def debug_routes():
    """Debug endpoint pour lister les routes disponibles"""
    return {
        "available_routes": [
            "/expert/ask-router",
            "/expert/ask-public-router", 
            "/expert/feedback-router",
            "/expert/topics-router",
            "/expert/history-router",
            "/expert/debug/status",
            "/expert/debug/test-question",
            "/expert/debug/routes"
        ],
        "note": "Routes non-conflictuelles avec main.py",
        "timestamp": datetime.now().isoformat()
    }

# =============================================================================
# CONFIGURATION
# =============================================================================

# Import des variables d'environnement
import os

# Configuration OpenAI
openai_api_key = os.getenv('OPENAI_API_KEY')
if openai_api_key and OPENAI_AVAILABLE:
    openai.api_key = openai_api_key
    logger.info("‚úÖ OpenAI configur√© avec succ√®s dans expert router")
else:
    logger.warning("‚ö†Ô∏è OpenAI API key manquante ou module indisponible dans expert router")