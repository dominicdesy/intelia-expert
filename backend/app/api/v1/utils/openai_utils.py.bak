import openai
import os
import logging
from typing import Any, Dict, List, Optional
from functools import wraps
import time

# Configuration du logging
logger = logging.getLogger(__name__)

# ==================== AM√âLIORATION MAJEURE: Gestion centralis√©e de l'API key ====================
def _get_api_key() -> str:
    """
    ‚úÖ AM√âLIORATION: Fonction centralis√©e pour la gestion de la cl√© API
    
    PROBL√àME R√âSOLU:
    - Code dupliqu√© dans safe_chat_completion et safe_embedding_create
    - V√©rification r√©p√©t√©e de OPENAI_API_KEY
    
    SOLUTION:
    - Fonction unique pour r√©cup√©rer et valider la cl√©
    - Gestion d'erreurs centralis√©e
    - Configuration flexible via variables d'environnement
    """
    api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        logger.error("‚ùå OPENAI_API_KEY non configur√©e")
        raise RuntimeError(
            "OPENAI_API_KEY n'est pas configur√©e dans les variables d'environnement. "
            "Veuillez d√©finir cette variable pour utiliser les services OpenAI."
        )
    
    if len(api_key.strip()) < 10:
        logger.error("‚ùå OPENAI_API_KEY semble invalide (trop courte)")
        raise RuntimeError("OPENAI_API_KEY semble invalide - v√©rifiez la configuration")
    
    return api_key.strip()

def _configure_openai_client() -> None:
    """
    ‚úÖ AM√âLIORATION: Configuration centralis√©e du client OpenAI
    """
    try:
        api_key = _get_api_key()
        openai.api_key = api_key
        logger.debug("‚úÖ Client OpenAI configur√©")
    except Exception as e:
        logger.error(f"‚ùå Erreur configuration OpenAI: {e}")
        raise

# ==================== AM√âLIORATION: D√©corateur pour retry et gestion d'erreurs ====================
def openai_retry(max_retries: int = 2, delay: float = 1.0):
    """
    ‚úÖ NOUVEAU: D√©corateur pour retry automatique des appels OpenAI
    
    G√®re les erreurs temporaires comme:
    - Rate limiting
    - Erreurs r√©seau temporaires
    - Timeouts
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                    
                except openai.RateLimitError as e:
                    logger.warning(f"‚ö†Ô∏è Rate limit atteint (tentative {attempt + 1}/{max_retries + 1}): {e}")
                    if attempt < max_retries:
                        time.sleep(delay * (2 ** attempt))  # Backoff exponentiel
                        continue
                    last_exception = e
                    
                except openai.APITimeoutError as e:
                    logger.warning(f"‚ö†Ô∏è Timeout OpenAI (tentative {attempt + 1}/{max_retries + 1}): {e}")
                    if attempt < max_retries:
                        time.sleep(delay)
                        continue
                    last_exception = e
                    
                except openai.APIConnectionError as e:
                    logger.warning(f"‚ö†Ô∏è Erreur connexion OpenAI (tentative {attempt + 1}/{max_retries + 1}): {e}")
                    if attempt < max_retries:
                        time.sleep(delay)
                        continue
                    last_exception = e
                    
                except Exception as e:
                    # Pour les autres erreurs, pas de retry
                    logger.error(f"‚ùå Erreur OpenAI non-retry: {type(e).__name__}: {e}")
                    last_exception = e
                    break
            
            # Si on arrive ici, tous les retries ont √©chou√©
            raise RuntimeError(f"√âchec apr√®s {max_retries + 1} tentatives: {last_exception}")
        
        return wrapper
    return decorator

# ==================== FONCTION AM√âLIOR√âE: safe_chat_completion ====================
@openai_retry(max_retries=2, delay=1.0)
def safe_chat_completion(**kwargs) -> Any:
    """
    Wrapper s√©curis√© pour openai.chat.completions.create
    
    AM√âLIORATIONS APPLIQU√âES:
    - Utilisation de _get_api_key() centralis√©e (plus de duplication)
    - Retry automatique avec backoff exponentiel
    - Gestion d'erreurs sp√©cialis√©e par type
    - Validation des param√®tres d'entr√©e
    - Logging d√©taill√© pour debug
    """
    
    # ‚úÖ AM√âLIORATION: Validation des param√®tres essentiels
    if 'model' not in kwargs:
        kwargs['model'] = os.getenv('OPENAI_DEFAULT_MODEL', 'gpt-4o')
        logger.debug(f"üîß Mod√®le par d√©faut utilis√©: {kwargs['model']}")
    
    if 'messages' not in kwargs or not kwargs['messages']:
        raise ValueError("Le param√®tre 'messages' est requis et ne peut pas √™tre vide")
    
    # ‚úÖ AM√âLIORATION: Configuration avec param√®tres par d√©faut intelligents
    default_params = {
        'temperature': float(os.getenv('OPENAI_DEFAULT_TEMPERATURE', '0.7')),
        'max_tokens': int(os.getenv('OPENAI_DEFAULT_MAX_TOKENS', '500')),
        'timeout': int(os.getenv('OPENAI_DEFAULT_TIMEOUT', '30'))
    }
    
    # Appliquer les d√©fauts seulement si non sp√©cifi√©s
    for key, value in default_params.items():
        if key not in kwargs:
            kwargs[key] = value
    
    logger.debug(f"ü§ñ Appel OpenAI Chat: model={kwargs.get('model')}, temp={kwargs.get('temperature')}")
    
    try:
        # ‚úÖ AM√âLIORATION: Configuration centralis√©e
        _configure_openai_client()
        
        # ‚úÖ AM√âLIORATION: Mesure du temps de r√©ponse
        start_time = time.time()
        response = openai.chat.completions.create(**kwargs)
        elapsed_time = time.time() - start_time
        
        logger.debug(f"‚úÖ R√©ponse OpenAI Chat re√ßue en {elapsed_time:.2f}s")
        
        # ‚úÖ AM√âLIORATION: Validation de la r√©ponse
        if not response or not response.choices:
            raise RuntimeError("R√©ponse OpenAI vide ou malform√©e")
        
        # ‚úÖ AM√âLIORATION: Logging des m√©triques d'usage
        if hasattr(response, 'usage') and response.usage:
            logger.debug(f"üìä Tokens utilis√©s: {response.usage.total_tokens} "
                        f"(prompt: {response.usage.prompt_tokens}, "
                        f"completion: {response.usage.completion_tokens})")
        
        return response
        
    except openai.AuthenticationError as e:
        logger.error("‚ùå Erreur authentification OpenAI - v√©rifiez votre cl√© API")
        raise RuntimeError(f"Authentification OpenAI √©chou√©e: {e}")
        
    except openai.PermissionDeniedError as e:
        logger.error("‚ùå Permission refus√©e OpenAI - v√©rifiez vos droits d'acc√®s")
        raise RuntimeError(f"Permission OpenAI refus√©e: {e}")
        
    except openai.BadRequestError as e:
        logger.error(f"‚ùå Requ√™te OpenAI invalide: {e}")
        raise RuntimeError(f"Requ√™te OpenAI invalide: {e}")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur inattendue OpenAI Chat: {type(e).__name__}: {e}")
        raise RuntimeError(f"Erreur lors de l'appel √† OpenAI ChatCompletion: {e}")

# ==================== FONCTION AM√âLIOR√âE: safe_embedding_create ====================
@openai_retry(max_retries=2, delay=0.5)
def safe_embedding_create(input: Any, model: str = "text-embedding-ada-002", **kwargs) -> List[List[float]]:
    """
    Wrapper s√©curis√© pour openai.embeddings.create
    
    AM√âLIORATIONS APPLIQU√âES:
    - Utilisation de _get_api_key() centralis√©e (plus de duplication)
    - Retry automatique pour erreurs temporaires
    - Validation et normalisation des inputs
    - Gestion d'erreurs sp√©cialis√©e
    - Support des embeddings batch
    - Format de retour standardis√©
    """
    
    # ‚úÖ AM√âLIORATION: Validation et normalisation des inputs
    if not input:
        raise ValueError("Le param√®tre 'input' ne peut pas √™tre vide")
    
    # Normaliser input en liste si n√©cessaire
    if isinstance(input, str):
        input_list = [input]
        single_input = True
    elif isinstance(input, list):
        input_list = input
        single_input = False
    else:
        raise ValueError("Le param√®tre 'input' doit √™tre une string ou une liste de strings")
    
    # Validation du contenu
    for i, text in enumerate(input_list):
        if not isinstance(text, str):
            raise ValueError(f"√âl√©ment {i} de input doit √™tre une string")
        if not text.strip():
            logger.warning(f"‚ö†Ô∏è √âl√©ment {i} de input est vide")
    
    # ‚úÖ AM√âLIORATION: Filtrer les textes vides
    filtered_input = [text.strip() for text in input_list if text.strip()]
    if not filtered_input:
        raise ValueError("Aucun texte valide apr√®s filtrage")
    
    # ‚úÖ AM√âLIORATION: Configuration avec mod√®le par d√©faut
    if not model:
        model = os.getenv('OPENAI_EMBEDDING_MODEL', 'text-embedding-ada-002')
    
    logger.debug(f"üî§ Appel OpenAI Embeddings: model={model}, inputs={len(filtered_input)}")
    
    try:
        # ‚úÖ AM√âLIORATION: Configuration centralis√©e
        _configure_openai_client()
        
        # ‚úÖ AM√âLIORATION: Gestion des grandes listes (batch processing)
        max_batch_size = int(os.getenv('OPENAI_EMBEDDING_BATCH_SIZE', '100'))
        all_embeddings = []
        
        for i in range(0, len(filtered_input), max_batch_size):
            batch = filtered_input[i:i + max_batch_size]
            
            start_time = time.time()
            response = openai.embeddings.create(
                input=batch,
                model=model,
                **kwargs
            )
            elapsed_time = time.time() - start_time
            
            logger.debug(f"‚úÖ Batch embeddings {i//max_batch_size + 1} trait√© en {elapsed_time:.2f}s")
            
            # ‚úÖ AM√âLIORATION: Extraction robuste des embeddings avec compatibilit√©
            if hasattr(response, 'data') and response.data:
                batch_embeddings = [item.embedding for item in response.data]
            elif isinstance(response, dict) and 'data' in response:
                batch_embeddings = [
                    item.get('embedding') if isinstance(item, dict) else item.embedding 
                    for item in response['data']
                ]
            else:
                raise RuntimeError("Format de r√©ponse OpenAI Embeddings non reconnu")
            
            all_embeddings.extend(batch_embeddings)
        
        # ‚úÖ AM√âLIORATION: Validation des embeddings retourn√©s
        if len(all_embeddings) != len(filtered_input):
            raise RuntimeError(f"Nombre d'embeddings ({len(all_embeddings)}) "
                             f"ne correspond pas aux inputs ({len(filtered_input)})")
        
        # V√©rification de la dimension des embeddings
        if all_embeddings and all_embeddings[0]:
            embedding_dim = len(all_embeddings[0])
            logger.debug(f"üìä Embeddings g√©n√©r√©s: {len(all_embeddings)} vecteurs de dimension {embedding_dim}")
        
        # ‚úÖ AM√âLIORATION: Retour adapt√© au format d'entr√©e
        if single_input:
            return all_embeddings[0] if all_embeddings else []
        else:
            return all_embeddings
        
    except openai.AuthenticationError as e:
        logger.error("‚ùå Erreur authentification OpenAI Embeddings")
        raise RuntimeError(f"Authentification OpenAI √©chou√©e: {e}")
        
    except openai.InvalidRequestError as e:
        logger.error(f"‚ùå Requ√™te OpenAI Embeddings invalide: {e}")
        raise RuntimeError(f"Requ√™te OpenAI Embeddings invalide: {e}")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur inattendue OpenAI Embeddings: {type(e).__name__}: {e}")
        raise RuntimeError(f"Erreur lors de l'appel √† OpenAI Embedding: {e}")

# ==================== NOUVELLES FONCTIONNALIT√âS UTILITAIRES ====================
def test_openai_connection() -> Dict[str, Any]:
    """
    ‚úÖ NOUVELLE FONCTIONNALIT√â: Test de connexion OpenAI
    Utile pour les diagnostics et la validation de configuration
    """
    try:
        logger.info("üîß Test de connexion OpenAI...")
        
        # Test simple avec un prompt minimal
        response = safe_chat_completion(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Test"}],
            max_tokens=5,
            temperature=0
        )
        
        return {
            "status": "success",
            "message": "Connexion OpenAI fonctionnelle",
            "model_tested": "gpt-3.5-turbo",
            "response_preview": response.choices[0].message.content[:50] if response.choices else "N/A"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"√âchec connexion OpenAI: {str(e)}",
            "error_type": type(e).__name__
        }

def get_openai_models() -> List[str]:
    """
    ‚úÖ NOUVELLE FONCTIONNALIT√â: Liste des mod√®les OpenAI disponibles
    """
    try:
        _configure_openai_client()
        models = openai.models.list()
        return [model.id for model in models.data if model.id]
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration mod√®les: {e}")
        return []

def estimate_tokens(text: str, model: str = "gpt-4") -> int:
    """
    ‚úÖ NOUVELLE FONCTIONNALIT√â: Estimation approximative du nombre de tokens
    Utile pour √©viter les d√©passements de limites
    """
    # Estimation grossi√®re : ~4 caract√®res par token pour l'anglais/fran√ßais
    # Plus pr√©cis avec tiktoken si disponible
    try:
        import tiktoken
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except ImportError:
        # Fallback vers estimation approximative
        return len(text) // 4
    except Exception:
        # Fallback s√©curis√©
        return len(text) // 4

# ==================== CONFIGURATION ET CONSTANTES ====================
# ‚úÖ AM√âLIORATION: Constantes configurables
DEFAULT_MODELS = {
    "chat": os.getenv('OPENAI_DEFAULT_CHAT_MODEL', 'gpt-4o'),
    "embedding": os.getenv('OPENAI_DEFAULT_EMBEDDING_MODEL', 'text-embedding-ada-002')
}

MAX_TOKENS_LIMITS = {
    "gpt-3.5-turbo": 4096,
    "gpt-4": 8192,
    "gpt-4o": 4096,
    "gpt-4-turbo": 128000
}

def get_model_max_tokens(model: str) -> int:
    """
    ‚úÖ NOUVELLE FONCTIONNALIT√â: R√©cup√®re la limite de tokens pour un mod√®le
    """
    return MAX_TOKENS_LIMITS.get(model, 4096)

# ==================== LOGGING ET DIAGNOSTICS ====================
def get_openai_status() -> Dict[str, Any]:
    """
    ‚úÖ NOUVELLE FONCTIONNALIT√â: Status complet du syst√®me OpenAI
    """
    return {
        "api_key_configured": bool(os.getenv("OPENAI_API_KEY")),
        "default_models": DEFAULT_MODELS,
        "max_tokens_limits": MAX_TOKENS_LIMITS,
        "retry_config": {
            "max_retries": 2,
            "base_delay": 1.0
        },
        "batch_config": {
            "embedding_batch_size": os.getenv('OPENAI_EMBEDDING_BATCH_SIZE', '100')
        }
    }