import openai
import os
import logging
from typing import Any, Dict, List, Optional
from functools import wraps
import time

# Configuration du logging
logger = logging.getLogger(__name__)

# ==================== AM√âLIORATION MAJEURE: Gestion centralis√©e de l'API key ====================
def _get_api_key() -> str:
    """
    ‚úÖ AM√âLIORATION: Fonction centralis√©e pour la gestion de la cl√© API
    
    PROBL√àME R√âSOLU:
    - Code dupliqu√© dans safe_chat_completion et safe_embedding_create
    - V√©rification r√©p√©t√©e de OPENAI_API_KEY
    
    SOLUTION:
    - Fonction unique pour r√©cup√©rer et valider la cl√©
    - Gestion d'erreurs centralis√©e
    - Configuration flexible via variables d'environnement
    """
    api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        logger.error("‚ùå OPENAI_API_KEY non configur√©e")
        raise RuntimeError(
            "OPENAI_API_KEY n'est pas configur√©e dans les variables d'environnement. "
            "Veuillez d√©finir cette variable pour utiliser les services OpenAI."
        )
    
    if len(api_key.strip()) < 10:
        logger.error("‚ùå OPENAI_API_KEY semble invalide (trop courte)")
        raise RuntimeError("OPENAI_API_KEY semble invalide - v√©rifiez la configuration")
    
    return api_key.strip()

def _configure_openai_client() -> None:
    """
    ‚úÖ AM√âLIORATION: Configuration centralis√©e du client OpenAI
    """
    try:
        api_key = _get_api_key()
        openai.api_key = api_key
        logger.debug("‚úÖ Client OpenAI configur√©")
    except Exception as e:
        logger.error(f"‚ùå Erreur configuration OpenAI: {e}")
        raise

# ==================== AM√âLIORATION: D√©corateur pour retry et gestion d'erreurs ====================
def openai_retry(max_retries: int = 2, delay: float = 1.0):
    """
    ‚úÖ NOUVEAU: D√©corateur pour retry automatique des appels OpenAI
    
    G√®re les erreurs temporaires comme:
    - Rate limiting
    - Erreurs r√©seau temporaires
    - Timeouts
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                    
                except openai.RateLimitError as e:
                    logger.warning(f"‚ö†Ô∏è Rate limit atteint (tentative {attempt + 1}/{max_retries + 1}): {e}")
                    if attempt < max_retries:
                        time.sleep(delay * (2 ** attempt))  # Backoff exponentiel
                        continue
                    last_exception = e
                    
                except openai.APITimeoutError as e:
                    logger.warning(f"‚ö†Ô∏è Timeout OpenAI (tentative {attempt + 1}/{max_retries + 1}): {e}")
                    if attempt < max_retries:
                        time.sleep(delay)
                        continue
                    last_exception = e
                    
                except openai.APIConnectionError as e:
                    logger.warning(f"‚ö†Ô∏è Erreur connexion OpenAI (tentative {attempt + 1}/{max_retries + 1}): {e}")
                    if attempt < max_retries:
                        time.sleep(delay)
                        continue
                    last_exception = e
                    
                except Exception as e:
                    # Pour les autres erreurs, pas de retry
                    logger.error(f"‚ùå Erreur OpenAI non-retry: {type(e).__name__}: {e}")
                    last_exception = e
                    break
            
            # Si on arrive ici, tous les retries ont √©chou√©
            raise RuntimeError(f"√âchec apr√®s {max_retries + 1} tentatives: {last_exception}")
        
        return wrapper
    return decorator

# ==================== üÜï NOUVELLE FONCTION: complete() pour dialogue_manager ====================
@openai_retry(max_retries=2, delay=1.0)
def complete(prompt: str, temperature: float = 0.2, max_tokens: Optional[int] = None, model: Optional[str] = None) -> str:
    """
    üÜï NOUVEAU: Wrapper simple pour completion de texte utilis√© par dialogue_manager
    
    Fonction standardis√©e pour la synth√®se et am√©lioration de texte dans le pipeline RAG.
    
    Args:
        prompt: Prompt/question √† envoyer au mod√®le
        temperature: Contr√¥le de la cr√©ativit√© (0.0 = d√©terministe, 1.0 = cr√©atif)
        max_tokens: Limite de tokens pour la r√©ponse (None = utilise d√©faut adaptatif)
        model: Mod√®le √† utiliser (None = utilise d√©faut)
    
    Returns:
        str: R√©ponse g√©n√©r√©e par le mod√®le
    
    Raises:
        RuntimeError: En cas d'erreur OpenAI
        ValueError: Si les param√®tres sont invalides
    """
    
    # ‚úÖ Validation des param√®tres
    if not prompt or not prompt.strip():
        raise ValueError("Le prompt ne peut pas √™tre vide")
    
    if not 0.0 <= temperature <= 2.0:
        logger.warning(f"‚ö†Ô∏è Temperature {temperature} hors limite [0.0-2.0], ajust√© √† 0.2")
        temperature = 0.2
    
    # ‚úÖ Configuration intelligente du mod√®le
    if model is None:
        model = os.getenv('OPENAI_SYNTHESIS_MODEL', os.getenv('OPENAI_DEFAULT_MODEL', 'gpt-4o'))
    
    # ‚úÖ Calcul adaptatif de max_tokens
    if max_tokens is None:
        # Estimation bas√©e sur la longueur du prompt
        prompt_length = len(prompt)
        if prompt_length < 500:
            max_tokens = 300
        elif prompt_length < 1500:
            max_tokens = 500
        else:
            max_tokens = 800
        
        # V√©rification des limites du mod√®le
        model_limit = get_model_max_tokens(model)
        estimated_prompt_tokens = estimate_tokens(prompt, model)
        available_tokens = model_limit - estimated_prompt_tokens - 100  # Marge de s√©curit√©
        
        if available_tokens > 0:
            max_tokens = min(max_tokens, available_tokens)
        
        logger.debug(f"üéØ max_tokens calcul√©: {max_tokens} (model_limit: {model_limit}, prompt_tokens: ~{estimated_prompt_tokens})")
    
    # ‚úÖ Construction du message optimis√© pour la synth√®se
    messages = [
        {
            "role": "system",
            "content": (
                "Tu es un expert en synth√®se de contenu technique avicole. "
                "R√©ponds de mani√®re concise, pr√©cise et professionnelle. "
                "Utilise un fran√ßais clair et structure ton texte avec du Markdown si appropri√©."
            )
        },
        {
            "role": "user", 
            "content": prompt.strip()
        }
    ]
    
    logger.debug(f"ü§ñ Appel complete(): model={model}, temp={temperature}, max_tokens={max_tokens}")
    
    try:
        # ‚úÖ Appel via safe_chat_completion avec configuration optimis√©e
        response = safe_chat_completion(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=0.9,  # Am√©liore la coh√©rence
            frequency_penalty=0.1,  # R√©duit les r√©p√©titions
            presence_penalty=0.1   # Encourage la diversit√©
        )
        
        # ‚úÖ Extraction robuste du contenu
        if not response or not response.choices:
            raise RuntimeError("R√©ponse OpenAI vide")
        
        content = response.choices[0].message.content
        if not content:
            raise RuntimeError("Contenu de r√©ponse vide")
        
        # ‚úÖ Post-traitement de la r√©ponse
        content = content.strip()
        
        # Logging des m√©triques pour monitoring
        if hasattr(response, 'usage') and response.usage:
            logger.debug(f"üìä Complete tokens: {response.usage.total_tokens} "
                        f"(prompt: {response.usage.prompt_tokens}, completion: {response.usage.completion_tokens})")
        
        logger.debug(f"‚úÖ Complete termin√©: {len(content)} caract√®res g√©n√©r√©s")
        return content
        
    except Exception as e:
        logger.error(f"‚ùå Erreur dans complete(): {type(e).__name__}: {e}")
        # Re-raise avec contexte pour le dialogue_manager
        raise RuntimeError(f"Erreur lors de la synth√®se LLM: {e}")

# ==================== FONCTION AM√âLIOR√âE: safe_chat_completion ====================
@openai_retry(max_retries=2, delay=1.0)
def safe_chat_completion(**kwargs) -> Any:
    """
    Wrapper s√©curis√© pour openai.chat.completions.create
    
    AM√âLIORATIONS APPLIQU√âES:
    - Utilisation de _get_api_key() centralis√©e (plus de duplication)
    - Retry automatique avec backoff exponentiel
    - Gestion d'erreurs sp√©cialis√©e par type
    - Validation des param√®tres d'entr√©e
    - Logging d√©taill√© pour debug
    """
    
    # ‚úÖ AM√âLIORATION: Validation des param√®tres essentiels
    if 'model' not in kwargs:
        kwargs['model'] = os.getenv('OPENAI_DEFAULT_MODEL', 'gpt-4o')
        logger.debug(f"üîß Mod√®le par d√©faut utilis√©: {kwargs['model']}")
    
    if 'messages' not in kwargs or not kwargs['messages']:
        raise ValueError("Le param√®tre 'messages' est requis et ne peut pas √™tre vide")
    
    # ‚úÖ AM√âLIORATION: Configuration avec param√®tres par d√©faut intelligents
    default_params = {
        'temperature': float(os.getenv('OPENAI_DEFAULT_TEMPERATURE', '0.7')),
        'max_tokens': int(os.getenv('OPENAI_DEFAULT_MAX_TOKENS', '500')),
        'timeout': int(os.getenv('OPENAI_DEFAULT_TIMEOUT', '30'))
    }
    
    # Appliquer les d√©fauts seulement si non sp√©cifi√©s
    for key, value in default_params.items():
        if key not in kwargs:
            kwargs[key] = value
    
    logger.debug(f"ü§ñ Appel OpenAI Chat: model={kwargs.get('model')}, temp={kwargs.get('temperature')}")
    
    try:
        # ‚úÖ AM√âLIORATION: Configuration centralis√©e
        _configure_openai_client()
        
        # ‚úÖ AM√âLIORATION: Mesure du temps de r√©ponse
        start_time = time.time()
        response = openai.chat.completions.create(**kwargs)
        elapsed_time = time.time() - start_time
        
        logger.debug(f"‚úÖ R√©ponse OpenAI Chat re√ßue en {elapsed_time:.2f}s")
        
        # ‚úÖ AM√âLIORATION: Validation de la r√©ponse
        if not response or not response.choices:
            raise RuntimeError("R√©ponse OpenAI vide ou malform√©e")
        
        # ‚úÖ AM√âLIORATION: Logging des m√©triques d'usage
        if hasattr(response, 'usage') and response.usage:
            logger.debug(f"üìä Tokens utilis√©s: {response.usage.total_tokens} "
                        f"(prompt: {response.usage.prompt_tokens}, "
                        f"completion: {response.usage.completion_tokens})")
        
        return response
        
    except openai.AuthenticationError as e:
        logger.error("‚ùå Erreur authentification OpenAI - v√©rifiez votre cl√© API")
        raise RuntimeError(f"Authentification OpenAI √©chou√©e: {e}")
        
    except openai.PermissionDeniedError as e:
        logger.error("‚ùå Permission refus√©e OpenAI - v√©rifiez vos droits d'acc√®s")
        raise RuntimeError(f"Permission OpenAI refus√©e: {e}")
        
    except openai.BadRequestError as e:
        logger.error(f"‚ùå Requ√™te OpenAI invalide: {e}")
        raise RuntimeError(f"Requ√™te OpenAI invalide: {e}")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur inattendue OpenAI Chat: {type(e).__name__}: {e}")
        raise RuntimeError(f"Erreur lors de l'appel √† OpenAI ChatCompletion: {e}")

# ==================== FONCTION AM√âLIOR√âE: safe_embedding_create ====================
@openai_retry(max_retries=2, delay=0.5)
def safe_embedding_create(input: Any, model: str = "text-embedding-ada-002", **kwargs) -> List[List[float]]:
    """
    Wrapper s√©curis√© pour openai.embeddings.create
    
    AM√âLIORATIONS APPLIQU√âES:
    - Utilisation de _get_api_key() centralis√©e (plus de duplication)
    - Retry automatique pour erreurs temporaires
    - Validation et normalisation des inputs
    - Gestion d'erreurs sp√©cialis√©e
    - Support des embeddings batch
    - Format de retour standardis√©
    """
    
    # ‚úÖ AM√âLIORATION: Validation et normalisation des inputs
    if not input:
        raise ValueError("Le param√®tre 'input' ne peut pas √™tre vide")
    
    # Normaliser input en liste si n√©cessaire
    if isinstance(input, str):
        input_list = [input]
        single_input = True
    elif isinstance(input, list):
        input_list = input
        single_input = False
    else:
        raise ValueError("Le param√®tre 'input' doit √™tre une string ou une liste de strings")
    
    # Validation du contenu
    for i, text in enumerate(input_list):
        if not isinstance(text, str):
            raise ValueError(f"√âl√©ment {i} de input doit √™tre une string")
        if not text.strip():
            logger.warning(f"‚ö†Ô∏è √âl√©ment {i} de input est vide")
    
    # ‚úÖ AM√âLIORATION: Filtrer les textes vides
    filtered_input = [text.strip() for text in input_list if text.strip()]
    if not filtered_input:
        raise ValueError("Aucun texte valide apr√®s filtrage")
    
    # ‚úÖ AM√âLIORATION: Configuration avec mod√®le par d√©faut
    if not model:
        model = os.getenv('OPENAI_EMBEDDING_MODEL', 'text-embedding-ada-002')
    
    logger.debug(f"üî§ Appel OpenAI Embeddings: model={model}, inputs={len(filtered_input)}")
    
    try:
        # ‚úÖ AM√âLIORATION: Configuration centralis√©e
        _configure_openai_client()
        
        # ‚úÖ AM√âLIORATION: Gestion des grandes listes (batch processing)
        max_batch_size = int(os.getenv('OPENAI_EMBEDDING_BATCH_SIZE', '100'))
        all_embeddings = []
        
        for i in range(0, len(filtered_input), max_batch_size):
            batch = filtered_input[i:i + max_batch_size]
            
            start_time = time.time()
            response = openai.embeddings.create(
                input=batch,
                model=model,
                **kwargs
            )
            elapsed_time = time.time() - start_time
            
            logger.debug(f"‚úÖ Batch embeddings {i//max_batch_size + 1} trait√© en {elapsed_time:.2f}s")
            
            # ‚úÖ AM√âLIORATION: Extraction robuste des embeddings avec compatibilit√©
            if hasattr(response, 'data') and response.data:
                batch_embeddings = [item.embedding for item in response.data]
            elif isinstance(response, dict) and 'data' in response:
                batch_embeddings = [
                    item.get('embedding') if isinstance(item, dict) else item.embedding 
                    for item in response['data']
                ]
            else:
                raise RuntimeError("Format de r√©ponse OpenAI Embeddings non reconnu")
            
            all_embeddings.extend(batch_embeddings)
        
        # ‚úÖ AM√âLIORATION: Validation des embeddings retourn√©s
        if len(all_embeddings) != len(filtered_input):
            raise RuntimeError(f"Nombre d'embeddings ({len(all_embeddings)}) "
                             f"ne correspond pas aux inputs ({len(filtered_input)})")
        
        # V√©rification de la dimension des embeddings
        if all_embeddings and all_embeddings[0]:
            embedding_dim = len(all_embeddings[0])
            logger.debug(f"üìä Embeddings g√©n√©r√©s: {len(all_embeddings)} vecteurs de dimension {embedding_dim}")
        
        # ‚úÖ AM√âLIORATION: Retour adapt√© au format d'entr√©e
        if single_input:
            return all_embeddings[0] if all_embeddings else []
        else:
            return all_embeddings
        
    except openai.AuthenticationError as e:
        logger.error("‚ùå Erreur authentification OpenAI Embeddings")
        raise RuntimeError(f"Authentification OpenAI √©chou√©e: {e}")
        
    except openai.InvalidRequestError as e:
        logger.error(f"‚ùå Requ√™te OpenAI Embeddings invalide: {e}")
        raise RuntimeError(f"Requ√™te OpenAI Embeddings invalide: {e}")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur inattendue OpenAI Embeddings: {type(e).__name__}: {e}")
        raise RuntimeError(f"Erreur lors de l'appel √† OpenAI Embedding: {e}")

# ==================== üÜï NOUVELLES FONCTIONS POUR DIALOGUE_MANAGER ====================

def synthesize_rag_content(question: str, raw_content: str, max_length: int = 300) -> str:
    """
    üÜï NOUVEAU: Synth√®se sp√©cialis√©e pour le contenu RAG du dialogue_manager
    
    Optimis√©e pour nettoyer et reformater le contenu brut des PDFs avicoles.
    
    Args:
        question: Question originale de l'utilisateur
        raw_content: Contenu brut √† synth√©tiser
        max_length: Longueur approximative souhait√©e pour la r√©ponse
    
    Returns:
        str: Contenu synth√©tis√© et nettoy√©
    """
    
    if not raw_content or not raw_content.strip():
        return "Informations techniques disponibles."
    
    # Prompt sp√©cialis√© pour le contenu avicole
    synthesis_prompt = f"""Synth√©tise ces informations techniques avicoles de mani√®re claire et professionnelle.

INSTRUCTIONS CRITIQUES :
- NE JAMAIS mentionner de sources, fichiers PDF ou r√©f√©rences dans ta r√©ponse
- NE JAMAIS inclure de fragments de tableaux mal format√©s
- Utiliser du Markdown pour la structure (##, **, -)
- R√©ponse concise (~{max_length} mots maximum)
- Si donn√©es incertaines, donner une fourchette
- Garder uniquement les informations pertinentes √† la question

Question utilisateur : {question}

Contenu technique √† synth√©tiser :
{raw_content[:1500]}

R√©ponse synth√©tique (format Markdown, sans sources) :"""

    try:
        return complete(
            prompt=synthesis_prompt, 
            temperature=0.2,  # Peu de cr√©ativit√© pour info technique
            max_tokens=min(400, max_length + 100)  # Marge pour le formatage
        )
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è √âchec synth√®se RAG, fallback: {e}")
        # Fallback simple : nettoyage basique
        cleaned = raw_content.strip()[:max_length]
        if len(raw_content) > max_length:
            cleaned += "..."
        return cleaned

def generate_clarification_response(intent: str, missing_fields: List[str], general_info: str = "") -> str:
    """
    üÜï NOUVEAU: G√©n√®re des r√©ponses de clarification intelligentes
    
    Args:
        intent: Intention d√©tect√©e (ex: "PerfTargets")
        missing_fields: Champs manquants d√©tect√©s
        general_info: Information g√©n√©rale disponible
    
    Returns:
        str: R√©ponse de clarification structur√©e
    """
    
    prompt = f"""G√©n√®re une r√©ponse de clarification courte et utile pour un syst√®me d'expertise avicole.

CONTEXTE :
- Intention d√©tect√©e : {intent}
- Informations manquantes : {', '.join(missing_fields)}
- Info g√©n√©rale disponible : {general_info[:200] if general_info else 'Aucune'}

INSTRUCTIONS :
- R√©ponse en 2-3 phrases maximum
- Expliquer pourquoi ces infos sont importantes
- Ton professionnel mais accessible
- Pas de mention de sources

R√©ponse de clarification :"""

    try:
        return complete(prompt=prompt, temperature=0.3, max_tokens=150)
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è √âchec g√©n√©ration clarification: {e}")
        # Fallback g√©n√©rique
        return f"Pour vous donner une r√©ponse pr√©cise sur {intent}, j'aurais besoin de quelques pr√©cisions suppl√©mentaires."

# ==================== NOUVELLES FONCTIONNALIT√âS UTILITAIRES ====================
def test_openai_connection() -> Dict[str, Any]:
    """
    ‚úÖ NOUVELLE FONCTIONNALIT√â: Test de connexion OpenAI
    Utile pour les diagnostics et la validation de configuration
    """
    try:
        logger.info("üîß Test de connexion OpenAI...")
        
        # Test simple avec un prompt minimal
        response = safe_chat_completion(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Test"}],
            max_tokens=5,
            temperature=0
        )
        
        return {
            "status": "success",
            "message": "Connexion OpenAI fonctionnelle",
            "model_tested": "gpt-3.5-turbo",
            "response_preview": response.choices[0].message.content[:50] if response.choices else "N/A"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"√âchec connexion OpenAI: {str(e)}",
            "error_type": type(e).__name__
        }

def get_openai_models() -> List[str]:
    """
    ‚úÖ NOUVELLE FONCTIONNALIT√â: Liste des mod√®les OpenAI disponibles
    """
    try:
        _configure_openai_client()
        models = openai.models.list()
        return [model.id for model in models.data if model.id]
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration mod√®les: {e}")
        return []

def estimate_tokens(text: str, model: str = "gpt-4") -> int:
    """
    ‚úÖ NOUVELLE FONCTIONNALIT√â: Estimation approximative du nombre de tokens
    Utile pour √©viter les d√©passements de limites
    """
    # Estimation grossi√®re : ~4 caract√®res par token pour l'anglais/fran√ßais
    # Plus pr√©cis avec tiktoken si disponible
    try:
        import tiktoken
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except ImportError:
        # Fallback vers estimation approximative
        return len(text) // 4
    except Exception:
        # Fallback s√©curis√©
        return len(text) // 4

# ==================== CONFIGURATION ET CONSTANTES ====================
# ‚úÖ AM√âLIORATION: Constantes configurables
DEFAULT_MODELS = {
    "chat": os.getenv('OPENAI_DEFAULT_CHAT_MODEL', 'gpt-4o'),
    "embedding": os.getenv('OPENAI_DEFAULT_EMBEDDING_MODEL', 'text-embedding-ada-002'),
    "synthesis": os.getenv('OPENAI_SYNTHESIS_MODEL', 'gpt-4o')  # üÜï Nouveau pour synth√®se
}

MAX_TOKENS_LIMITS = {
    "gpt-3.5-turbo": 4096,
    "gpt-4": 8192,
    "gpt-4o": 4096,
    "gpt-4-turbo": 128000,
    "gpt-4o-mini": 4096
}

def get_model_max_tokens(model: str) -> int:
    """
    ‚úÖ NOUVELLE FONCTIONNALIT√â: R√©cup√®re la limite de tokens pour un mod√®le
    """
    return MAX_TOKENS_LIMITS.get(model, 4096)

# ==================== LOGGING ET DIAGNOSTICS ====================
def get_openai_status() -> Dict[str, Any]:
    """
    ‚úÖ NOUVELLE FONCTIONNALIT√â: Status complet du syst√®me OpenAI
    """
    return {
        "api_key_configured": bool(os.getenv("OPENAI_API_KEY")),
        "default_models": DEFAULT_MODELS,
        "max_tokens_limits": MAX_TOKENS_LIMITS,
        "retry_config": {
            "max_retries": 2,
            "base_delay": 1.0
        },
        "batch_config": {
            "embedding_batch_size": os.getenv('OPENAI_EMBEDDING_BATCH_SIZE', '100')
        },
        "synthesis_config": {  # üÜï Nouveau pour dialogue_manager
            "synthesis_model": DEFAULT_MODELS["synthesis"],
            "default_temperature": 0.2,
            "max_synthesis_tokens": 500
        }
    }

def test_synthesis_pipeline() -> Dict[str, Any]:
    """
    üÜï NOUVEAU: Test complet du pipeline de synth√®se pour dialogue_manager
    """
    try:
        # Test de la fonction complete()
        test_response = complete(
            prompt="Test de synth√®se : r√©sume en une phrase que les poules pondent des ≈ìufs.",
            temperature=0.2,
            max_tokens=50
        )
        
        # Test de synth√®se RAG
        rag_test = synthesize_rag_content(
            question="Poids id√©al poule?",
            raw_content="Les poules Ross 308 atteignent un poids optimal de 2.2kg √† 42 jours selon les standards techniques...",
            max_length=100
        )
        
        return {
            "status": "success",
            "complete_test": {
                "success": True,
                "response": test_response[:100] + "..." if len(test_response) > 100 else test_response
            },
            "rag_synthesis_test": {
                "success": True,
                "response": rag_test[:100] + "..." if len(rag_test) > 100 else rag_test
            },
            "message": "Pipeline de synth√®se fonctionnel"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"√âchec test pipeline synth√®se: {str(e)}",
            "error_type": type(e).__name__
        }