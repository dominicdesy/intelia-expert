import openai
import os
import logging
import re
from typing import Any, Dict, List, Optional
from functools import wraps
import time

# Configuration du logging
logger = logging.getLogger(__name__)

# ==================== AM√âLIORATION MAJEURE: Gestion centralis√©e de l'API key ====================
def _get_api_key() -> str:
    """
    ‚úÖ AM√âLIORATION: Fonction centralis√©e pour la gestion de la cl√© API
    
    PROBL√àME R√âSOLU:
    - Code dupliqu√© dans safe_chat_completion et safe_embedding_create
    - V√©rification r√©p√©t√©e de OPENAI_API_KEY
    
    SOLUTION:
    - Fonction unique pour r√©cup√©rer et valider la cl√©
    - Gestion d'erreurs centralis√©e
    - Configuration flexible via variables d'environnement
    """
    api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        logger.error("‚ùå OPENAI_API_KEY non configur√©e")
        raise RuntimeError(
            "OPENAI_API_KEY n'est pas configur√©e dans les variables d'environnement. "
            "Veuillez d√©finir cette variable pour utiliser les services OpenAI."
        )
    
    if len(api_key.strip()) < 10:
        logger.error("‚ùå OPENAI_API_KEY semble invalide (trop courte)")
        raise RuntimeError("OPENAI_API_KEY semble invalide - v√©rifiez la configuration")
    
    return api_key.strip()

def _configure_openai_client() -> None:
    """
    ‚úÖ AM√âLIORATION: Configuration centralis√©e du client OpenAI
    """
    try:
        api_key = _get_api_key()
        openai.api_key = api_key
        logger.debug("‚úÖ Client OpenAI configur√©")
    except Exception as e:
        logger.error(f"‚ùå Erreur configuration OpenAI: {e}")
        raise

# ==================== AM√âLIORATION: D√©corateur pour retry et gestion d'erreurs ====================
def openai_retry(max_retries: int = 2, delay: float = 1.0):
    """
    ‚úÖ NOUVEAU: D√©corateur pour retry automatique des appels OpenAI
    
    G√®re les erreurs temporaires comme:
    - Rate limiting
    - Erreurs r√©seau temporaires
    - Timeouts
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                    
                except openai.RateLimitError as e:
                    logger.warning(f"‚ö†Ô∏è Rate limit atteint (tentative {attempt + 1}/{max_retries + 1}): {e}")
                    if attempt < max_retries:
                        time.sleep(delay * (2 ** attempt))  # Backoff exponentiel
                        continue
                    last_exception = e
                    
                except openai.APITimeoutError as e:
                    logger.warning(f"‚ö†Ô∏è Timeout OpenAI (tentative {attempt + 1}/{max_retries + 1}): {e}")
                    if attempt < max_retries:
                        time.sleep(delay)
                        continue
                    last_exception = e
                    
                except openai.APIConnectionError as e:
                    logger.warning(f"‚ö†Ô∏è Erreur connexion OpenAI (tentative {attempt + 1}/{max_retries + 1}): {e}")
                    if attempt < max_retries:
                        time.sleep(delay)
                        continue
                    last_exception = e
                    
                except Exception as e:
                    # Pour les autres erreurs, pas de retry
                    logger.error(f"‚ùå Erreur OpenAI non-retry: {type(e).__name__}: {e}")
                    last_exception = e
                    break
            
            # Si on arrive ici, tous les retries ont √©chou√©
            raise RuntimeError(f"√âchec apr√®s {max_retries + 1} tentatives: {last_exception}")
        
        return wrapper
    return decorator

# ==================== üÜï NOUVELLES FONCTIONS CHAIN-OF-THOUGHT ====================

@openai_retry(max_retries=2, delay=1.0)
def complete_with_cot(prompt: str, temperature: float = 0.3, max_tokens: Optional[int] = None, 
                     model: Optional[str] = None, parse_cot: bool = True) -> Dict[str, Any]:
    """
    üÜï NOUVEAU: Completion sp√©cialis√©e pour Chain-of-Thought avec parsing des balises
    
    Args:
        prompt: Prompt CoT avec balises XML structurantes
        temperature: Cr√©ativit√© du mod√®le (0.0-2.0)
        max_tokens: Limite de tokens (None = calcul automatique)
        model: Mod√®le √† utiliser (None = d√©faut CoT)
        parse_cot: Si True, parse les balises XML dans la r√©ponse
    
    Returns:
        Dict contenant 'raw_response', 'parsed_sections', 'final_answer'
    """
    
    # Validation des param√®tres
    if not prompt or not prompt.strip():
        raise ValueError("Le prompt CoT ne peut pas √™tre vide")
    
    if not 0.0 <= temperature <= 2.0:
        logger.warning(f"‚ö†Ô∏è Temperature {temperature} ajust√©e √† 0.3")
        temperature = 0.3
    
    # Configuration mod√®le CoT
    if model is None:
        model = os.getenv('OPENAI_COT_MODEL', os.getenv('OPENAI_DEFAULT_MODEL', 'gpt-4o'))
    
    # Calcul tokens adaptatif pour CoT (plus g√©n√©reux)
    if max_tokens is None:
        prompt_length = len(prompt)
        if prompt_length < 1000:
            max_tokens = 600
        elif prompt_length < 2000:
            max_tokens = 800
        else:
            max_tokens = 1000
        
        # Ajustement selon limites mod√®le
        model_limit = get_model_max_tokens(model)
        estimated_prompt_tokens = estimate_tokens(prompt, model)
        available_tokens = model_limit - estimated_prompt_tokens - 150  # Marge CoT
        
        if available_tokens > 0:
            max_tokens = min(max_tokens, available_tokens)
    
    logger.debug(f"üß† Appel CoT: model={model}, temp={temperature}, max_tokens={max_tokens}")
    
    # Messages optimis√©s pour CoT
    messages = [
        {
            "role": "system",
            "content": (
                "Tu es un expert v√©t√©rinaire avicole avec une approche m√©thodologique rigoureuse. "
                "Suis pr√©cis√©ment la structure de raisonnement demand√©e avec les balises XML. "
                "Reste factuel, pr√©cis et professionnel dans ton analyse."
            )
        },
        {
            "role": "user",
            "content": prompt.strip()
        }
    ]
    
    try:
        response = safe_chat_completion(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=0.9,
            frequency_penalty=0.0,  # Pas de p√©nalit√© pour CoT structur√©
            presence_penalty=0.1
        )
        
        if not response or not response.choices:
            raise RuntimeError("R√©ponse OpenAI CoT vide")
        
        raw_content = response.choices[0].message.content
        if not raw_content:
            raise RuntimeError("Contenu CoT vide")
        
        result = {
            "raw_response": raw_content.strip(),
            "model_used": model,
            "temperature": temperature
        }
        
        # üÜï Parsing des sections CoT si demand√©
        if parse_cot:
            parsed_sections = _parse_cot_sections(raw_content)
            result["parsed_sections"] = parsed_sections
            
            # Extraction de la r√©ponse finale
            final_answer = _extract_final_answer(raw_content, parsed_sections)
            result["final_answer"] = final_answer
        
        # M√©triques
        if hasattr(response, 'usage') and response.usage:
            result["token_usage"] = {
                "total": response.usage.total_tokens,
                "prompt": response.usage.prompt_tokens,
                "completion": response.usage.completion_tokens
            }
        
        logger.debug(f"‚úÖ CoT termin√©: {len(raw_content)} caract√®res g√©n√©r√©s")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erreur CoT: {type(e).__name__}: {e}")
        raise RuntimeError(f"Erreur lors du raisonnement CoT: {e}")

def _parse_cot_sections(cot_response: str) -> Dict[str, str]:
    """
    üÜï Parse les sections d√©limit√©es par des balises XML dans une r√©ponse CoT
    """
    sections = {}
    
    # Balises CoT standards
    cot_tags = [
        "thinking", "analysis", "reasoning", "factors", "recommendations", 
        "validation", "problem_decomposition", "factor_analysis", 
        "interconnections", "solution_pathway", "risk_mitigation",
        "economic_context", "cost_benefit_breakdown", "scenario_analysis",
        "optimization_levers", "financial_recommendation", "current_analysis",
        "optimization_factors", "strategy", "impact_prediction"
    ]
    
    for tag in cot_tags:
        pattern = f"<{tag}>(.*?)</{tag}>"
        match = re.search(pattern, cot_response, re.DOTALL | re.IGNORECASE)
        if match:
            content = match.group(1).strip()
            sections[tag] = content
    
    return sections

def _extract_final_answer(raw_response: str, parsed_sections: Dict[str, str]) -> str:
    """
    üÜï Extrait la r√©ponse finale d'une r√©ponse CoT
    """
    # 1. Chercher apr√®s la derni√®re balise fermante
    last_tag_pattern = r"</[^>]+>\s*(.+)$"
    match = re.search(last_tag_pattern, raw_response, re.DOTALL | re.IGNORECASE)
    if match:
        final_part = match.group(1).strip()
        if len(final_part) > 50:  # Assez substantiel
            return final_part
    
    # 2. Utiliser la section "recommendations" si disponible
    if "recommendations" in parsed_sections:
        return parsed_sections["recommendations"]
    
    # 3. Utiliser "solution_pathway" ou "financial_recommendation"
    for key in ["solution_pathway", "financial_recommendation", "strategy"]:
        if key in parsed_sections:
            return parsed_sections[key]
    
    # 4. Fallback: derniers 300 caract√®res
    return raw_response[-300:].strip()

@openai_retry(max_retries=2, delay=1.0)
def generate_cot_followup(initial_question: str, cot_sections: Dict[str, str], 
                         entities: Dict[str, Any], missing_info: List[str] = None) -> str:
    """
    üÜï NOUVEAU: G√©n√®re une question de suivi intelligente bas√©e sur l'analyse CoT
    
    Args:
        initial_question: Question originale de l'utilisateur
        cot_sections: Sections pars√©es de l'analyse CoT pr√©c√©dente
        entities: Entit√©s extraites du contexte
        missing_info: Informations manquantes identifi√©es
    
    Returns:
        Question de suivi structur√©e
    """
    
    # Construction du contexte pour la g√©n√©ration
    context_parts = [
        f"QUESTION INITIALE: {initial_question}",
        f"CONTEXTE: {entities}",
    ]
    
    if missing_info:
        context_parts.append(f"INFORMATIONS MANQUANTES: {', '.join(missing_info)}")
    
    # R√©sum√© des analyses CoT disponibles
    available_analysis = []
    for section, content in cot_sections.items():
        if content and len(content) > 20:
            available_analysis.append(f"{section}: {content[:100]}...")
    
    if available_analysis:
        context_parts.append("ANALYSES PR√âC√âDENTES:")
        context_parts.extend(available_analysis[:3])  # Limite pour √©viter surcharge
    
    context = "\n".join(context_parts)
    
    prompt = f"""Bas√© sur l'analyse pr√©c√©dente, g√©n√®re une question de suivi pertinente pour approfondir le diagnostic ou la solution.

{context}

INSTRUCTIONS:
- Question courte et pr√©cise (1-2 phrases max)
- Focus sur les √©l√©ments les plus critiques identifi√©s
- Ton professionnel mais accessible
- Viser l'action pratique

Question de suivi appropri√©e:"""

    try:
        followup = complete(prompt=prompt, temperature=0.4, max_tokens=150)
        return followup.strip()
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è √âchec g√©n√©ration followup CoT: {e}")
        return f"Pour approfondir l'analyse de votre situation avec {entities.get('species', 'ces animaux')}, pourriez-vous pr√©ciser les √©l√©ments mentionn√©s ci-dessus ?"

# ==================== üÜï AM√âLIORATION FUNCTION: complete() ====================
@openai_retry(max_retries=2, delay=1.0)
def complete(prompt: str, temperature: float = 0.2, max_tokens: Optional[int] = None, model: Optional[str] = None) -> str:
    """
    üÜï AM√âLIOR√â: Wrapper simple pour completion de texte avec support CoT automatique
    
    NOUVEAUT√âS:
    - D√©tection automatique de prompts CoT (avec balises XML)
    - Routing intelligent vers complete_with_cot() si n√©cessaire
    - Meilleure gestion des tokens pour prompts longs
    """
    
    # Validation des param√®tres
    if not prompt or not prompt.strip():
        raise ValueError("Le prompt ne peut pas √™tre vide")
    
    if not 0.0 <= temperature <= 2.0:
        logger.warning(f"‚ö†Ô∏è Temperature {temperature} hors limite [0.0-2.0], ajust√© √† 0.2")
        temperature = 0.2
    
    # üÜï D√âTECTION AUTOMATIQUE DE COT
    cot_indicators = ["<thinking>", "<analysis>", "<reasoning>", "<factors>", "<recommendations>"]
    is_cot_prompt = any(indicator in prompt for indicator in cot_indicators)
    
    if is_cot_prompt:
        logger.debug("üß† Prompt CoT d√©tect√©, routing vers complete_with_cot()")
        try:
            cot_result = complete_with_cot(
                prompt=prompt, 
                temperature=temperature, 
                max_tokens=max_tokens, 
                model=model,
                parse_cot=True
            )
            # Retourner la r√©ponse finale pour compatibilit√©
            return cot_result.get("final_answer", cot_result.get("raw_response", ""))
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Fallback vers complete standard apr√®s √©chec CoT: {e}")
            # Continuer avec completion standard en fallback
    
    # Configuration standard
    if model is None:
        model = os.getenv('OPENAI_SYNTHESIS_MODEL', os.getenv('OPENAI_DEFAULT_MODEL', 'gpt-4o'))
    
    # Calcul adaptatif de max_tokens (am√©lior√©)
    if max_tokens is None:
        prompt_length = len(prompt)
        if prompt_length < 500:
            max_tokens = 300
        elif prompt_length < 1500:
            max_tokens = 500
        elif prompt_length < 3000:
            max_tokens = 700
        else:
            max_tokens = 800
        
        # V√©rification des limites du mod√®le
        model_limit = get_model_max_tokens(model)
        estimated_prompt_tokens = estimate_tokens(prompt, model)
        available_tokens = model_limit - estimated_prompt_tokens - 100
        
        if available_tokens > 0:
            max_tokens = min(max_tokens, available_tokens)
        
        logger.debug(f"üéØ max_tokens calcul√©: {max_tokens}")
    
    # Construction du message optimis√©
    messages = [
        {
            "role": "system",
            "content": (
                "Tu es un expert en synth√®se de contenu technique avicole. "
                "R√©ponds de mani√®re concise, pr√©cise et professionnelle. "
                "Utilise un fran√ßais clair et structure ton texte avec du Markdown si appropri√©."
            )
        },
        {
            "role": "user", 
            "content": prompt.strip()
        }
    ]
    
    logger.debug(f"ü§ñ Appel complete(): model={model}, temp={temperature}, max_tokens={max_tokens}")
    
    try:
        response = safe_chat_completion(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=0.9,
            frequency_penalty=0.1,
            presence_penalty=0.1
        )
        
        if not response or not response.choices:
            raise RuntimeError("R√©ponse OpenAI vide")
        
        content = response.choices[0].message.content
        if not content:
            raise RuntimeError("Contenu de r√©ponse vide")
        
        content = content.strip()
        
        # Logging des m√©triques
        if hasattr(response, 'usage') and response.usage:
            logger.debug(f"üìä Complete tokens: {response.usage.total_tokens}")
        
        logger.debug(f"‚úÖ Complete termin√©: {len(content)} caract√®res g√©n√©r√©s")
        return content
        
    except Exception as e:
        logger.error(f"‚ùå Erreur dans complete(): {type(e).__name__}: {e}")
        raise RuntimeError(f"Erreur lors de la synth√®se LLM: {e}")

# ==================== FONCTION AM√âLIOR√âE: safe_chat_completion ====================
@openai_retry(max_retries=2, delay=1.0)
def safe_chat_completion(**kwargs) -> Any:
    """
    Wrapper s√©curis√© pour openai.chat.completions.create
    
    AM√âLIORATIONS APPLIQU√âES:
    - Utilisation de _get_api_key() centralis√©e (plus de duplication)
    - Retry automatique avec backoff exponentiel
    - Gestion d'erreurs sp√©cialis√©e par type
    - Validation des param√®tres d'entr√©e
    - Logging d√©taill√© pour debug
    """
    
    # ‚úÖ AM√âLIORATION: Validation des param√®tres essentiels
    if 'model' not in kwargs:
        kwargs['model'] = os.getenv('OPENAI_DEFAULT_MODEL', 'gpt-4o')
        logger.debug(f"üîß Mod√®le par d√©faut utilis√©: {kwargs['model']}")
    
    if 'messages' not in kwargs or not kwargs['messages']:
        raise ValueError("Le param√®tre 'messages' est requis et ne peut pas √™tre vide")
    
    # ‚úÖ AM√âLIORATION: Configuration avec param√®tres par d√©faut intelligents
    default_params = {
        'temperature': float(os.getenv('OPENAI_DEFAULT_TEMPERATURE', '0.7')),
        'max_tokens': int(os.getenv('OPENAI_DEFAULT_MAX_TOKENS', '500')),
        'timeout': int(os.getenv('OPENAI_DEFAULT_TIMEOUT', '30'))
    }
    
    # Appliquer les d√©fauts seulement si non sp√©cifi√©s
    for key, value in default_params.items():
        if key not in kwargs:
            kwargs[key] = value
    
    logger.debug(f"ü§ñ Appel OpenAI Chat: model={kwargs.get('model')}, temp={kwargs.get('temperature')}")
    
    try:
        # ‚úÖ AM√âLIORATION: Configuration centralis√©e
        _configure_openai_client()
        
        # ‚úÖ AM√âLIORATION: Mesure du temps de r√©ponse
        start_time = time.time()
        response = openai.chat.completions.create(**kwargs)
        elapsed_time = time.time() - start_time
        
        logger.debug(f"‚úÖ R√©ponse OpenAI Chat re√ßue en {elapsed_time:.2f}s")
        
        # ‚úÖ AM√âLIORATION: Validation de la r√©ponse
        if not response or not response.choices:
            raise RuntimeError("R√©ponse OpenAI vide ou malform√©e")
        
        # ‚úÖ AM√âLIORATION: Logging des m√©triques d'usage
        if hasattr(response, 'usage') and response.usage:
            logger.debug(f"üìä Tokens utilis√©s: {response.usage.total_tokens}")
        
        return response
        
    except openai.AuthenticationError as e:
        logger.error("‚ùå Erreur authentification OpenAI - v√©rifiez votre cl√© API")
        raise RuntimeError(f"Authentification OpenAI √©chou√©e: {e}")
        
    except openai.PermissionDeniedError as e:
        logger.error("‚ùå Permission refus√©e OpenAI - v√©rifiez vos droits d'acc√®s")
        raise RuntimeError(f"Permission OpenAI refus√©e: {e}")
        
    except openai.BadRequestError as e:
        logger.error(f"‚ùå Requ√™te OpenAI invalide: {e}")
        raise RuntimeError(f"Requ√™te OpenAI invalide: {e}")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur inattendue OpenAI Chat: {type(e).__name__}: {e}")
        raise RuntimeError(f"Erreur lors de l'appel √† OpenAI ChatCompletion: {e}")

# ==================== FONCTION AM√âLIOR√âE: safe_embedding_create ====================
@openai_retry(max_retries=2, delay=0.5)
def safe_embedding_create(input: Any, model: str = "text-embedding-ada-002", **kwargs) -> List[List[float]]:
    """
    Wrapper s√©curis√© pour openai.embeddings.create
    
    AM√âLIORATIONS APPLIQU√âES:
    - Utilisation de _get_api_key() centralis√©e (plus de duplication)
    - Retry automatique pour erreurs temporaires
    - Validation et normalisation des inputs
    - Gestion d'erreurs sp√©cialis√©e
    - Support des embeddings batch
    - Format de retour standardis√©
    """
    
    # ‚úÖ AM√âLIORATION: Validation et normalisation des inputs
    if not input:
        raise ValueError("Le param√®tre 'input' ne peut pas √™tre vide")
    
    # Normaliser input en liste si n√©cessaire
    if isinstance(input, str):
        input_list = [input]
        single_input = True
    elif isinstance(input, list):
        input_list = input
        single_input = False
    else:
        raise ValueError("Le param√®tre 'input' doit √™tre une string ou une liste de strings")
    
    # Validation du contenu
    for i, text in enumerate(input_list):
        if not isinstance(text, str):
            raise ValueError(f"√âl√©ment {i} de input doit √™tre une string")
        if not text.strip():
            logger.warning(f"‚ö†Ô∏è √âl√©ment {i} de input est vide")
    
    # ‚úÖ AM√âLIORATION: Filtrer les textes vides
    filtered_input = [text.strip() for text in input_list if text.strip()]
    if not filtered_input:
        raise ValueError("Aucun texte valide apr√®s filtrage")
    
    # ‚úÖ AM√âLIORATION: Configuration avec mod√®le par d√©faut
    if not model:
        model = os.getenv('OPENAI_EMBEDDING_MODEL', 'text-embedding-ada-002')
    
    logger.debug(f"üì§ Appel OpenAI Embeddings: model={model}, inputs={len(filtered_input)}")
    
    try:
        # ‚úÖ AM√âLIORATION: Configuration centralis√©e
        _configure_openai_client()
        
        # ‚úÖ AM√âLIORATION: Gestion des grandes listes (batch processing)
        max_batch_size = int(os.getenv('OPENAI_EMBEDDING_BATCH_SIZE', '100'))
        all_embeddings = []
        
        for i in range(0, len(filtered_input), max_batch_size):
            batch = filtered_input[i:i + max_batch_size]
            
            start_time = time.time()
            response = openai.embeddings.create(
                input=batch,
                model=model,
                **kwargs
            )
            elapsed_time = time.time() - start_time
            
            logger.debug(f"‚úÖ Batch embeddings {i//max_batch_size + 1} trait√© en {elapsed_time:.2f}s")
            
            # ‚úÖ AM√âLIORATION: Extraction robuste des embeddings avec compatibilit√©
            if hasattr(response, 'data') and response.data:
                batch_embeddings = [item.embedding for item in response.data]
            elif isinstance(response, dict) and 'data' in response:
                batch_embeddings = [
                    item.get('embedding') if isinstance(item, dict) else item.embedding 
                    for item in response['data']
                ]
            else:
                raise RuntimeError("Format de r√©ponse OpenAI Embeddings non reconnu")
            
            all_embeddings.extend(batch_embeddings)
        
        # ‚úÖ AM√âLIORATION: Validation des embeddings retourn√©s
        if len(all_embeddings) != len(filtered_input):
            raise RuntimeError(f"Nombre d'embeddings ({len(all_embeddings)}) "
                             f"ne correspond pas aux inputs ({len(filtered_input)})")
        
        # V√©rification de la dimension des embeddings
        if all_embeddings and all_embeddings[0]:
            embedding_dim = len(all_embeddings[0])
            logger.debug(f"üìä Embeddings g√©n√©r√©s: {len(all_embeddings)} vecteurs de dimension {embedding_dim}")
        
        # ‚úÖ AM√âLIORATION: Retour adapt√© au format d'entr√©e
        if single_input:
            return all_embeddings[0] if all_embeddings else []
        else:
            return all_embeddings
        
    except openai.AuthenticationError as e:
        logger.error("‚ùå Erreur authentification OpenAI Embeddings")
        raise RuntimeError(f"Authentification OpenAI √©chou√©e: {e}")
        
    except openai.InvalidRequestError as e:
        logger.error(f"‚ùå Requ√™te OpenAI Embeddings invalide: {e}")
        raise RuntimeError(f"Requ√™te OpenAI Embeddings invalide: {e}")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur inattendue OpenAI Embeddings: {type(e).__name__}: {e}")
        raise RuntimeError(f"Erreur lors de l'appel √† OpenAI Embedding: {e}")

# ==================== üÜï NOUVELLES FONCTIONS POUR DIALOGUE_MANAGER ====================

def synthesize_rag_content(question: str, raw_content: str, max_length: int = 300) -> str:
    """
    üÜï NOUVEAU: Synth√®se sp√©cialis√©e pour le contenu RAG du dialogue_manager
    
    Optimis√©e pour nettoyer et reformater le contenu brut des PDFs avicoles.
    """
    
    if not raw_content or not raw_content.strip():
        return "Informations techniques disponibles."
    
    # Prompt sp√©cialis√© pour le contenu avicole
    synthesis_prompt = f"""Synth√©tise ces informations techniques avicoles de mani√®re claire et professionnelle.

INSTRUCTIONS CRITIQUES :
- NE JAMAIS mentionner de sources, fichiers PDF ou r√©f√©rences dans ta r√©ponse
- NE JAMAIS inclure de fragments de tableaux mal format√©s
- Utiliser du Markdown pour la structure (##, **, -)
- R√©ponse concise (~{max_length} mots maximum)
- Si donn√©es incertaines, donner une fourchette
- Garder uniquement les informations pertinentes √† la question

Question utilisateur : {question}

Contenu technique √† synth√©tiser :
{raw_content[:1500]}

R√©ponse synth√©tique (format Markdown, sans sources) :"""

    try:
        return complete(
            prompt=synthesis_prompt, 
            temperature=0.2,  # Peu de cr√©ativit√© pour info technique
            max_tokens=min(400, max_length + 100)  # Marge pour le formatage
        )
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è √âchec synth√®se RAG, fallback: {e}")
        # Fallback simple : nettoyage basique
        cleaned = raw_content.strip()[:max_length]
        if len(raw_content) > max_length:
            cleaned += "..."
        return cleaned

def generate_clarification_response(intent: str, missing_fields: List[str], general_info: str = "") -> str:
    """
    üÜï NOUVEAU: G√©n√®re des r√©ponses de clarification intelligentes
    """
    
    prompt = f"""G√©n√®re une r√©ponse de clarification courte et utile pour un syst√®me d'expertise avicole.

CONTEXTE :
- Intention d√©tect√©e : {intent}
- Informations manquantes : {', '.join(missing_fields)}
- Info g√©n√©rale disponible : {general_info[:200] if general_info else 'Aucune'}

INSTRUCTIONS :
- R√©ponse en 2-3 phrases maximum
- Expliquer pourquoi ces infos sont importantes
- Ton professionnel mais accessible
- Pas de mention de sources

R√©ponse de clarification :"""

    try:
        return complete(prompt=prompt, temperature=0.3, max_tokens=150)
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è √âchec g√©n√©ration clarification: {e}")
        # Fallback g√©n√©rique
        return f"Pour vous donner une r√©ponse pr√©cise sur {intent}, j'aurais besoin de quelques pr√©cisions suppl√©mentaires."

# ==================== NOUVELLES FONCTIONNALIT√âS UTILITAIRES ====================
def test_openai_connection() -> Dict[str, Any]:
    """
    ‚úÖ NOUVELLE FONCTIONNALIT√â: Test de connexion OpenAI
    Utile pour les diagnostics et la validation de configuration
    """
    try:
        logger.info("üîß Test de connexion OpenAI...")
        
        # Test simple avec un prompt minimal
        response = safe_chat_completion(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Test"}],
            max_tokens=5,
            temperature=0
        )
        
        return {
            "status": "success",
            "message": "Connexion OpenAI fonctionnelle",
            "model_tested": "gpt-3.5-turbo",
            "response_preview": response.choices[0].message.content[:50] if response.choices else "N/A"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"√âchec connexion OpenAI: {str(e)}",
            "error_type": type(e).__name__
        }

def get_openai_models() -> List[str]:
    """
    ‚úÖ NOUVELLE FONCTIONNALIT√â: Liste des mod√®les OpenAI disponibles
    """
    try:
        _configure_openai_client()
        models = openai.models.list()
        return [model.id for model in models.data if model.id]
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration mod√®les: {e}")
        return []

def estimate_tokens(text: str, model: str = "gpt-4") -> int:
    """
    ‚úÖ NOUVELLE FONCTIONNALIT√â: Estimation approximative du nombre de tokens
    Utile pour √©viter les d√©passements de limites
    """
    # Estimation grossi√®re : ~4 caract√®res par token pour l'anglais/fran√ßais
    # Plus pr√©cis avec tiktoken si disponible
    try:
        import tiktoken
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except ImportError:
        # Fallback vers estimation approximative
        return len(text) // 4
    except Exception:
        # Fallback s√©curis√©
        return len(text) // 4

def get_model_max_tokens(model: str) -> int:
    """
    ‚úÖ NOUVELLE FONCTIONNALIT√â: R√©cup√®re la limite de tokens pour un mod√®le
    """
    MAX_TOKENS_LIMITS = {
        "gpt-3.5-turbo": 4096,
        "gpt-4": 8192,
        "gpt-4o": 4096,
        "gpt-4-turbo": 128000,
        "gpt-4o-mini": 4096
    }
    return MAX_TOKENS_LIMITS.get(model, 4096)

# ==================== CONFIGURATION ET CONSTANTES ====================
# ‚úÖ AM√âLIORATION: Constantes configurables
DEFAULT_MODELS = {
    "chat": os.getenv('OPENAI_DEFAULT_CHAT_MODEL', 'gpt-4o'),
    "embedding": os.getenv('OPENAI_DEFAULT_EMBEDDING_MODEL', 'text-embedding-ada-002'),
    "synthesis": os.getenv('OPENAI_SYNTHESIS_MODEL', 'gpt-4o'),  # üÜï Nouveau pour synth√®se
    "cot": os.getenv('OPENAI_COT_MODEL', 'gpt-4o')  # üÜï Nouveau pour Chain-of-Thought
}

# ==================== üÜï FONCTIONS DE DIAGNOSTIC COT ====================

def test_cot_pipeline() -> Dict[str, Any]:
    """
    üÜï NOUVEAU: Test complet du pipeline Chain-of-Thought
    """
    try:
        # Test prompt CoT simple
        test_cot_prompt = """<thinking>
Analyse de la question : Il s'agit d'un test du syst√®me CoT.
</thinking>

<analysis>
Le syst√®me doit parser cette structure et extraire les sections.
</analysis>

<recommendations>
Le test CoT fonctionne correctement si ce texte est pars√©.
</recommendations>

R√©ponse finale : Test CoT r√©ussi."""

        # Test complete_with_cot
        cot_result = complete_with_cot(
            prompt=test_cot_prompt,
            temperature=0.2,
            max_tokens=200,
            parse_cot=True
        )
        
        # Test parsing
        sections_found = len(cot_result.get("parsed_sections", {}))
        has_final_answer = bool(cot_result.get("final_answer"))
        
        # Test complete() avec d√©tection automatique
        auto_cot_result = complete(
            prompt="<thinking>Test automatique</thinking>\n\nR√©ponse automatique CoT.",
            temperature=0.2,
            max_tokens=100
        )
        
        return {
            "status": "success",
            "cot_direct_test": {
                "success": True,
                "sections_parsed": sections_found,
                "final_answer_extracted": has_final_answer,
                "raw_length": len(cot_result.get("raw_response", ""))
            },
            "cot_auto_detection": {
                "success": True,
                "response_length": len(auto_cot_result)
            },
            "message": "Pipeline CoT enti√®rement fonctionnel"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"√âchec test pipeline CoT: {str(e)}",
            "error_type": type(e).__name__
        }

def test_synthesis_pipeline() -> Dict[str, Any]:
    """
    üÜï AM√âLIOR√â: Test complet du pipeline de synth√®se pour dialogue_manager
    """
    try:
        # Test de la fonction complete()
        test_response = complete(
            prompt="Test de synth√®se : r√©sume en une phrase que les poules pondent des ≈ìufs.",
            temperature=0.2,
            max_tokens=50
        )
        
        # Test de synth√®se RAG
        rag_test = synthesize_rag_content(
            question="Poids id√©al poule?",
            raw_content="Les poules Ross 308 atteignent un poids optimal de 2.2kg √† 42 jours selon les standards techniques...",
            max_length=100
        )
        
        # Test clarification
        clarification_test = generate_clarification_response(
            intent="PerfTargets",
            missing_fields=["age_days", "line"],
            general_info="Information sur les performances"
        )
        
        return {
            "status": "success",
            "complete_test": {
                "success": True,
                "response": test_response[:100] + "..." if len(test_response) > 100 else test_response
            },
            "rag_synthesis_test": {
                "success": True,
                "response": rag_test[:100] + "..." if len(rag_test) > 100 else rag_test
            },
            "clarification_test": {
                "success": True,
                "response": clarification_test[:100] + "..." if len(clarification_test) > 100 else clarification_test
            },
            "message": "Pipeline de synth√®se fonctionnel"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"√âchec test pipeline synth√®se: {str(e)}",
            "error_type": type(e).__name__
        }

# ==================== LOGGING ET DIAGNOSTICS ====================
def get_openai_status() -> Dict[str, Any]:
    """
    ‚úÖ AM√âLIOR√â: Status complet du syst√®me OpenAI avec support CoT
    """
    return {
        "api_key_configured": bool(os.getenv("OPENAI_API_KEY")),
        "default_models": DEFAULT_MODELS,
        "max_tokens_limits": {
            "gpt-3.5-turbo": 4096,
            "gpt-4": 8192,
            "gpt-4o": 4096,
            "gpt-4-turbo": 128000,
            "gpt-4o-mini": 4096
        },
        "retry_config": {
            "max_retries": 2,
            "base_delay": 1.0
        },
        "batch_config": {
            "embedding_batch_size": os.getenv('OPENAI_EMBEDDING_BATCH_SIZE', '100')
        },
        "synthesis_config": {  # üÜï Nouveau pour dialogue_manager
            "synthesis_model": DEFAULT_MODELS["synthesis"],
            "default_temperature": 0.2,
            "max_synthesis_tokens": 500
        },
        "cot_config": {  # üÜï NOUVEAU: Configuration CoT
            "cot_model": DEFAULT_MODELS["cot"],
            "auto_detection_enabled": True,
            "supported_tags": [
                "thinking", "analysis", "reasoning", "factors", "recommendations",
                "validation", "problem_decomposition", "solution_pathway"
            ],
            "max_cot_tokens": 1000
        }
    }

def get_cot_capabilities() -> Dict[str, Any]:
    """
    üÜï NOUVEAU: Retourne les capacit√©s Chain-of-Thought disponibles
    """
    return {
        "cot_available": True,
        "auto_detection": True,
        "parsing_enabled": True,
        "followup_generation": True,
        "supported_sections": [
            "thinking", "analysis", "reasoning", "factors", "recommendations",
            "validation", "problem_decomposition", "factor_analysis", 
            "interconnections", "solution_pathway", "risk_mitigation",
            "economic_context", "cost_benefit_breakdown", "scenario_analysis"
        ],
        "supported_intents": [
            "HealthDiagnosis", "OptimizationStrategy", "TroubleshootingMultiple",
            "ProductionAnalysis", "MultiFactor", "Economics"
        ],
        "models": {
            "preferred": DEFAULT_MODELS["cot"],
            "fallback": DEFAULT_MODELS["chat"]
        }
    }