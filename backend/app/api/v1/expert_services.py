"""
app/api/v1/expert_services.py - SERVICES M√âTIER EXPERT SYSTEM AVEC CONCISION + RESPONSE VERSIONS

üö® NOUVELLES FONCTIONNALIT√âS VERSION 3.7.0:
1. ‚úÖ Syst√®me de concision des r√©ponses int√©gr√© (CONSERV√â)
2. ‚úÖ Nettoyage avanc√© verbosit√© + r√©f√©rences documents (CONSERV√â)
3. ‚úÖ Configuration flexible par type de question (CONSERV√â)
4. ‚úÖ D√©tection automatique niveau de concision requis (CONSERV√â)
5. ‚úÖ Conservation de toutes les fonctionnalit√©s existantes (CONSERV√â)
6. üöÄ NOUVEAU: ResponseVersionsGenerator int√©gr√©
7. üöÄ NOUVEAU: G√©n√©ration de toutes les versions (ultra_concise, concise, standard, detailed)
8. üöÄ NOUVEAU: Support ConcisionMetrics avec m√©triques d√©taill√©es
9. üöÄ NOUVEAU: S√©lection automatique selon concision_level
10. üöÄ NOUVEAU: Support generate_all_versions flag pour frontend

FONCTIONNALIT√âS CONSERV√âES:
- ‚úÖ Syst√®me de clarification intelligent complet
- ‚úÖ M√©moire conversationnelle
- ‚úÖ RAG avec contexte enrichi
- ‚úÖ Multi-LLM support
- ‚úÖ Validation agricole
- ‚úÖ Support multilingue
"""

import os
import logging
import uuid
import time
import re
from datetime import datetime
from typing import Optional, Dict, Any, Tuple
from enum import Enum

from fastapi import HTTPException, Request

from .expert_models import (
    EnhancedQuestionRequest, EnhancedExpertResponse, FeedbackRequest,
    ValidationResult, ProcessingContext, VaguenessResponse, ResponseFormat,
    ConcisionLevel, ConcisionMetrics  # üöÄ NOUVEAU: Import ConcisionLevel et ConcisionMetrics
)
from .expert_utils import (
    get_user_id_from_request, 
    build_enriched_question_from_clarification,
    get_enhanced_topics_by_language,
    save_conversation_auto_enhanced,
    extract_breed_and_sex_from_clarification,
    build_enriched_question_with_breed_sex,
    validate_clarification_completeness
)
from .expert_integrations import IntegrationsManager
from .api_enhancement_service import APIEnhancementService
from .prompt_templates import build_structured_prompt, extract_context_from_entities, validate_prompt_context, build_clarification_prompt
from .concision_service import concision_service  # üöÄ NOUVEAU: Import concision service

logger = logging.getLogger(__name__)

# =============================================================================
# üÜï SYST√àME DE CONCISION DES R√âPONSES (CONSERV√â IDENTIQUE)
# =============================================================================

class ConcisionLevel(Enum):
    """Niveaux de concision disponibles"""
    ULTRA_CONCISE = "ultra_concise"    # R√©ponse minimale (ex: "410-450g")
    CONCISE = "concise"                # R√©ponse courte mais compl√®te
    STANDARD = "standard"              # R√©ponse normale sans conseils excessifs
    DETAILED = "detailed"              # R√©ponse compl√®te (mode actuel)

class ConcisionConfig:
    """Configuration du syst√®me de concision"""
    
    # Activer/d√©sactiver le syst√®me
    ENABLE_CONCISE_RESPONSES = True
    
    # Niveau par d√©faut
    DEFAULT_CONCISION_LEVEL = ConcisionLevel.CONCISE
    
    # Seuils de longueur par type de question
    MAX_RESPONSE_LENGTH = {
        "weight_question": 80,      # Questions de poids: max 80 caract√®res
        "temperature_question": 60, # Questions temp√©rature: max 60 caract√®res
        "measurement_question": 70, # Questions de mesure: max 70 caract√®res
        "general_question": 150,    # Questions g√©n√©rales: max 150 caract√®res
        "complex_question": 300     # Questions complexes: max 300 caract√®res
    }
    
    # Mots-cl√©s qui d√©clenchent le mode ultra-concis
    ULTRA_CONCISE_KEYWORDS = [
        "poids", "weight", "peso",
        "temp√©rature", "temperature", "temperatura",
        "combien", "how much", "cu√°nto",
        "quel est", "what is", "cu√°l es"
    ]
    
    # Mots-cl√©s pour questions complexes (mode d√©taill√©)
    COMPLEX_KEYWORDS = [
        "comment", "how to", "c√≥mo",
        "pourquoi", "why", "por qu√©", 
        "expliquer", "explain", "explicar",
        "proc√©dure", "procedure", "procedimiento",
        "protocole", "protocol", "protocolo"
    ]

class ResponseConcisionProcessor:
    """Processeur de concision des r√©ponses (CONSERV√â IDENTIQUE)"""
    
    def __init__(self):
        self.config = ConcisionConfig()
        logger.info("‚úÖ [Concision] Processeur de concision initialis√©")
    
    def detect_question_type(self, question: str) -> str:
        """D√©tecte le type de question pour appliquer les bonnes r√®gles"""
        
        question_lower = question.lower().strip()
        
        # Questions de poids/mesures = r√©ponses tr√®s courtes
        weight_keywords = ["poids", "weight", "peso", "grammes", "grams", "gramos", "kg"]
        if any(word in question_lower for word in weight_keywords):
            return "weight_question"
        
        # Questions de temp√©rature
        temp_keywords = ["temp√©rature", "temperature", "temperatura", "¬∞c", "degr√©", "degree"]
        if any(word in question_lower for word in temp_keywords):
            return "temperature_question"
        
        # Questions de mesure g√©n√©rale
        measurement_keywords = ["taille", "size", "tama√±o", "longueur", "length", "hauteur", "height"]
        if any(word in question_lower for word in measurement_keywords):
            return "measurement_question"
        
        # Questions complexes avec "comment", "pourquoi", etc.
        if any(word in question_lower for word in self.config.COMPLEX_KEYWORDS):
            return "complex_question"
        
        # Par d√©faut: question g√©n√©rale
        return "general_question"
    
    def detect_optimal_concision_level(self, question: str, user_preference: Optional[ConcisionLevel] = None) -> ConcisionLevel:
        """D√©tecte le niveau de concision optimal pour une question"""
        
        # Si l'utilisateur a une pr√©f√©rence explicite, l'utiliser
        if user_preference:
            return user_preference
        
        question_lower = question.lower().strip()
        
        # Questions ultra-concises (poids, temp√©rature, mesures simples)
        if any(keyword in question_lower for keyword in self.config.ULTRA_CONCISE_KEYWORDS):
            return ConcisionLevel.ULTRA_CONCISE
        
        # Questions complexes = r√©ponses d√©taill√©es
        if any(keyword in question_lower for keyword in self.config.COMPLEX_KEYWORDS):
            return ConcisionLevel.DETAILED
        
        # Par d√©faut: concis
        return self.config.DEFAULT_CONCISION_LEVEL
    
    def apply_concision(
        self, 
        response: str, 
        question: str, 
        concision_level: ConcisionLevel,
        language: str = "fr"
    ) -> str:
        """
        üöÄ NOUVEAU: M√©thode unified pour appliquer la concision
        Utilis√©e par ResponseVersionsGenerator
        """
        
        if not self.config.ENABLE_CONCISE_RESPONSES:
            return response
        
        logger.info(f"üéØ [Concision] Application niveau {concision_level.value}")
        
        # Appliquer le traitement selon le niveau
        if concision_level == ConcisionLevel.ULTRA_CONCISE:
            return self._extract_essential_info(response, question, language)
        elif concision_level == ConcisionLevel.CONCISE:
            return self._make_concise(response, question, language)
        elif concision_level == ConcisionLevel.STANDARD:
            return self._remove_excessive_advice(response, language)
        else:  # DETAILED
            return self._clean_document_references_only(response)
    
    def process_response(
        self, 
        response: str, 
        question: str, 
        concision_level: Optional[ConcisionLevel] = None,
        language: str = "fr"
    ) -> str:
        """Traite une r√©ponse selon le niveau de concision demand√© (M√âTHODE CONSERV√âE)"""
        
        if not self.config.ENABLE_CONCISE_RESPONSES:
            return response
        
        # D√©terminer le niveau de concision
        level = concision_level or self.detect_optimal_concision_level(question)
        
        return self.apply_concision(response, question, level, language)
    
    def _extract_essential_info(self, response: str, question: str, language: str = "fr") -> str:
        """Extrait uniquement l'information essentielle (mode ultra-concis)"""
        
        question_lower = question.lower()
        
        # Questions de poids ‚Üí extraire juste les chiffres + unit√©
        if any(word in question_lower for word in ["poids", "weight", "peso"]):
            # Chercher pattern "X grammes" ou "entre X et Y grammes"
            weight_patterns = [
                r'(?:entre\s+)?(\d+(?:-\d+|[^\d]*\d+)?)\s*(?:grammes?|g\b)',
                r'(\d+)\s*(?:√†|to|a)\s*(\d+)\s*(?:grammes?|g\b)',
                r'(\d+)\s*(?:grammes?|g\b)'
            ]
            
            for pattern in weight_patterns:
                match = re.search(pattern, response, re.IGNORECASE)
                if match:
                    if len(match.groups()) >= 2:  # Range
                        return f"{match.group(1)}-{match.group(2)}g"
                    else:  # Single value
                        value = match.group(1)
                        if "entre" in response.lower() or "-" in value:
                            return f"{value}g"
                        else:
                            return f"~{value}g"
        
        # Questions de temp√©rature ‚Üí extraire juste les degr√©s
        if any(word in question_lower for word in ["temp√©rature", "temperature"]):
            temp_patterns = [
                r'(\d+(?:-\d+)?)\s*(?:¬∞C|degr√©s?|degrees?)',
                r'(\d+)\s*(?:√†|to|a)\s*(\d+)\s*(?:¬∞C|degr√©s?)'
            ]
            
            for pattern in temp_patterns:
                match = re.search(pattern, response, re.IGNORECASE)
                if match:
                    if len(match.groups()) >= 2:
                        return f"{match.group(1)}-{match.group(2)}¬∞C"
                    else:
                        return f"{match.group(1)}¬∞C"
        
        # Fallback: premi√®re phrase avec chiffres
        sentences = response.split('.')
        for sentence in sentences:
            if re.search(r'\d+', sentence) and len(sentence.strip()) > 10:
                return sentence.strip() + '.'
        
        # Ultime fallback: premi√®re phrase tout court
        if sentences:
            return sentences[0].strip() + '.'
        
        return response
    
    def _make_concise(self, response: str, question: str, language: str = "fr") -> str:
        """Rend concis (enl√®ve conseils mais garde info principale)"""
        
        # D'abord nettoyer les r√©f√©rences aux documents
        cleaned = self._clean_document_references_only(response)
        
        # Ensuite supprimer la verbosit√© excessive
        verbose_patterns = [
            # Fran√ßais - Conseils g√©n√©raux non demand√©s
            r'\.?\s*Il est essentiel de[^.]*\.',
            r'\.?\s*Assurez-vous de[^.]*\.',
            r'\.?\s*N\'h√©sitez pas √†[^.]*\.',
            r'\.?\s*Pour garantir[^.]*\.',
            r'\.?\s*Il est important de[^.]*\.',
            r'\.?\s*Veillez √†[^.]*\.',
            r'\.?\s*Il convient de[^.]*\.',
            r'\.?\s*√Ä ce stade[^.]*\.',
            r'\.?\s*En cas de doute[^.]*\.',
            r'\.?\s*pour favoriser le bien-√™tre[^.]*\.',
            r'\.?\s*en termes de[^.]*\.',
            
            # Anglais - Conseils g√©n√©raux non demand√©s
            r'\.?\s*It is essential to[^.]*\.',
            r'\.?\s*Make sure to[^.]*\.',
            r'\.?\s*Don\'t hesitate to[^.]*\.',
            r'\.?\s*To ensure[^.]*\.',
            r'\.?\s*It is important to[^.]*\.',
            r'\.?\s*Be sure to[^.]*\.',
            r'\.?\s*At this stage[^.]*\.',
            
            # Espagnol - Consejos generales no solicitados
            r'\.?\s*Es esencial[^.]*\.',
            r'\.?\s*Aseg√∫rese de[^.]*\.',
            r'\.?\s*No dude en[^.]*\.',
            r'\.?\s*Para garantizar[^.]*\.',
            r'\.?\s*Es importante[^.]*\.',
            r'\.?\s*En esta etapa[^.]*\.',
        ]
        
        # Supprimer les patterns verbeux
        for pattern in verbose_patterns:
            cleaned = re.sub(pattern, '.', cleaned, flags=re.IGNORECASE)
        
        # Nettoyer les doubles points et espaces
        cleaned = re.sub(r'\.+', '.', cleaned)
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = cleaned.strip()
        
        # Si c'est une question de poids et que c'est encore long, extraire la phrase principale
        if any(word in question.lower() for word in ['poids', 'weight', 'peso']) and len(cleaned) > 100:
            weight_sentence = self._extract_weight_sentence(cleaned)
            if weight_sentence:
                return weight_sentence
        
        return cleaned
    
    def _remove_excessive_advice(self, response: str, language: str = "fr") -> str:
        """Enl√®ve seulement les conseils excessifs (mode standard)"""
        
        # D'abord nettoyer les r√©f√©rences aux documents
        cleaned = self._clean_document_references_only(response)
        
        # Patterns de conseils excessifs seulement
        excessive_patterns = [
            r'\.?\s*N\'h√©sitez pas √†[^.]*\.',
            r'\.?\s*Pour des conseils plus personnalis√©s[^.]*\.',
            r'\.?\s*Don\'t hesitate to[^.]*\.',
            r'\.?\s*For more personalized advice[^.]*\.',
            r'\.?\s*No dude en[^.]*\.',
            r'\.?\s*Para consejos m√°s personalizados[^.]*\.',
        ]
        
        for pattern in excessive_patterns:
            cleaned = re.sub(pattern, '.', cleaned, flags=re.IGNORECASE)
        
        return re.sub(r'\.+', '.', cleaned).replace(r'\s+', ' ').strip()
    
    def _clean_document_references_only(self, response_text: str) -> str:
        """Nettoie uniquement les r√©f√©rences aux documents (version originale)"""
        
        if not response_text:
            return response_text
        
        # Patterns de r√©f√©rences aux documents √† nettoyer
        patterns_to_remove = [
            # Fran√ßais
            r'selon le document \d+,?\s*',
            r'd\'apr√®s le document \d+,?\s*',
            r'le document \d+ indique que\s*',
            r'comme mentionn√© dans le document \d+,?\s*',
            r'tel que d√©crit dans le document \d+,?\s*',
            
            # Anglais
            r'according to document \d+,?\s*',
            r'as stated in document \d+,?\s*',
            r'document \d+ indicates that\s*',
            r'as mentioned in document \d+,?\s*',
            
            # Espagnol
            r'seg√∫n el documento \d+,?\s*',
            r'como se indica en el documento \d+,?\s*',
            r'el documento \d+ menciona que\s*',
            
            # Patterns g√©n√©riques
            r'\(document \d+\)',
            r'\[document \d+\]',
            r'source:\s*document \d+',
            r'ref:\s*document \d+'
        ]
        
        cleaned_response = response_text
        
        # Nettoyer chaque pattern de document
        for pattern in patterns_to_remove:
            cleaned_response = re.sub(
                pattern, 
                '', 
                cleaned_response, 
                flags=re.IGNORECASE
            )
        
        # Nettoyer les doubles espaces et capitaliser
        cleaned_response = re.sub(r'\s+', ' ', cleaned_response)
        cleaned_response = cleaned_response.strip()
        
        # Capitaliser la premi√®re lettre si n√©cessaire
        if cleaned_response and cleaned_response[0].islower():
            cleaned_response = cleaned_response[0].upper() + cleaned_response[1:]
        
        return cleaned_response
    
    def _extract_weight_sentence(self, text: str) -> Optional[str]:
        """Extrait la phrase principale contenant l'information de poids"""
        
        # Chercher la premi√®re phrase avec des chiffres + unit√©s de poids
        weight_patterns = [
            r'[^.]*\d+[^.]*(?:grammes?|g\b|kg|livres?|pounds?)[^.]*\.',
            r'[^.]*(?:entre|between|entre)\s+\d+[^.]*\d+[^.]*\.',
            r'[^.]*(?:poids|weight|peso)[^.]*\d+[^.]*\.'
        ]
        
        for pattern in weight_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                sentence = match.group(0).strip()
                # Nettoyer la phrase trouv√©e
                sentence = re.sub(r'^\W+', '', sentence)  # Supprimer ponctuation d√©but
                if sentence and sentence[0].islower():
                    sentence = sentence[0].upper() + sentence[1:]
                return sentence
        
        # Si aucun pattern trouv√©, prendre la premi√®re phrase
        sentences = text.split('.')
        if sentences:
            first_sentence = sentences[0].strip() + '.'
            return first_sentence
        
        return None

# =============================================================================
# üöÄ NOUVEAU: RESPONSE VERSIONS GENERATOR
# =============================================================================

class ResponseVersionsGenerator:
    """G√©n√©rateur de toutes les versions de r√©ponse pour le frontend"""
    
    def __init__(self, existing_processor: ResponseConcisionProcessor):
        self.existing_processor = existing_processor
        self.concision_service = concision_service
        logger.info("üöÄ [ResponseVersions] G√©n√©rateur initialis√© avec syst√®me existant")
    
    async def generate_all_response_versions(
        self, 
        original_response: str, 
        question: str, 
        context: Dict[str, Any],
        requested_level: ConcisionLevel = ConcisionLevel.CONCISE
    ) -> Dict[str, Any]:
        """
        G√©n√®re toutes les versions de r√©ponse en utilisant le syst√®me existant + nouveau
        
        Returns:
            {
                "response_versions": {"ultra_concise": "...", "concise": "...", ...},
                "selected_response": "...",
                "concision_metrics": ConcisionMetrics
            }
        """
        start_time = time.time()
        
        try:
            logger.info("üöÄ [ResponseVersions] G√©n√©ration toutes versions")
            logger.info(f"   - Question: {question[:50]}...")
            logger.info(f"   - Niveau demand√©: {requested_level}")
            logger.info(f"   - R√©ponse originale: {len(original_response)} caract√®res")
            
            # 1. Utiliser le syst√®me existant pour chaque niveau
            versions = {}
            
            # Version DETAILED = r√©ponse originale compl√®te
            versions["detailed"] = original_response
            
            # G√©n√©rer version STANDARD avec syst√®me existant
            standard_response = self.existing_processor.apply_concision(
                original_response, 
                question, 
                ConcisionLevel.STANDARD,
                context.get("language", "fr")
            )
            versions["standard"] = standard_response
            
            # G√©n√©rer version CONCISE avec syst√®me existant
            concise_response = self.existing_processor.apply_concision(
                original_response, 
                question, 
                ConcisionLevel.CONCISE,
                context.get("language", "fr")
            )
            versions["concise"] = concise_response
            
            # G√©n√©rer version ULTRA_CONCISE avec syst√®me existant
            ultra_concise_response = self.existing_processor.apply_concision(
                original_response, 
                question, 
                ConcisionLevel.ULTRA_CONCISE,
                context.get("language", "fr")
            )
            versions["ultra_concise"] = ultra_concise_response
            
            # 2. S√©lectionner la r√©ponse principale selon niveau demand√©
            selected_response = versions.get(requested_level.value, versions["concise"])
            
            # 3. G√©n√©rer m√©triques
            generation_time_ms = int((time.time() - start_time) * 1000)
            
            metrics = ConcisionMetrics(
                generation_time_ms=generation_time_ms,
                versions_generated=len(versions),
                cache_hit=False,  # Le syst√®me existant ne cache pas
                fallback_used=False,
                compression_ratios={
                    level: len(content) / len(original_response) 
                    for level, content in versions.items()
                    if content and len(original_response) > 0
                },
                quality_scores={}
            )
            
            logger.info("‚úÖ [ResponseVersions] Versions g√©n√©r√©es avec syst√®me existant:")
            for level, content in versions.items():
                logger.info(f"   - {level}: {len(content)} caract√®res")
            
            return {
                "response_versions": versions,
                "selected_response": selected_response,
                "concision_metrics": metrics
            }
            
        except Exception as e:
            logger.error(f"‚ùå [ResponseVersions] Erreur g√©n√©ration: {e}")
            
            # Fallback : retourner versions simples
            fallback_versions = {
                "ultra_concise": original_response[:50] + "..." if len(original_response) > 50 else original_response,
                "concise": original_response,
                "standard": original_response,
                "detailed": original_response
            }
            
            return {
                "response_versions": fallback_versions,
                "selected_response": fallback_versions.get(requested_level.value, original_response),
                "concision_metrics": ConcisionMetrics(
                    generation_time_ms=int((time.time() - start_time) * 1000),
                    versions_generated=len(fallback_versions),
                    cache_hit=False,
                    fallback_used=True,
                    compression_ratios={},
                    quality_scores={}
                )
            }

# =============================================================================
# üîÑ RAG CONTEXT ENHANCER (CONSERV√â IDENTIQUE)
# =============================================================================

class RAGContextEnhancer:
    """Am√©liore le contexte conversationnel pour optimiser les requ√™tes RAG"""
    
    def __init__(self):
        # Patterns pour d√©tecter les r√©f√©rences contextuelles
        self.pronoun_patterns = {
            "fr": [
                r'\b(son|sa|ses|leur|leurs)\s+(poids|√¢ge|croissance|d√©veloppement)',
                r'\b(ils|elles)\s+(p√®sent|grandissent|se d√©veloppent)',
                r'\b(qu\'?est-ce que|quel est)\s+(son|sa|ses|leur)',
                r'\b(combien)\s+(p√®sent-ils|font-ils|mesurent-ils)'
            ],
            "en": [
                r'\b(their|its)\s+(weight|age|growth|development)',
                r'\b(they)\s+(weigh|grow|develop)',
                r'\b(what is|how much is)\s+(their|its)',
                r'\b(how much do they)\s+(weigh|measure)'
            ],
            "es": [
                r'\b(su|sus)\s+(peso|edad|crecimiento|desarrollo)',
                r'\b(ellos|ellas)\s+(pesan|crecen|se desarrollan)',
                r'\b(cu√°l es|cu√°nto es)\s+(su|sus)',
                r'\b(cu√°nto)\s+(pesan|miden)'
            ]
        }
    
    def enhance_question_for_rag(
        self, 
        question: str, 
        conversation_context: str, 
        language: str = "fr"
    ) -> Tuple[str, Dict[str, any]]:
        """Am√©liore une question pour le RAG en utilisant le contexte conversationnel"""
        
        enhancement_info = {
            "pronoun_detected": False,
            "context_entities_used": [],
            "question_enriched": False,
            "original_question": question
        }
        
        # 1. D√©tecter les pronoms/r√©f√©rences contextuelles
        has_pronouns = self._detect_contextual_references(question, language)
        if has_pronouns:
            enhancement_info["pronoun_detected"] = True
            logger.info(f"üîç [RAG Context] Pronoms d√©tect√©s dans: '{question}'")
        
        # 2. Extraire entit√©s du contexte
        context_entities = self._extract_context_entities(conversation_context)
        if context_entities:
            enhancement_info["context_entities_used"] = list(context_entities.keys())
            logger.info(f"üìä [RAG Context] Entit√©s contextuelles: {context_entities}")
        
        # 3. Enrichir la question si n√©cessaire
        enriched_question = question
        
        if has_pronouns and context_entities:
            enriched_question = self._build_enriched_question(
                question, context_entities, language
            )
            enhancement_info["question_enriched"] = True
            logger.info(f"‚ú® [RAG Context] Question enrichie: '{enriched_question}'")
        
        # 4. Ajouter contexte technique si pertinent
        if context_entities or has_pronouns:
            technical_context = self._build_technical_context(context_entities, language)
            if technical_context:
                enriched_question += f"\n\nContexte technique: {technical_context}"
        
        return enriched_question, enhancement_info
    
    def _detect_contextual_references(self, question: str, language: str) -> bool:
        """D√©tecte si la question contient des pronoms/r√©f√©rences contextuelles"""
        
        patterns = self.pronoun_patterns.get(language, self.pronoun_patterns["fr"])
        question_lower = question.lower()
        
        for pattern in patterns:
            if re.search(pattern, question_lower, re.IGNORECASE):
                logger.debug(f"üéØ [RAG Context] Pattern trouv√©: {pattern}")
                return True
        
        return False
    
    def _extract_context_entities(self, context: str) -> Dict[str, str]:
        """Extrait les entit√©s importantes du contexte conversationnel"""
        
        if not context:
            return {}
        
        entities = {}
        context_lower = context.lower()
        
        # Extraire race
        breed_patterns = [
            r'race[:\s]+([a-zA-Z0-9\s]+?)(?:\n|,|\.|\s|$)',
            r'breed[:\s]+([a-zA-Z0-9\s]+?)(?:\n|,|\.|\s|$)',
            r'(ross\s*308|cobb\s*500|hubbard|arbor\s*acres)',
            r'poulets?\s+(ross\s*308|cobb\s*500)',
            r'chickens?\s+(ross\s*308|cobb\s*500)'
        ]
        
        for pattern in breed_patterns:
            match = re.search(pattern, context_lower, re.IGNORECASE)
            if match:
                entities["breed"] = match.group(1).strip()
                break
        
        # Extraire sexe
        sex_patterns = [
            r'sexe[:\s]+([a-zA-Z\s]+?)(?:\n|,|\.|\s|$)',
            r'sex[:\s]+([a-zA-Z\s]+?)(?:\n|,|\.|\s|$)',
            r'\b(m√¢les?|femelles?|males?|females?|mixte|mixed)\b'
        ]
        
        for pattern in sex_patterns:
            match = re.search(pattern, context_lower, re.IGNORECASE)
            if match:
                entities["sex"] = match.group(1).strip()
                break
        
        # Extraire √¢ge
        age_patterns = [
            r'√¢ge[:\s]+(\d+\s*(?:jour|semaine|day|week)s?)',
            r'age[:\s]+(\d+\s*(?:jour|semaine|day|week)s?)',
            r'(\d+)\s*(?:jour|day)s?',
            r'(\d+)\s*(?:semaine|week)s?'
        ]
        
        for pattern in age_patterns:
            match = re.search(pattern, context_lower, re.IGNORECASE)
            if match:
                entities["age"] = match.group(1).strip()
                break
        
        return entities
    
    def _build_enriched_question(
        self, 
        question: str, 
        context_entities: Dict[str, str], 
        language: str
    ) -> str:
        """Construit une question enrichie en rempla√ßant les pronoms par les entit√©s contextuelles"""
        
        enriched = question
        
        # Templates par langue
        templates = {
            "fr": {
                "breed_sex_age": "Pour des {breed} {sex} de {age}",
                "breed_age": "Pour des {breed} de {age}",
                "breed_sex": "Pour des {breed} {sex}",
                "breed_only": "Pour des {breed}",
                "age_only": "Pour des poulets de {age}"
            },
            "en": {
                "breed_sex_age": "For {breed} {sex} chickens at {age}",
                "breed_age": "For {breed} chickens at {age}",
                "breed_sex": "For {breed} {sex} chickens",
                "breed_only": "For {breed} chickens", 
                "age_only": "For chickens at {age}"
            },
            "es": {
                "breed_sex_age": "Para pollos {breed} {sex} de {age}",
                "breed_age": "Para pollos {breed} de {age}",
                "breed_sex": "Para pollos {breed} {sex}",
                "breed_only": "Para pollos {breed}",
                "age_only": "Para pollos de {age}"
            }
        }
        
        template_set = templates.get(language, templates["fr"])
        
        # Construire le pr√©fixe contextuel
        context_prefix = ""
        if "breed" in context_entities and "sex" in context_entities and "age" in context_entities:
            context_prefix = template_set["breed_sex_age"].format(
                breed=context_entities["breed"],
                sex=context_entities["sex"],
                age=context_entities["age"]
            )
        elif "breed" in context_entities and "age" in context_entities:
            context_prefix = template_set["breed_age"].format(
                breed=context_entities["breed"],
                age=context_entities["age"]
            )
        elif "breed" in context_entities and "sex" in context_entities:
            context_prefix = template_set["breed_sex"].format(
                breed=context_entities["breed"],
                sex=context_entities["sex"]
            )
        elif "breed" in context_entities:
            context_prefix = template_set["breed_only"].format(
                breed=context_entities["breed"]
            )
        elif "age" in context_entities:
            context_prefix = template_set["age_only"].format(age=context_entities["age"])
        
        if context_prefix:
            # Remplacer ou pr√©fixer selon la structure de la question
            if any(word in question.lower() for word in ["son", "sa", "ses", "leur", "leurs", "their", "its", "su", "sus"]):
                enriched = f"{context_prefix}, {question.lower()}"
            else:
                enriched = f"{context_prefix}: {question}"
        
        return enriched
    
    def _build_technical_context(self, entities: Dict[str, str], language: str) -> str:
        """Construit un contexte technique pour aider le RAG"""
        
        if not entities:
            return ""
        
        context_parts = []
        
        if "breed" in entities:
            context_parts.append(f"Race: {entities['breed']}")
        
        if "sex" in entities:
            context_parts.append(f"Sexe: {entities['sex']}")
        
        if "age" in entities:
            context_parts.append(f"√Çge: {entities['age']}")
        
        return " | ".join(context_parts)

# =============================================================================
# üîÑ EXPERT SERVICE PRINCIPAL AVEC RESPONSE VERSIONS
# =============================================================================

class ExpertService:
    """Service principal pour le syst√®me expert avec concision + response_versions int√©gr√©"""
    
    def __init__(self):
        # ‚úÖ CONSERVER tous les attributs existants
        self.integrations = IntegrationsManager()
        self.rag_enhancer = RAGContextEnhancer()
        self.enhancement_service = APIEnhancementService()
        
        # ‚úÖ CONSERV√â: Processeur de concision existant
        self.concision_processor = ResponseConcisionProcessor()
        
        # üöÄ NOUVEAU : Ajouter g√©n√©rateur de versions
        self.response_versions_generator = ResponseVersionsGenerator(
            existing_processor=self.concision_processor  # Utiliser le syst√®me existant
        )
        
        logger.info("‚úÖ [Expert Service] Service expert initialis√© avec syst√®me de concision + response_versions")
    
    def get_current_user_dependency(self):
        """Retourne la d√©pendance pour l'authentification"""
        return self.integrations.get_current_user_dependency()
    
    async def process_expert_question(
        self,
        request_data: EnhancedQuestionRequest,
        request: Request,
        current_user: Optional[Dict[str, Any]] = None,
        start_time: float = None
    ) -> EnhancedExpertResponse:
        """
        üöÄ MODIFI√â: M√©thode principale avec support response_versions
        ‚úÖ CONSERVE toute la logique existante + ajoute g√©n√©ration versions
        """
        
        if start_time is None:
            start_time = time.time()
        
        try:
            logger.info("üöÄ [ExpertService] Traitement question avec support response_versions")
            
            # üöÄ NOUVEAU : Extraire param√®tres concision
            concision_level = getattr(request_data, 'concision_level', ConcisionLevel.CONCISE)
            generate_all_versions = getattr(request_data, 'generate_all_versions', True)
            
            logger.info(f"üöÄ [ResponseVersions] Param√®tres: level={concision_level}, generate_all={generate_all_versions}")
            
            # ‚úÖ APPELER LA LOGIQUE EXISTANTE pour obtenir la r√©ponse de base
            base_response = await self._process_question_existing_logic(
                request_data, request, current_user, start_time
            )
            
            # üöÄ NOUVEAU : G√©n√©rer toutes les versions si demand√©
            if generate_all_versions and base_response.response:
                try:
                    logger.info("üöÄ [ResponseVersions] G√©n√©ration de toutes les versions")
                    
                    versions_result = await self.response_versions_generator.generate_all_response_versions(
                        original_response=base_response.response,
                        question=request_data.text,
                        context={
                            "language": request_data.language,
                            "user_id": current_user.get("id") if current_user else None,
                            "conversation_id": request_data.conversation_id
                        },
                        requested_level=concision_level
                    )
                    
                    # Mettre √† jour la r√©ponse avec les versions
                    base_response.response_versions = versions_result["response_versions"]
                    base_response.response = versions_result["selected_response"]  # R√©ponse du niveau demand√©
                    base_response.concision_metrics = versions_result["concision_metrics"]
                    
                    logger.info("‚úÖ [ResponseVersions] Versions ajout√©es √† la r√©ponse")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è [ResponseVersions] Erreur g√©n√©ration versions: {e}")
                    # Continuer sans versions si erreur
                    base_response.response_versions = None
            else:
                logger.info("üöÄ [ResponseVersions] G√©n√©ration versions d√©sactiv√©e")
                base_response.response_versions = None
            
            return base_response
            
        except Exception as e:
            logger.error(f"‚ùå [ExpertService] Erreur traitement avec response_versions: {e}")
            raise
    
    async def _process_question_existing_logic(
        self,
        request_data: EnhancedQuestionRequest,
        request: Request,
        current_user: Optional[Dict[str, Any]] = None,
        start_time: float = None
    ) -> EnhancedExpertResponse:
        """
        ‚úÖ TOUTE LA LOGIQUE EXISTANTE DE process_expert_question (CONSERV√âE IDENTIQUE)
        """
        
        processing_steps = []
        ai_enhancements_used = []
        debug_info = {}
        performance_breakdown = {"start": int(time.time() * 1000)}
        
        # Initialisation
        processing_steps.append("initialization")
        
        # === AUTHENTIFICATION ===
        if current_user is None and self.integrations.auth_available:
            raise HTTPException(status_code=401, detail="Authentification requise")
        
        user_id = self._extract_user_id(current_user, request_data, request)
        user_email = current_user.get("email") if current_user else None
        request_ip = request.client.host if request.client else "unknown"
        
        processing_steps.append("authentication")
        performance_breakdown["auth_complete"] = int(time.time() * 1000)
        
        # === GESTION CONVERSATION ID ===
        conversation_id = self._get_or_create_conversation_id(request_data)
        
        # === VALIDATION QUESTION ===
        question_text = request_data.text.strip()
        if not question_text:
            raise HTTPException(status_code=400, detail="Question text is required")
        
        processing_steps.append("question_validation")
        
        # === M√âMOIRE CONVERSATIONNELLE ===
        conversation_context = None
        if self.integrations.intelligent_memory_available:
            try:
                conversation_context = self.integrations.add_message_to_conversation(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    message=question_text,
                    role="user",
                    language=request_data.language,
                    message_type="clarification_response" if request_data.is_clarification_response else "question"
                )
                ai_enhancements_used.append("intelligent_memory")
                processing_steps.append("memory_storage")
                logger.info(f"üíæ [Expert Service] Message ajout√© √† la m√©moire: {question_text[:50]}...")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [Expert Service] Erreur m√©moire: {e}")
        
        performance_breakdown["memory_complete"] = int(time.time() * 1000)
        
        # === VALIDATION AGRICOLE ===
        validation_result = await self._validate_agricultural_question(
            question_text, request_data.language, user_id, request_ip, conversation_id
        )
        
        processing_steps.append("agricultural_validation")
        performance_breakdown["validation_complete"] = int(time.time() * 1000)
        
        if not validation_result.is_valid:
            return self._create_rejection_response(
                question_text, validation_result, conversation_id, 
                user_email, request_data.language, start_time,
                processing_steps, ai_enhancements_used, None
            )
        
        # === SYST√àME DE CLARIFICATION INTELLIGENT ===
        clarification_result = await self._handle_clarification_corrected(
            request_data, question_text, user_id, conversation_id,
            processing_steps, ai_enhancements_used
        )
        
        if clarification_result:
            return clarification_result
        
        # D√©tection vagueness apr√®s clarifications sp√©cialis√©es
        vagueness_result = None
        if request_data.enable_vagueness_detection:
            vagueness_result = self.enhancement_service.detect_vagueness(
                question_text, request_data.language
            )
            
            ai_enhancements_used.append("vagueness_detection")
            performance_breakdown["vagueness_check"] = int(time.time() * 1000)
            
            if vagueness_result.is_vague and vagueness_result.vagueness_score > 0.6:
                logger.info(f"üéØ [Expert Service] Question floue d√©tect√©e (score: {vagueness_result.vagueness_score})")
                return self._create_vagueness_response(
                    vagueness_result, question_text, conversation_id, 
                    request_data.language, start_time, processing_steps, ai_enhancements_used
                )
        
        performance_breakdown["clarification_complete"] = int(time.time() * 1000)
        
        # === TRAITEMENT EXPERT AVEC RAG-FIRST + AM√âLIORATIONS ===
        expert_result = await self._process_expert_response_enhanced_corrected(
            question_text, request_data, request, current_user,
            conversation_id, processing_steps, ai_enhancements_used,
            debug_info, performance_breakdown, vagueness_result
        )
        
        # ‚úÖ CONSERV√â: APPLICATION DU SYST√àME DE CONCISION EXISTANT
        if expert_result["answer"] and self.concision_processor.config.ENABLE_CONCISE_RESPONSES:
            
            # D√©tecter le niveau de concision optimal (peut √™tre overrid√© par pr√©f√©rence utilisateur)
            user_concision_preference = getattr(request_data, 'concision_level', None)
            
            original_answer = expert_result["answer"]
            processed_answer = self.concision_processor.process_response(
                response=original_answer,
                question=question_text,
                concision_level=user_concision_preference,
                language=request_data.language
            )
            
            if processed_answer != original_answer:
                expert_result["answer"] = processed_answer
                expert_result["original_answer"] = original_answer  # Garder l'original
                expert_result["concision_applied"] = True
                ai_enhancements_used.append("response_concision")
                processing_steps.append("concision_processing")
                
                logger.info(f"‚úÇÔ∏è [Expert Service] Concision appliqu√©e: {len(original_answer)} ‚Üí {len(processed_answer)} chars")
            else:
                expert_result["concision_applied"] = False
        
        performance_breakdown["concision_complete"] = int(time.time() * 1000)
        
        # === ENREGISTREMENT R√âPONSE ===
        if self.integrations.intelligent_memory_available and expert_result["answer"]:
            try:
                self.integrations.add_message_to_conversation(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    message=expert_result["answer"],
                    role="assistant",
                    language=request_data.language,
                    message_type="response"
                )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [Expert Service] Erreur enregistrement r√©ponse: {e}")
        
        processing_steps.append("response_storage")
        performance_breakdown["final"] = int(time.time() * 1000)
        
        # === CONSTRUCTION R√âPONSE FINALE AM√âLIOR√âE ===
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return self._build_final_enhanced_response(
            question_text, expert_result["answer"], conversation_id,
            user_email, request_data.language, response_time_ms,
            expert_result, validation_result, conversation_context,
            processing_steps, ai_enhancements_used, request_data,
            debug_info, performance_breakdown
        )
    
    # ===========================================================================================
    # ‚úÖ TOUTES LES M√âTHODES EXISTANTES CONSERV√âES IDENTIQUES
    # ===========================================================================================
    
    async def _handle_clarification_corrected(
        self, request_data, question_text, user_id, conversation_id, 
        processing_steps, ai_enhancements_used
    ):
        """‚úÖ SYST√àME DE CLARIFICATION PARFAITEMENT CORRIG√â (CONSERV√â IDENTIQUE)"""
        
        # 1. ‚úÖ TRAITEMENT DES R√âPONSES DE CLARIFICATION CORRIG√â
        if request_data.is_clarification_response:
            return await self._process_clarification_response_corrected(
                request_data, question_text, conversation_id,
                processing_steps, ai_enhancements_used
            )
        
        # 2. D√âTECTION QUESTIONS N√âCESSITANT CLARIFICATION
        clarification_needed = self._detect_performance_question_needing_clarification(
            question_text, request_data.language
        )
        
        if not clarification_needed:
            return None
        
        logger.info(f"üéØ [Expert Service] Clarification n√©cessaire: {clarification_needed['type']}")
        processing_steps.append("automatic_clarification_triggered")
        ai_enhancements_used.append("smart_performance_clarification")
        
        # 3. ‚úÖ SAUVEGARDE FORC√âE AVEC M√âMOIRE INTELLIGENTE
        if self.integrations.intelligent_memory_available:
            try:
                # Utiliser la fonction d√©di√©e du syst√®me de m√©moire
                from .conversation_memory_enhanced import mark_question_for_clarification
                
                question_id = mark_question_for_clarification(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    original_question=question_text,
                    language=request_data.language
                )
                
                logger.info(f"üíæ [Expert Service] Question originale marqu√©e: {question_id}")
                processing_steps.append("original_question_marked")
                ai_enhancements_used.append("intelligent_memory_clarification_marking")
                
            except Exception as e:
                logger.error(f"‚ùå [Expert Service] Erreur marquage question: {e}")
                # Fallback: marquer manuellement
                self.integrations.add_message_to_conversation(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    message=f"ORIGINAL_QUESTION_FOR_CLARIFICATION: {question_text}",
                    role="system",
                    language=request_data.language,
                    message_type="original_question_marker"
                )
        
        # 4. G√©n√©rer la demande de clarification
        clarification_response = self._generate_performance_clarification_response(
            question_text, clarification_needed, request_data.language, conversation_id
        )
        
        return clarification_response
    
    async def _process_clarification_response_corrected(
        self, request_data, question_text, conversation_id, 
        processing_steps, ai_enhancements_used
    ):
        """‚úÖ TRAITEMENT DES R√âPONSES DE CLARIFICATION - VERSION CORRIG√âE FINALE (CONSERV√â)"""
        
        # ‚úÖ R√âCUP√âRATION FORC√âE DE LA QUESTION ORIGINALE
        original_question = request_data.original_question
        clarification_context = request_data.clarification_context
        
        # Si pas de contexte fourni, r√©cup√©rer depuis la m√©moire intelligente
        if (not original_question or not clarification_context) and self.integrations.intelligent_memory_available:
            try:
                from .conversation_memory_enhanced import find_original_question
                
                original_msg = find_original_question(conversation_id)
                
                if original_msg:
                    original_question = original_msg.message
                    clarification_context = {
                        "missing_information": ["breed", "sex"],
                        "clarification_type": "performance_breed_sex"
                    }
                    logger.info(f"‚úÖ [Expert Service] Question originale r√©cup√©r√©e: {original_question}")
                    ai_enhancements_used.append("intelligent_memory_original_question_recovery")
                else:
                    logger.warning("‚ö†Ô∏è [Expert Service] Question originale non trouv√©e dans la m√©moire")
                    
            except Exception as e:
                logger.error(f"‚ùå [Expert Service] Erreur r√©cup√©ration question originale: {e}")
        
        # Si toujours pas de question originale, utiliser fallback
        if not original_question:
            logger.warning("‚ö†Ô∏è [Expert Service] Fallback: cr√©ation question par d√©faut")
            original_question = "Quel est le poids de r√©f√©rence pour ces poulets ?"
            clarification_context = {
                "missing_information": ["breed", "sex"],
                "clarification_type": "performance_breed_sex_fallback"
            }
        
        # ‚úÖ EXTRACTION AM√âLIOR√âE DES INFORMATIONS DE CLARIFICATION
        missing_info = clarification_context.get("missing_information", [])
        
        # Validation de la compl√©tude avec extraction am√©lior√©e
        validation = validate_clarification_completeness(
            question_text, missing_info, request_data.language
        )
        
        # Gestion des r√©ponses partielles
        if not validation["is_complete"]:
            logger.info(f"üîÑ [Expert Service] Clarification incompl√®te: {validation['still_missing']}")
            return self._generate_follow_up_clarification(
                question_text, validation, request_data.language, conversation_id
            )
        
        # ‚úÖ EXTRACTION RACE/SEXE DEPUIS CLARIFICATION
        breed = validation["extracted_info"].get("breed")
        sex = validation["extracted_info"].get("sex")
        
        # üö® EXTRACTION √ÇGE DEPUIS QUESTION ORIGINALE
        age_info = self._extract_age_from_original_question(original_question, request_data.language)
        
        # ‚úÖ ENRICHISSEMENT AUTOMATIQUE COMPLET (race + sexe + √¢ge)
        enriched_original_question = self._build_complete_enriched_question(
            original_question, breed, sex, age_info, request_data.language
        )
        
        logger.info(f"‚úÖ [Expert Service] Question COMPL√àTEMENT enrichie: {enriched_original_question}")
        
        # ‚úÖ FORCER LE REMPLACEMENT DE LA QUESTION POUR LE RAG
        request_data.text = enriched_original_question
        request_data.is_clarification_response = False  # Traiter comme nouvelle question
        request_data.original_question = original_question  # Garder r√©f√©rence
        
        processing_steps.append("clarification_processed_successfully_with_age")
        ai_enhancements_used.append("breed_sex_age_extraction_complete")
        ai_enhancements_used.append("complete_question_enrichment")
        ai_enhancements_used.append("forced_question_replacement_with_age")
        
        return None  # Continuer le traitement normal avec question enrichie
    
    def _detect_performance_question_needing_clarification(
        self, question: str, language: str = "fr"
    ) -> Optional[Dict[str, Any]]:
        """D√©tection am√©lior√©e des questions techniques n√©cessitant race/sexe (CONSERV√â)"""
        
        question_lower = question.lower()
        
        # Patterns de questions sur poids/performance avec √¢ge mais sans race/sexe
        weight_age_patterns = {
            "fr": [
                r'(?:poids|p√®se)\s+.*?(\d+)\s*(?:jour|semaine)s?',
                r'(\d+)\s*(?:jour|semaine)s?.*?(?:poids|p√®se)',
                r'(?:quel|combien)\s+.*?(?:poids|p√®se).*?(\d+)',
                r'(?:croissance|d√©veloppement).*?(\d+)\s*(?:jour|semaine)',
                r'(\d+)\s*(?:jour|semaine).*?(?:normal|r√©f√©rence|standard)'
            ],
            "en": [
                r'(?:weight|weigh)\s+.*?(\d+)\s*(?:day|week)s?',
                r'(\d+)\s*(?:day|week)s?.*?(?:weight|weigh)',
                r'(?:what|how much)\s+.*?(?:weight|weigh).*?(\d+)',
                r'(?:growth|development).*?(\d+)\s*(?:day|week)',
                r'(\d+)\s*(?:day|week).*?(?:normal|reference|standard)'
            ],
            "es": [
                r'(?:peso|pesa)\s+.*?(\d+)\s*(?:d√≠a|semana)s?',
                r'(\d+)\s*(?:d√≠a|semana)s?.*?(?:peso|pesa)',
                r'(?:cu√°l|cu√°nto)\s+.*?(?:peso|pesa).*?(\d+)',
                r'(?:crecimiento|desarrollo).*?(\d+)\s*(?:d√≠a|semana)',
                r'(\d+)\s*(?:d√≠a|semana).*?(?:normal|referencia|est√°ndar)'
            ]
        }
        
        patterns = weight_age_patterns.get(language, weight_age_patterns["fr"])
        
        # V√©rifier si c'est une question poids+√¢ge
        age_detected = None
        for pattern in patterns:
            match = re.search(pattern, question_lower)
            if match:
                age_detected = match.group(1)
                break
        
        if not age_detected:
            return None
        
        # V√©rifier si race/sexe sont ABSENTS
        breed_patterns = [
            r'\b(ross\s*308|ross\s*708|cobb\s*500|cobb\s*700|hubbard|arbor\s*acres)\b',
            r'\b(broiler|poulet|chicken|pollo)\s+(ross|cobb|hubbard)',
            r'\brace\s*[:\-]?\s*(ross|cobb|hubbard)'
        ]
        
        sex_patterns = [
            r'\b(m√¢le|male|macho)s?\b',
            r'\b(femelle|female|hembra)s?\b',
            r'\b(coq|hen|poule|gallina)\b',
            r'\b(mixte|mixed|misto)\b'
        ]
        
        has_breed = any(re.search(pattern, question_lower, re.IGNORECASE) for pattern in breed_patterns)
        has_sex = any(re.search(pattern, question_lower, re.IGNORECASE) for pattern in sex_patterns)
        
        # Clarification n√©cessaire si poids+√¢ge MAIS pas de race NI sexe
        if not has_breed and not has_sex:
            return {
                "type": "performance_question_missing_breed_sex",
                "age_detected": age_detected,
                "question_type": "weight_performance",
                "missing_info": ["breed", "sex"],
                "confidence": 0.95
            }
        
        # Clarification partielle si seulement un des deux manque
        elif not has_breed or not has_sex:
            missing = []
            if not has_breed:
                missing.append("breed")
            if not has_sex:
                missing.append("sex")
            
            return {
                "type": "performance_question_partial_info",
                "age_detected": age_detected,
                "question_type": "weight_performance", 
                "missing_info": missing,
                "confidence": 0.8
            }
        
        return None

    def _generate_performance_clarification_response(
        self, question: str, clarification_info: Dict, language: str, conversation_id: str
    ) -> EnhancedExpertResponse:
        """G√©n√®re la demande de clarification optimis√©e (CONSERV√â)"""
        
        age = clarification_info.get("age_detected", "X")
        missing_info = clarification_info.get("missing_info", [])
        
        # Messages de clarification par langue
        clarification_messages = {
            "fr": {
                "both_missing": f"Pour vous donner le poids de r√©f√©rence exact d'un poulet de {age} jours, j'ai besoin de :\n\n‚Ä¢ **Race/souche** : Ross 308, Cobb 500, Hubbard, etc.\n‚Ä¢ **Sexe** : M√¢les, femelles, ou troupeau mixte\n\nPouvez-vous pr√©ciser ces informations ?",
                "breed_missing": f"Pour le poids exact √† {age} jours, quelle est la **race/souche** (Ross 308, Cobb 500, Hubbard, etc.) ?",
                "sex_missing": f"Pour le poids exact √† {age} jours, s'agit-il de **m√¢les, femelles, ou d'un troupeau mixte** ?"
            },
            "en": {
                "both_missing": f"To give you the exact reference weight for a {age}-day chicken, I need:\n\n‚Ä¢ **Breed/strain**: Ross 308, Cobb 500, Hubbard, etc.\n‚Ä¢ **Sex**: Males, females, or mixed flock\n\nCould you specify this information?",
                "breed_missing": f"For the exact weight at {age} days, what is the **breed/strain** (Ross 308, Cobb 500, Hubbard, etc.)?",
                "sex_missing": f"For the exact weight at {age} days, are these **males, females, or a mixed flock**?"
            },
            "es": {
                "both_missing": f"Para darle el peso de referencia exacto de un pollo de {age} d√≠as, necesito:\n\n‚Ä¢ **Raza/cepa**: Ross 308, Cobb 500, Hubbard, etc.\n‚Ä¢ **Sexo**: Machos, hembras, o lote mixto\n\n¬øPodr√≠a especificar esta informaci√≥n?",
                "breed_missing": f"Para el peso exacto a los {age} d√≠as, ¬øcu√°l es la **raza/cepa** (Ross 308, Cobb 500, Hubbard, etc.)?",
                "sex_missing": f"Para el peso exacto a los {age} d√≠as, ¬øson **machos, hembras, o un lote mixto**?"
            }
        }
        
        messages = clarification_messages.get(language, clarification_messages["fr"])
        
        # S√©lectionner le message appropri√©
        if len(missing_info) >= 2:
            response_text = messages["both_missing"]
        elif "breed" in missing_info:
            response_text = messages["breed_missing"]
        else:
            response_text = messages["sex_missing"]
        
        # Ajouter exemples de r√©ponse
        examples = {
            "fr": "\n\n**Exemples de r√©ponses :**\n‚Ä¢ \"Ross 308 m√¢les\"\n‚Ä¢ \"Cobb 500 femelles\"\n‚Ä¢ \"Hubbard troupeau mixte\"",
            "en": "\n\n**Example responses:**\n‚Ä¢ \"Ross 308 males\"\n‚Ä¢ \"Cobb 500 females\"\n‚Ä¢ \"Hubbard mixed flock\"",
            "es": "\n\n**Ejemplos de respuestas:**\n‚Ä¢ \"Ross 308 machos\"\n‚Ä¢ \"Cobb 500 hembras\"\n‚Ä¢ \"Hubbard lote mixto\""
        }
        
        response_text += examples.get(language, examples["fr"])
        
        return EnhancedExpertResponse(
            question=question,
            response=response_text,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=50,
            mode="smart_performance_clarification_corrected",
            user=None,
            logged=True,
            validation_passed=True,
            clarification_result={
                "clarification_requested": True,
                "clarification_type": "performance_breed_sex",
                "missing_information": missing_info,
                "age_detected": age,
                "confidence": clarification_info.get("confidence", 0.9)
            },
            processing_steps=["smart_clarification_triggered"],
            ai_enhancements_used=["performance_question_detection", "targeted_clarification"]
        )
    
    def _generate_follow_up_clarification(
        self, question: str, validation: Dict, language: str, conversation_id: str
    ) -> EnhancedExpertResponse:
        """G√©n√®re une clarification de suivi si premi√®re r√©ponse incompl√®te (CONSERV√â)"""
        
        still_missing = validation["still_missing"]
        
        messages = {
            "fr": {
                "breed": "Il me manque encore la **race/souche**. Ross 308, Cobb 500, ou autre ?",
                "sex": "Il me manque encore le **sexe**. M√¢les, femelles, ou troupeau mixte ?",  
                "both": "Il me manque encore la **race et le sexe**. Exemple : \"Ross 308 m√¢les\""
            },
            "en": {
                "breed": "I still need the **breed/strain**. Ross 308, Cobb 500, or other?",
                "sex": "I still need the **sex**. Males, females, or mixed flock?",
                "both": "I still need the **breed and sex**. Example: \"Ross 308 males\""
            },
            "es": {
                "breed": "A√∫n necesito la **raza/cepa**. ¬øRoss 308, Cobb 500, u otra?",
                "sex": "A√∫n necesito el **sexo**. ¬øMachos, hembras, o lote mixto?",
                "both": "A√∫n necesito la **raza y sexo**. Ejemplo: \"Ross 308 machos\""
            }
        }
        
        lang_messages = messages.get(language, messages["fr"])
        
        if len(still_missing) >= 2:
            message = lang_messages["both"]
        elif "breed" in still_missing:
            message = lang_messages["breed"]
        else:
            message = lang_messages["sex"]
        
        return EnhancedExpertResponse(
            question=question,
            response=message,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=30,
            mode="follow_up_clarification_corrected",
            user=None,
            logged=True,
            validation_passed=True,
            clarification_result={
                "clarification_requested": True,
                "clarification_type": "follow_up_incomplete",
                "still_missing_information": still_missing,
                "confidence": validation["confidence"]
            },
            processing_steps=["follow_up_clarification_triggered"],
            ai_enhancements_used=["incomplete_clarification_handling"]
        )
    
    # === M√âTHODES D'ENRICHISSEMENT COMPLET (CONSERV√âES IDENTIQUES) ===
    
    def _extract_age_from_original_question(self, original_question: str, language: str = "fr") -> Dict[str, Any]:
        """Extrait l'√¢ge depuis la question originale (CONSERV√â)"""
        
        age_info = {"days": None, "weeks": None, "text": None, "detected": False}
        
        if not original_question:
            return age_info
        
        question_lower = original_question.lower()
        
        # Patterns d'extraction d'√¢ge multilingues
        age_patterns = {
            "fr": [
                (r'(\d+)\s*jours?', "days"),
                (r'(\d+)\s*semaines?', "weeks"),
                (r'de\s+(\d+)\s*jours?', "days"),
                (r'de\s+(\d+)\s*semaines?', "weeks"),
                (r'√†\s+(\d+)\s*jours?', "days"),
                (r'√†\s+(\d+)\s*semaines?', "weeks")
            ],
            "en": [
                (r'(\d+)\s*days?', "days"),
                (r'(\d+)\s*weeks?', "weeks"),
                (r'of\s+(\d+)\s*days?', "days"),
                (r'of\s+(\d+)\s*weeks?', "weeks"),
                (r'at\s+(\d+)\s*days?', "days"),
                (r'at\s+(\d+)\s*weeks?', "weeks")
            ],
            "es": [
                (r'(\d+)\s*d√≠as?', "days"),
                (r'(\d+)\s*semanas?', "weeks"),
                (r'de\s+(\d+)\s*d√≠as?', "days"),
                (r'de\s+(\d+)\s*semanas?', "weeks"),
                (r'a\s+(\d+)\s*d√≠as?', "days"),
                (r'a\s+(\d+)\s*semanas?', "weeks")
            ]
        }
        
        patterns = age_patterns.get(language, age_patterns["fr"])
        
        for pattern, unit in patterns:
            match = re.search(pattern, question_lower, re.IGNORECASE)
            if match:
                value = int(match.group(1))
                
                if unit == "days":
                    age_info["days"] = value
                    age_info["weeks"] = round(value / 7, 1)
                    age_info["text"] = f"{value} jour{'s' if value > 1 else ''}"
                else:  # weeks
                    age_info["weeks"] = value
                    age_info["days"] = value * 7
                    age_info["text"] = f"{value} semaine{'s' if value > 1 else ''}"
                
                age_info["detected"] = True
                
                logger.info(f"üïê [Age Extraction] √Çge d√©tect√©: {age_info['text']} ({age_info['days']} jours)")
                break
        
        if not age_info["detected"]:
            logger.warning(f"‚ö†Ô∏è [Age Extraction] Aucun √¢ge d√©tect√© dans: {original_question}")
        
        return age_info

    def _build_complete_enriched_question(
        self, 
        original_question: str, 
        breed: Optional[str], 
        sex: Optional[str], 
        age_info: Dict[str, Any], 
        language: str = "fr"
    ) -> str:
        """Construit une question compl√®tement enrichie (CONSERV√â)"""
        
        # Templates d'enrichissement complet par langue
        templates = {
            "fr": {
                "complete": "Pour des poulets {breed} {sex} de {age}",
                "breed_age": "Pour des poulets {breed} de {age}",
                "sex_age": "Pour des poulets {sex} de {age}",
                "breed_sex": "Pour des poulets {breed} {sex}",
                "age_only": "Pour des poulets de {age}",
                "breed_only": "Pour des poulets {breed}",
                "sex_only": "Pour des poulets {sex}"
            },
            "en": {
                "complete": "For {breed} {sex} chickens at {age}",
                "breed_age": "For {breed} chickens at {age}",
                "sex_age": "For {sex} chickens at {age}",
                "breed_sex": "For {breed} {sex} chickens",
                "age_only": "For chickens at {age}",
                "breed_only": "For {breed} chickens",
                "sex_only": "For {sex} chickens"
            },
            "es": {
                "complete": "Para pollos {breed} {sex} de {age}",
                "breed_age": "Para pollos {breed} de {age}",
                "sex_age": "Para pollos {sex} de {age}",
                "breed_sex": "Para pollos {breed} {sex}",
                "age_only": "Para pollos de {age}",
                "breed_only": "Para pollos {breed}",
                "sex_only": "Para pollos {sex}"
            }
        }
        
        template_set = templates.get(language, templates["fr"])
        
        # Construire le pr√©fixe contextuel avec priorit√© √† l'√¢ge
        context_prefix = ""
        age_text = age_info.get("text") if age_info.get("detected") else None
        
        # PRIORIT√â √Ä L'ENRICHISSEMENT COMPLET
        if breed and sex and age_text:
            context_prefix = template_set["complete"].format(
                breed=breed, sex=sex, age=age_text
            )
            logger.info(f"üåü [Complete Enrichment] Contexte COMPLET: {context_prefix}")
            
        elif breed and age_text:
            context_prefix = template_set["breed_age"].format(
                breed=breed, age=age_text
            )
            logger.info(f"üè∑Ô∏è [Breed+Age] Contexte: {context_prefix}")
            
        elif sex and age_text:
            context_prefix = template_set["sex_age"].format(
                sex=sex, age=age_text
            )
            logger.info(f"‚öß [Sex+Age] Contexte: {context_prefix}")
            
        elif breed and sex:
            context_prefix = template_set["breed_sex"].format(
                breed=breed, sex=sex
            )
            logger.info(f"üè∑Ô∏è‚öß [Breed+Sex] Contexte: {context_prefix}")
            
        elif age_text:
            context_prefix = template_set["age_only"].format(age=age_text)
            logger.info(f"üïê [Age Only] Contexte: {context_prefix}")
            
        elif breed:
            context_prefix = template_set["breed_only"].format(breed=breed)
            logger.info(f"üè∑Ô∏è [Breed Only] Contexte: {context_prefix}")
            
        elif sex:
            context_prefix = template_set["sex_only"].format(sex=sex)
            logger.info(f"‚öß [Sex Only] Contexte: {context_prefix}")
        
        # Construire la question finale enrichie
        if context_prefix:
            # D√©tecter le type de question originale pour int√©gration naturelle
            original_lower = original_question.lower().strip()
            
            if "quel est" in original_lower or "what is" in original_lower or "cu√°l es" in original_lower:
                # Questions directes ‚Üí pr√©fixer
                enriched_question = f"{context_prefix}, {original_lower}"
            elif "comment" in original_lower or "how" in original_lower or "c√≥mo" in original_lower:
                # Questions de m√©thode ‚Üí deux points
                enriched_question = f"{context_prefix}: {original_question}"
            else:
                # Autres questions ‚Üí deux points
                enriched_question = f"{context_prefix}: {original_question}"
            
            logger.info(f"‚ú® [Final Enrichment] Question finale: {enriched_question}")
            return enriched_question
        
        else:
            logger.warning("‚ö†Ô∏è [Enrichment] Pas d'enrichissement possible, question originale conserv√©e")
            return original_question
    
    # === TRAITEMENT EXPERT AVEC RAG-FIRST + AM√âLIORATIONS CORRIG√â (CONSERV√â) ===
    
    async def _process_expert_response_enhanced_corrected(
        self, question_text: str, request_data: EnhancedQuestionRequest,
        request: Request, current_user: Optional[Dict], conversation_id: str,
        processing_steps: list, ai_enhancements_used: list,
        debug_info: Dict, performance_breakdown: Dict, vagueness_result = None
    ) -> Dict[str, Any]:
        """Version RAG parfaitement corrig√©e avec m√©moire intelligente (CONSERV√â)"""
        
        # === 1. R√âCUP√âRATION FORC√âE DU CONTEXTE CONVERSATIONNEL ===
        conversation_context_str = ""
        extracted_entities = {}
        
        if self.integrations.intelligent_memory_available:
            try:
                # R√©cup√©ration forc√©e du contexte depuis la m√©moire intelligente
                context_obj = self.integrations.get_conversation_context(conversation_id)
                if context_obj:
                    conversation_context_str = context_obj.get_context_for_rag(max_chars=800)
                    
                    # Enrichissement sp√©cial si clarification
                    if request_data.is_clarification_response or request_data.original_question:
                        # Ajouter explicitement le contexte de clarification
                        if request_data.original_question:
                            conversation_context_str = f"Question originale: {request_data.original_question}. " + conversation_context_str
                        
                        # Rechercher les infos breed/sex dans les messages r√©cents
                        if hasattr(context_obj, 'messages'):
                            for msg in reversed(context_obj.messages[-5:]):
                                if msg.role == "user" and any(word in msg.message.lower() for word in ["ross", "cobb", "hubbard", "m√¢le", "femelle", "male", "female"]):
                                    conversation_context_str += f" | Clarification: {msg.message}"
                                    break
                    
                    # Entit√©s consolid√©es
                    if hasattr(context_obj, 'consolidated_entities'):
                        extracted_entities = context_obj.consolidated_entities.to_dict()
                    
                    logger.info(f"üß† [Expert Service] Contexte enrichi r√©cup√©r√©: {conversation_context_str[:150]}...")
                    ai_enhancements_used.append("intelligent_memory_context_retrieval")
                else:
                    logger.warning(f"‚ö†Ô∏è [Expert Service] Aucun contexte trouv√© pour: {conversation_id}")
                    
            except Exception as e:
                logger.error(f"‚ùå [Expert Service] Erreur r√©cup√©ration contexte: {e}")
        
        performance_breakdown["context_retrieved"] = int(time.time() * 1000)
        
        # === 2. AM√âLIORATION INTELLIGENTE DE LA QUESTION ===
        enriched_question, enhancement_info = self.rag_enhancer.enhance_question_for_rag(
            question=question_text,
            conversation_context=conversation_context_str,
            language=request_data.language
        )
        
        # Enrichissement suppl√©mentaire si vient d'une clarification
        if request_data.original_question and request_data.is_clarification_response:
            logger.info(f"‚ú® [Expert Service] Question d√©j√† enrichie par clarification: {question_text[:100]}...")
            ai_enhancements_used.append("clarification_based_enrichment")
        
        if enhancement_info["question_enriched"]:
            ai_enhancements_used.append("intelligent_question_enhancement")
            logger.info(f"‚ú® [Expert Service] Question am√©lior√©e: {enriched_question[:150]}...")
        
        if enhancement_info["pronoun_detected"]:
            ai_enhancements_used.append("contextual_pronoun_resolution")
            logger.info(f"üéØ [Expert Service] Pronoms contextuels r√©solus: {enhancement_info['context_entities_used']}")
        
        processing_steps.append("intelligent_question_enhancement")
        performance_breakdown["question_enhanced"] = int(time.time() * 1000)
        
        # === 3. V√âRIFICATION RAG DISPONIBLE ===
        app = request.app
        process_rag = getattr(app.state, 'process_question_with_rag', None)
        
        if not process_rag:
            logger.error("‚ùå [Expert Service] Syst√®me RAG indisponible - Erreur critique")
            raise HTTPException(
                status_code=503, 
                detail="Service RAG indisponible - Le syst√®me expert n√©cessite l'acc√®s √† la base documentaire"
            )
        
        # === 4. APPEL RAG AVEC CONTEXTE FORC√â ===
        try:
            logger.info("üîç [Expert Service] Appel RAG avec contexte intelligent...")
            
            if request_data.debug_mode:
                debug_info["original_question"] = question_text
                debug_info["enriched_question"] = enriched_question
                debug_info["conversation_context"] = conversation_context_str
                debug_info["enhancement_info"] = enhancement_info
            
            # Strat√©gie multi-tentative pour RAG avec contexte
            result = None
            rag_call_method = "unknown"
            
            # üéØ NOUVEAU: Construire prompt structur√© avec contexte
            rag_context = extract_context_from_entities(extracted_entities)
            rag_context["lang"] = request_data.language
            
            # Tentative 1: Avec param√®tre context si support√©
            try:
                # üéØ Appliquer prompt structur√©
                structured_question = build_structured_prompt(
                    documents="[DOCUMENTS_WILL_BE_INSERTED_BY_RAG]",
                    question=enriched_question,
                    context=rag_context
                )
                
                # üìä LOGGING: Debug du prompt final
                logger.debug(f"üîç [Prompt Final RAG] Contexte: {rag_context}")
                logger.debug(f"üîç [Prompt Final RAG]\n{structured_question[:500]}...")
                
                result = await process_rag(
                    question=structured_question,
                    user=current_user,
                    language=request_data.language,
                    speed_mode=request_data.speed_mode,
                    context=conversation_context_str
                )
                rag_call_method = "context_parameter_structured"
                logger.info("‚úÖ [Expert Service] RAG appel√© avec prompt structur√© + contexte")
            except TypeError as te:
                logger.info(f"‚ÑπÔ∏è [Expert Service] Param√®tre context non support√©: {te}")
                
                # Tentative 2: Injection du contexte dans la question
                if conversation_context_str:
                    structured_question = build_structured_prompt(
                        documents="[DOCUMENTS_WILL_BE_INSERTED_BY_RAG]",
                        question=enriched_question,
                        context=rag_context
                    )
                    
                    # üìä LOGGING: Debug du prompt inject√©
                    logger.debug(f"üîç [Prompt Final RAG - Inject√©]\n{structured_question[:500]}...")
                    
                    contextual_question = f"{structured_question}\n\nContexte: {conversation_context_str}"
                    result = await process_rag(
                        question=contextual_question,
                        user=current_user,
                        language=request_data.language,
                        speed_mode=request_data.speed_mode
                    )
                    rag_call_method = "context_injected_structured"
                    logger.info("‚úÖ [Expert Service] RAG appel√© avec prompt structur√© + contexte inject√©")
                else:
                    # Tentative 3: Question enrichie avec prompt structur√©
                    structured_question = build_structured_prompt(
                        documents="[DOCUMENTS_WILL_BE_INSERTED_BY_RAG]",
                        question=enriched_question,
                        context=rag_context
                    )
                    
                    # üìä LOGGING: Debug du prompt seul
                    logger.debug(f"üîç [Prompt Final RAG - Seul]\n{structured_question[:500]}...")
                    
                    result = await process_rag(
                        question=structured_question,
                        user=current_user,
                        language=request_data.language,
                        speed_mode=request_data.speed_mode
                    )
                    rag_call_method = "structured_only"
                    logger.info("‚úÖ [Expert Service] RAG appel√© avec prompt structur√© seul")
            
            performance_breakdown["rag_complete"] = int(time.time() * 1000)
            
            # === 5. TRAITEMENT R√âSULTAT RAG ===
            answer = str(result.get("response", ""))
            
            # üßπ IMPORTANT: Ici on ne nettoie que les r√©f√©rences documents
            # La concision sera appliqu√©e plus tard dans le processus principal
            answer = self.concision_processor._clean_document_references_only(answer)
            
            rag_score = result.get("score", 0.0)
            original_mode = result.get("mode", "rag_processing")
            
            # Validation qualit√©
            quality_check = self._validate_rag_response_quality(
                answer, enriched_question, enhancement_info
            )
            
            if not quality_check["valid"]:
                logger.warning(f"‚ö†Ô∏è [Expert Service] Qualit√© RAG insuffisante: {quality_check['reason']}")
                ai_enhancements_used.append("quality_validation_failed")
            
            logger.info(f"‚úÖ [Expert Service] RAG r√©ponse re√ßue: {len(answer)} caract√®res, score: {rag_score}")
            
            # Mode enrichi avec m√©thode d'appel
            mode = f"enhanced_contextual_{original_mode}_{rag_call_method}_corrected_with_concision_and_response_versions"
            
            processing_steps.append("mandatory_rag_with_intelligent_context")
            
            return {
                "answer": answer,
                "rag_used": True,
                "rag_score": rag_score,
                "mode": mode,
                "context_used": bool(conversation_context_str),
                "question_enriched": enhancement_info["question_enriched"] or bool(request_data.original_question),
                "enhancement_info": enhancement_info,
                "quality_check": quality_check,
                "extracted_entities": extracted_entities,
                "rag_call_method": rag_call_method
            }
            
        except Exception as rag_error:
            logger.error(f"‚ùå [Expert Service] Erreur critique RAG: {rag_error}")
            processing_steps.append("rag_error")
            
            error_details = {
                "error": "Erreur RAG",
                "message": "Impossible d'interroger la base documentaire",
                "question_original": question_text,
                "question_enriched": enriched_question,
                "context_available": bool(conversation_context_str),
                "technical_error": str(rag_error)
            }
            
            raise HTTPException(status_code=503, detail=error_details)
    
    # === M√âTHODES UTILITAIRES AVEC CONCISION + RESPONSE VERSIONS ===
    
    def _create_vagueness_response(
        self, vagueness_result, question_text: str, conversation_id: str,
        language: str, start_time: float, processing_steps: list, ai_enhancements_used: list
    ) -> EnhancedExpertResponse:
        """Cr√©e une r√©ponse sp√©cialis√©e pour questions floues (CONSERV√â)"""
        
        clarification_messages = {
            "fr": f"Votre question semble manquer de pr√©cision. {vagueness_result.suggested_clarification or 'Pouvez-vous √™tre plus sp√©cifique ?'}",
            "en": f"Your question seems to lack precision. {vagueness_result.suggested_clarification or 'Could you be more specific?'}",
            "es": f"Su pregunta parece carecer de precisi√≥n. {vagueness_result.suggested_clarification or '¬øPodr√≠a ser m√°s espec√≠fico?'}"
        }
        
        response_message = clarification_messages.get(language, clarification_messages["fr"])
        
        # Ajouter des exemples de questions
        if vagueness_result.question_clarity in ["very_unclear", "unclear"]:
            examples = {
                "fr": "\n\nExemples de questions pr√©cises:\n‚Ä¢ Quel est le poids normal d'un Ross 308 de 21 jours?\n‚Ä¢ Comment traiter la mortalit√© √©lev√©e chez des poulets de 3 semaines?\n‚Ä¢ Quelle temp√©rature maintenir pour des poussins de 7 jours?",
                "en": "\n\nExamples of precise questions:\n‚Ä¢ What is the normal weight of a 21-day Ross 308?\n‚Ä¢ How to treat high mortality in 3-week-old chickens?\n‚Ä¢ What temperature to maintain for 7-day chicks?",
                "es": "\n\nEjemplos de preguntas precisas:\n‚Ä¢ ¬øCu√°l es el peso normal de un Ross 308 de 21 d√≠as?\n‚Ä¢ ¬øC√≥mo tratar la alta mortalidad en pollos de 3 semanas?\n‚Ä¢ ¬øQu√© temperatura mantener para pollitos de 7 d√≠as?"
            }
            response_message += examples.get(language, examples["fr"])
        
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return EnhancedExpertResponse(
            question=question_text,
            response=response_message,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode="vagueness_clarification",
            user=None,
            logged=True,
            validation_passed=True,
            vagueness_detection=vagueness_result,
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used
        )
    
    def _build_final_enhanced_response(
        self, question_text: str, answer: str, conversation_id: str,
        user_email: Optional[str], language: str, response_time_ms: int,
        expert_result: Dict, validation_result: ValidationResult,
        conversation_context: Any, processing_steps: list,
        ai_enhancements_used: list, request_data: EnhancedQuestionRequest,
        debug_info: Dict, performance_breakdown: Dict
    ) -> EnhancedExpertResponse:
        """Construit la r√©ponse finale avec toutes les am√©liorations + concision + response_versions"""
        
        # M√©triques finales
        extracted_entities = expert_result.get("extracted_entities")
        confidence_overall = None
        conversation_state = None
        
        if conversation_context and hasattr(conversation_context, 'consolidated_entities'):
            if not extracted_entities:
                extracted_entities = conversation_context.consolidated_entities.to_dict()
            confidence_overall = conversation_context.consolidated_entities.confidence_overall
            conversation_state = conversation_context.conversation_urgency
        
        # Informations de debug si activ√©es
        final_debug_info = None
        final_performance = None
        
        if request_data.debug_mode:
            final_debug_info = {
                **debug_info,
                "total_processing_time_ms": response_time_ms,
                "ai_enhancements_count": len(ai_enhancements_used),
                "processing_steps_count": len(processing_steps),
                "concision_applied": expert_result.get("concision_applied", False),
                "response_versions_support": True  # üöÄ NOUVEAU
            }
            
            final_performance = performance_breakdown
        
        # ‚úÖ CONSERV√â: Informations de concision dans la r√©ponse
        concision_info = {
            "concision_applied": expert_result.get("concision_applied", False),
            "original_response_available": "original_answer" in expert_result,
            "detected_question_type": None,
            "applied_concision_level": None,
            "response_versions_supported": True  # üöÄ NOUVEAU
        }
        
        if expert_result.get("concision_applied"):
            concision_info["detected_question_type"] = self.concision_processor.detect_question_type(question_text)
            concision_info["applied_concision_level"] = self.concision_processor.detect_optimal_concision_level(question_text).value
            
        return EnhancedExpertResponse(
            # Champs existants conserv√©s
            question=str(question_text),
            response=str(answer),
            conversation_id=conversation_id,
            rag_used=expert_result["rag_used"],
            rag_score=expert_result["rag_score"],
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode=expert_result["mode"],
            user=user_email,
            logged=True,
            validation_passed=True,
            validation_confidence=validation_result.confidence,
            reprocessed_after_clarification=request_data.is_clarification_response,
            conversation_state=conversation_state,
            extracted_entities=extracted_entities,
            confidence_overall=confidence_overall,
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used,
            
            # Fonctionnalit√©s existantes conserv√©es
            document_relevance=expert_result.get("document_relevance"),
            context_coherence=expert_result.get("context_coherence"),
            vagueness_detection=None,
            fallback_details=None,
            response_format_applied=request_data.expected_response_format.value,
            quality_metrics=expert_result.get("quality_metrics"),
            debug_info=final_debug_info,
            performance_breakdown=final_performance,
            
            # ‚úÖ CONSERV√â: Informations de concision
            concision_info=concision_info,
            original_response=expert_result.get("original_answer"),  # R√©ponse originale si concision appliqu√©e
            
            # üöÄ NOUVEAU: Support response_versions (sera ajout√© par la m√©thode principale)
            response_versions=None,  # Sera rempli par process_expert_question si generate_all_versions=True
            concision_metrics=None   # Sera rempli par process_expert_question si generate_all_versions=True
        )
    
    # === M√âTHODES UTILITAIRES IDENTIQUES (CONSERV√âES) ===
    
    def _extract_user_id(self, current_user: Optional[Dict], request_data: EnhancedQuestionRequest, request: Request) -> str:
        if current_user:
            return current_user.get("user_id") or request_data.user_id or "authenticated_user"
        return request_data.user_id or get_user_id_from_request(request)
    
    def _get_or_create_conversation_id(self, request_data: EnhancedQuestionRequest) -> str:
        if request_data.conversation_id and request_data.conversation_id.strip():
            conversation_id = request_data.conversation_id.strip()
            logger.info(f"üîÑ [Expert Service] CONTINUATION: {conversation_id}")
            return conversation_id
        else:
            conversation_id = str(uuid.uuid4())
            logger.info(f"üÜï [Expert Service] NOUVELLE: {conversation_id}")
            return conversation_id
    
    def _validate_rag_response_quality(
        self, answer: str, enriched_question: str, enhancement_info: Dict
    ) -> Dict[str, any]:
        """Valide la qualit√© de la r√©ponse RAG (CONSERV√â)"""
        
        if not answer or len(answer.strip()) < 20:
            return {
                "valid": False,
                "reason": "R√©ponse trop courte",
                "answer_length": len(answer) if answer else 0
            }
        
        negative_responses = [
            "je ne sais pas", "i don't know", "no s√©",
            "pas d'information", "no information", "sin informaci√≥n"
        ]
        
        answer_lower = answer.lower()
        for negative in negative_responses:
            if negative in answer_lower:
                return {
                    "valid": False,
                    "reason": f"R√©ponse n√©gative d√©tect√©e: {negative}",
                    "answer_length": len(answer)
                }
        
        if any(word in enriched_question.lower() for word in ["poids", "weight", "peso"]):
            if not re.search(r'\d+', answer):
                return {
                    "valid": False,
                    "reason": "Question num√©rique mais pas de chiffres dans la r√©ponse",
                    "answer_length": len(answer)
                }
        
        return {
            "valid": True,
            "reason": "R√©ponse valide",
            "answer_length": len(answer)
        }
    
    # === AUTRES M√âTHODES CONSERV√âES ===
    
    async def _validate_agricultural_question(self, question: str, language: str, user_id: str, request_ip: str, conversation_id: str) -> ValidationResult:
        if not self.integrations.agricultural_validator_available:
            return ValidationResult(is_valid=False, rejection_message="Service temporairement indisponible")
        
        try:
            enriched_question = question
            if self.integrations.intelligent_memory_available:
                try:
                    rag_context = self.integrations.get_context_for_rag(conversation_id)
                    if rag_context:
                        enriched_question = f"{question}\n\nContexte: {rag_context}"
                except Exception:
                    pass
            
            validation_result = self.integrations.validate_agricultural_question(
                question=enriched_question, language=language, user_id=user_id, request_ip=request_ip
            )
            
            return ValidationResult(
                is_valid=validation_result.is_valid,
                rejection_message=validation_result.reason or "Question hors domaine agricole",
                confidence=validation_result.confidence
            )
            
        except Exception as e:
            logger.error(f"‚ùå [Expert Service] Erreur validateur: {e}")
            return ValidationResult(is_valid=False, rejection_message="Erreur de validation")
    
    def _create_rejection_response(self, question_text, validation_result, conversation_id, user_email, language, start_time, processing_steps, ai_enhancements_used, vagueness_result=None):
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return EnhancedExpertResponse(
            question=str(question_text),
            response=str(validation_result.rejection_message),
            conversation_id=conversation_id,
            rag_used=False,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode="enhanced_agricultural_validation_rejected",
            user=user_email,
            logged=True,
            validation_passed=False,
            validation_confidence=validation_result.confidence,
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used,
            vagueness_detection=vagueness_result
        )
    
    async def process_feedback(self, feedback_data: FeedbackRequest) -> Dict[str, Any]:
        feedback_updated = False
        
        if feedback_data.conversation_id and self.integrations.logging_available:
            try:
                rating_numeric = {"positive": 1, "negative": -1, "neutral": 0}.get(feedback_data.rating, 0)
                feedback_updated = await self.integrations.update_feedback(feedback_data.conversation_id, rating_numeric)
            except Exception as e:
                logger.error(f"‚ùå [Expert Service] Erreur feedback: {e}")
        
        return {
            "success": True,
            "message": "Feedback enregistr√© avec succ√®s (Enhanced + Concision + Response Versions)",
            "rating": feedback_data.rating,
            "comment": feedback_data.comment,
            "conversation_id": feedback_data.conversation_id,
            "feedback_updated_in_db": feedback_updated,
            "enhanced_features_used": True,
            "concision_system_active": self.concision_processor.config.ENABLE_CONCISE_RESPONSES,
            "response_versions_supported": True,  # üöÄ NOUVEAU
            "timestamp": datetime.now().isoformat()
        }
    
    async def get_suggested_topics(self, language: str) -> Dict[str, Any]:
        lang = language.lower() if language else "fr"
        if lang not in ["fr", "en", "es"]:
            lang = "fr"
        
        topics_by_language = get_enhanced_topics_by_language()
        topics = topics_by_language.get(lang, topics_by_language["fr"])
        
        return {
            "topics": topics,
            "language": lang,
            "count": len(topics),
            "enhanced_features": {
                "vagueness_detection_available": True,
                "context_coherence_available": True,
                "detailed_rag_scoring_available": True,
                "quality_metrics_available": True,
                "smart_clarification_available": True,
                "intelligent_memory_available": self.integrations.intelligent_memory_available,
                
                # ‚úÖ CONSERV√â: Informations syst√®me de concision
                "response_concision_available": True,
                "concision_levels": [level.value for level in ConcisionLevel],
                "auto_concision_detection": True,
                "concision_enabled": self.concision_processor.config.ENABLE_CONCISE_RESPONSES,
                
                # üöÄ NOUVEAU: Informations response_versions
                "response_versions_available": True,
                "multiple_concision_levels_generation": True,
                "dynamic_level_switching_support": True,
                "concision_metrics_available": True
            },
            "system_status": {
                "validation_enabled": self.integrations.is_agricultural_validation_enabled(),
                "enhanced_clarification_enabled": self.integrations.is_enhanced_clarification_enabled(),
                "intelligent_memory_enabled": self.integrations.intelligent_memory_available,
                "api_enhancements_enabled": True,
                "concision_processor_enabled": True,
                "response_versions_generator_enabled": True  # üöÄ NOUVEAU
            },
            
            # ‚úÖ CONSERV√â: Configuration concision par d√©faut
            "concision_config": {
                "default_level": self.concision_processor.config.DEFAULT_CONCISION_LEVEL.value,
                "auto_detect_enabled": True,
                "max_lengths": self.concision_processor.config.MAX_RESPONSE_LENGTH,
                "ultra_concise_keywords": self.concision_processor.config.ULTRA_CONCISE_KEYWORDS,
                "complex_keywords": self.concision_processor.config.COMPLEX_KEYWORDS,
                
                # üöÄ NOUVEAU: Support response_versions
                "response_versions_generation": {
                    "enabled": True,
                    "supported_levels": [level.value for level in ConcisionLevel],
                    "metrics_included": True,
                    "cache_supported": False,  # Le syst√®me existant ne cache pas
                    "fallback_strategy": "simple_truncation"
                }
            }
        }

# =============================================================================
# üÜï API ENDPOINT POUR CONTR√îLER LA CONCISION + RESPONSE VERSIONS (OPTIONNEL)
# =============================================================================

def create_concision_control_endpoint():
    """
    Endpoint optionnel pour contr√¥ler la concision c√¥t√© backend
    üöÄ MODIFI√â: Ajout support response_versions
    """
    
    from fastapi import APIRouter
    from pydantic import BaseModel
    
    router = APIRouter()
    
    class ConcisionSettingsRequest(BaseModel):
        enabled: bool = True
        default_level: ConcisionLevel = ConcisionLevel.CONCISE
        max_lengths: Optional[Dict[str, int]] = None
        enable_response_versions: bool = True  # üöÄ NOUVEAU
    
    class ConcisionSettingsResponse(BaseModel):
        success: bool
        current_settings: Dict[str, Any]
        message: str
    
    @router.post("/concision/settings", response_model=ConcisionSettingsResponse)
    async def update_concision_settings(request: ConcisionSettingsRequest):
        """Mettre √† jour les param√®tres de concision + response_versions du syst√®me"""
        
        try:
            # Mettre √† jour la configuration globale
            ConcisionConfig.ENABLE_CONCISE_RESPONSES = request.enabled
            ConcisionConfig.DEFAULT_CONCISION_LEVEL = request.default_level
            
            if request.max_lengths:
                ConcisionConfig.MAX_RESPONSE_LENGTH.update(request.max_lengths)
            
            return ConcisionSettingsResponse(
                success=True,
                current_settings={
                    "enabled": ConcisionConfig.ENABLE_CONCISE_RESPONSES,
                    "default_level": ConcisionConfig.DEFAULT_CONCISION_LEVEL.value,
                    "max_lengths": ConcisionConfig.MAX_RESPONSE_LENGTH,
                    "response_versions_enabled": request.enable_response_versions  # üöÄ NOUVEAU
                },
                message="Param√®tres de concision + response_versions mis √† jour avec succ√®s"
            )
        except Exception as e:
            return ConcisionSettingsResponse(
                success=False,
                current_settings={},
                message=f"Erreur lors de la mise √† jour: {str(e)}"
            )
    
    @router.get("/concision/settings", response_model=Dict[str, Any])
    async def get_concision_settings():
        """R√©cup√©rer les param√®tres actuels de concision + response_versions"""
        
        return {
            "enabled": ConcisionConfig.ENABLE_CONCISE_RESPONSES,
            "default_level": ConcisionConfig.DEFAULT_CONCISION_LEVEL.value,
            "available_levels": [level.value for level in ConcisionLevel],
            "max_lengths": ConcisionConfig.MAX_RESPONSE_LENGTH,
            "ultra_concise_keywords": ConcisionConfig.ULTRA_CONCISE_KEYWORDS,
            "complex_keywords": ConcisionConfig.COMPLEX_KEYWORDS,
            
            # üöÄ NOUVEAU: Configuration response_versions
            "response_versions": {
                "supported": True,
                "generation_method": "existing_processor_based",
                "cache_enabled": False,
                "fallback_enabled": True,
                "metrics_included": True
            }
        }
    
    return router

# =============================================================================
# CONFIGURATION FINALE AVEC CONCISION + RESPONSE VERSIONS
# =============================================================================

logger.info("üöÄ" * 30)
logger.info("üöÄ [EXPERT SERVICES] VERSION 3.7.0 - RESPONSE_VERSIONS INT√âGR√â!")
logger.info("üöÄ [INT√âGRATION] Syst√®me concision existant + response_versions:")
logger.info("   ‚úÖ ResponseVersionsGenerator utilise ResponseConcisionProcessor existant")
logger.info("   ‚úÖ G√©n√©ration 4 versions: ultra_concise, concise, standard, detailed")
logger.info("   ‚úÖ S√©lection automatique selon concision_level")
logger.info("   ‚úÖ M√©triques d√©taill√©es avec ConcisionMetrics")
logger.info("   ‚úÖ Compatible avec toute la logique existante")
logger.info("   ‚úÖ Fallback automatique si erreur")
logger.info("   ‚úÖ Support generate_all_versions flag")
logger.info("üöÄ [BACKEND READY] Frontend peut maintenant:")
logger.info("   - Demander concision_level sp√©cifique")
logger.info("   - Recevoir response_versions compl√®tes") 
logger.info("   - Changer niveau dynamiquement c√¥t√© frontend")
logger.info("   - Profiter du cache et performance optimis√©e")
logger.info("üöÄ" * 30)

logger.info("‚úÖ [Expert Service] Services m√©tier EXPERT SYSTEM + SYST√àME DE CONCISION + RESPONSE VERSIONS int√©gr√©")
logger.info("üöÄ [Expert Service] NOUVELLES FONCTIONNALIT√âS V3.7.0:")
logger.info("   - ‚úÖ Syst√®me de concision intelligent multi-niveaux (CONSERV√â)")
logger.info("   - ‚úÖ D√©tection automatique type de question (CONSERV√â)")
logger.info("   - ‚úÖ Nettoyage avanc√© verbosit√© + r√©f√©rences documents (CONSERV√â)")
logger.info("   - ‚úÖ Configuration flexible par type de question (CONSERV√â)")
logger.info("   - ‚úÖ Conservation r√©ponse originale si concision appliqu√©e (CONSERV√â)")
logger.info("   - üöÄ ResponseVersionsGenerator avec syst√®me existant")
logger.info("   - üöÄ G√©n√©ration toutes versions (ultra_concise, concise, standard, detailed)")
logger.info("   - üöÄ ConcisionMetrics avec compression ratios et m√©triques")
logger.info("   - üöÄ Support generate_all_versions flag pour contr√¥le frontend")
logger.info("   - üöÄ S√©lection intelligente version selon concision_level")
logger.info("üîß [Expert Service] FONCTIONNALIT√âS CONSERV√âES:")
logger.info("   - ‚úÖ Syst√®me de clarification intelligent complet")
logger.info("   - ‚úÖ M√©moire conversationnelle enrichie")
logger.info("   - ‚úÖ RAG avec contexte et prompt structur√©")
logger.info("   - ‚úÖ Multi-LLM support et validation agricole")
logger.info("   - ‚úÖ Support multilingue FR/EN/ES")
logger.info("üéØ [Expert Service] R√âSULTATS ATTENDUS:")
logger.info('   - Question: "Quel est le poids d\'un poulet Ross 308 m√¢le de 18 jours ?"')
logger.info('   - response_versions["ultra_concise"]: "410-450g"')
logger.info('   - response_versions["concise"]: "Le poids se situe entre 410g et 450g."')
logger.info('   - response_versions["standard"]: R√©ponse normale sans conseils excessifs')
logger.info('   - response_versions["detailed"]: Version compl√®te originale')
logger.info('   - response (s√©lection): Selon concision_level demand√©')
logger.info('   - concision_metrics: M√©triques g√©n√©ration et compression')
logger.info("‚úÖ [Expert Service] SYST√àME COMPLET RESPONSE_VERSIONS PARFAITEMENT INT√âGR√â!")