"""
app/api/v1/expert_services.py - SERVICES M√âTIER EXPERT SYSTEM AVEC AUTO CLARIFICATION SIMPLIFI√âE + VALIDATION ROBUSTE + FALLBACK INTELLIGENT

üö® MODIFICATIONS APPLIQU√âES VERSION 3.11.0:
1. ‚úÖ Syst√®me de concision des r√©ponses int√©gr√© (CONSERV√â)
2. ‚úÖ Nettoyage avanc√© verbosit√© + r√©f√©rences documents (CONSERV√â)
3. ‚úÖ Configuration flexible par type de question (CONSERV√â)
4. ‚úÖ D√©tection automatique niveau de concision requis (CONSERV√â)
5. ‚úÖ Conservation de toutes les fonctionnalit√©s existantes (CONSERV√â)
6. üöÄ ResponseVersionsGenerator int√©gr√© (CONSERV√â)
7. üöÄ G√©n√©ration de toutes les versions (ultra_concise, concise, standard, detailed) (CONSERV√â)
8. üöÄ Support ConcisionMetrics avec m√©triques d√©taill√©es (CONSERV√â)
9. üöÄ S√©lection automatique selon concision_level (CONSERV√â)
10. üöÄ Support generate_all_versions flag pour frontend (CONSERV√â)
11. üè∑Ô∏è Filtrage taxonomique intelligent des documents RAG (CONSERV√â)
12. üè∑Ô∏è D√©tection automatique broiler/layer/swine/dairy/general (CONSERV√â)
13. üè∑Ô∏è Enhancement questions avec contexte taxonomique (CONSERV√â)
14. üè∑Ô∏è Filtres RAG adaptatifs selon la taxonomie d√©tect√©e (CONSERV√â)
15. üÜï Mode s√©mantique dynamique de clarification int√©gr√© (CONSERV√â)
16. üÜï G√©n√©ration intelligente de questions contextuelles via GPT (CONSERV√â)
17. üÜï Support param√®tre semantic_dynamic_mode dans les requ√™tes (CONSERV√â)
18. üîß MODIFI√â: Auto-clarification simplifi√©e avec validation robuste
19. üîß MODIFI√â: Scoring et validation qualit√© des questions g√©n√©r√©es
20. üîß MODIFI√â: Int√©gration visible dans expert_services avec auto_clarify_if_needed
21. üîß MODIFI√â: Fallback lisible si GPT √©choue compl√®tement

FONCTIONNALIT√âS CONSERV√âES:
- ‚úÖ Syst√®me de clarification intelligent complet
- ‚úÖ M√©moire conversationnelle
- ‚úÖ RAG avec contexte enrichi
- ‚úÖ Multi-LLM support
- ‚úÖ Validation agricole
- ‚úÖ Support multilingue
"""

import os
import logging
import uuid
import time
import re
from datetime import datetime
from typing import Optional, Dict, Any, Tuple, List
from enum import Enum

from fastapi import HTTPException, Request

from .expert_models import (
    EnhancedQuestionRequest, EnhancedExpertResponse, FeedbackRequest,
    ValidationResult, ProcessingContext, VaguenessResponse, ResponseFormat,
    ConcisionLevel, ConcisionMetrics, DynamicClarification
)
from .expert_utils import (
    get_user_id_from_request, 
    build_enriched_question_from_clarification,
    get_enhanced_topics_by_language,
    save_conversation_auto_enhanced,
    extract_breed_and_sex_from_clarification,
    build_enriched_question_with_breed_sex,
    validate_clarification_completeness
)
from .expert_integrations import IntegrationsManager
from .api_enhancement_service import APIEnhancementService
from .prompt_templates import build_structured_prompt, extract_context_from_entities, validate_prompt_context, build_clarification_prompt
from .concision_service import concision_service

logger = logging.getLogger(__name__)

# =============================================================================
# üÜï SYST√àME DE CONCISION DES R√âPONSES (CONSERV√â IDENTIQUE)
# =============================================================================

class ConcisionLevel(Enum):
    """Niveaux de concision disponibles"""
    ULTRA_CONCISE = "ultra_concise"
    CONCISE = "concise"
    STANDARD = "standard"
    DETAILED = "detailed"

class ConcisionConfig:
    """Configuration du syst√®me de concision"""
    
    ENABLE_CONCISE_RESPONSES = True
    DEFAULT_CONCISION_LEVEL = ConcisionLevel.CONCISE
    
    MAX_RESPONSE_LENGTH = {
        "weight_question": 80,
        "temperature_question": 60,
        "measurement_question": 70,
        "general_question": 150,
        "complex_question": 300
    }
    
    ULTRA_CONCISE_KEYWORDS = [
        "poids", "weight", "peso",
        "temp√©rature", "temperature", "temperatura",
        "combien", "how much", "cu√°nto",
        "quel est", "what is", "cu√°l es"
    ]
    
    COMPLEX_KEYWORDS = [
        "comment", "how to", "c√≥mo",
        "pourquoi", "why", "por qu√©", 
        "expliquer", "explain", "explicar",
        "proc√©dure", "procedure", "procedimiento",
        "protocole", "protocol", "protocolo"
    ]

class ResponseConcisionProcessor:
    """Processeur de concision des r√©ponses (CONSERV√â IDENTIQUE)"""
    
    def __init__(self):
        self.config = ConcisionConfig()
        logger.info("‚úÖ [Concision] Processeur de concision initialis√©")
    
    def detect_question_type(self, question: str) -> str:
        """D√©tecte le type de question pour appliquer les bonnes r√®gles"""
        
        question_lower = question.lower().strip()
        
        weight_keywords = ["poids", "weight", "peso", "grammes", "grams", "gramos", "kg"]
        if any(word in question_lower for word in weight_keywords):
            return "weight_question"
        
        temp_keywords = ["temp√©rature", "temperature", "temperatura", "¬∞c", "degr√©", "degree"]
        if any(word in question_lower for word in temp_keywords):
            return "temperature_question"
        
        measurement_keywords = ["taille", "size", "tama√±o", "longueur", "length", "hauteur", "height"]
        if any(word in question_lower for word in measurement_keywords):
            return "measurement_question"
        
        if any(word in question_lower for word in self.config.COMPLEX_KEYWORDS):
            return "complex_question"
        
        return "general_question"
    
    def detect_optimal_concision_level(self, question: str, user_preference: Optional[ConcisionLevel] = None) -> ConcisionLevel:
        """D√©tecte le niveau de concision optimal pour une question"""
        
        if user_preference:
            return user_preference
        
        question_lower = question.lower().strip()
        
        if any(keyword in question_lower for keyword in self.config.ULTRA_CONCISE_KEYWORDS):
            return ConcisionLevel.ULTRA_CONCISE
        
        if any(keyword in question_lower for keyword in self.config.COMPLEX_KEYWORDS):
            return ConcisionLevel.DETAILED
        
        return self.config.DEFAULT_CONCISION_LEVEL
    
    def apply_concision(
        self, 
        response: str, 
        question: str, 
        concision_level: ConcisionLevel,
        language: str = "fr"
    ) -> str:
        """
        üöÄ M√©thode unified pour appliquer la concision
        Utilis√©e par ResponseVersionsGenerator
        """
        
        if not self.config.ENABLE_CONCISE_RESPONSES:
            return response
        
        logger.info(f"üéØ [Concision] Application niveau {concision_level.value}")
        
        if concision_level == ConcisionLevel.ULTRA_CONCISE:
            return self._extract_essential_info(response, question, language)
        elif concision_level == ConcisionLevel.CONCISE:
            return self._make_concise(response, question, language)
        elif concision_level == ConcisionLevel.STANDARD:
            return self._remove_excessive_advice(response, language)
        else:
            return self._clean_document_references_only(response)
    
    def process_response(
        self, 
        response: str, 
        question: str, 
        concision_level: Optional[ConcisionLevel] = None,
        language: str = "fr"
    ) -> str:
        """Traite une r√©ponse selon le niveau de concision demand√© (M√âTHODE CONSERV√âE)"""
        
        if not self.config.ENABLE_CONCISE_RESPONSES:
            return response
        
        level = concision_level or self.detect_optimal_concision_level(question)
        
        return self.apply_concision(response, question, level, language)
    
    def _extract_essential_info(self, response: str, question: str, language: str = "fr") -> str:
        """Extrait uniquement l'information essentielle (mode ultra-concis)"""
        
        question_lower = question.lower()
        
        if any(word in question_lower for word in ["poids", "weight", "peso"]):
            weight_patterns = [
                r'(?:entre\s+)?(\d+(?:-\d+|[^\d]*\d+)?)\s*(?:grammes?|g\b)',
                r'(\d+)\s*(?:√†|to|a)\s*(\d+)\s*(?:grammes?|g\b)',
                r'(\d+)\s*(?:grammes?|g\b)'
            ]
            
            for pattern in weight_patterns:
                match = re.search(pattern, response, re.IGNORECASE)
                if match:
                    if len(match.groups()) >= 2:
                        return f"{match.group(1)}-{match.group(2)}g"
                    else:
                        value = match.group(1)
                        if "entre" in response.lower() or "-" in value:
                            return f"{value}g"
                        else:
                            return f"~{value}g"
        
        if any(word in question_lower for word in ["temp√©rature", "temperature"]):
            temp_patterns = [
                r'(\d+(?:-\d+)?)\s*(?:¬∞C|degr√©s?|degrees?)',
                r'(\d+)\s*(?:√†|to|a)\s*(\d+)\s*(?:¬∞C|degr√©s?)'
            ]
            
            for pattern in temp_patterns:
                match = re.search(pattern, response, re.IGNORECASE)
                if match:
                    if len(match.groups()) >= 2:
                        return f"{match.group(1)}-{match.group(2)}¬∞C"
                    else:
                        return f"{match.group(1)}¬∞C"
        
        sentences = response.split('.')
        for sentence in sentences:
            if re.search(r'\d+', sentence) and len(sentence.strip()) > 10:
                return sentence.strip() + '.'
        
        if sentences:
            return sentences[0].strip() + '.'
        
        return response
    
    def _make_concise(self, response: str, question: str, language: str = "fr") -> str:
        """Rend concis (enl√®ve conseils mais garde info principale)"""
        
        cleaned = self._clean_document_references_only(response)
        
        verbose_patterns = [
            r'\.?\s*Il est essentiel de[^.]*\.',
            r'\.?\s*Assurez-vous de[^.]*\.',
            r'\.?\s*N\'h√©sitez pas √†[^.]*\.',
            r'\.?\s*Pour garantir[^.]*\.',
            r'\.?\s*Il est important de[^.]*\.',
            r'\.?\s*Veillez √†[^.]*\.',
            r'\.?\s*Il convient de[^.]*\.',
            r'\.?\s*√Ä ce stade[^.]*\.',
            r'\.?\s*En cas de doute[^.]*\.',
            r'\.?\s*pour favoriser le bien-√™tre[^.]*\.',
            r'\.?\s*en termes de[^.]*\.',
            r'\.?\s*It is essential to[^.]*\.',
            r'\.?\s*Make sure to[^.]*\.',
            r'\.?\s*Don\'t hesitate to[^.]*\.',
            r'\.?\s*To ensure[^.]*\.',
            r'\.?\s*It is important to[^.]*\.',
            r'\.?\s*Be sure to[^.]*\.',
            r'\.?\s*At this stage[^.]*\.',
            r'\.?\s*Es esencial[^.]*\.',
            r'\.?\s*Aseg√∫rese de[^.]*\.',
            r'\.?\s*No dude en[^.]*\.',
            r'\.?\s*Para garantizar[^.]*\.',
            r'\.?\s*Es importante[^.]*\.',
            r'\.?\s*En esta etapa[^.]*\.',
        ]
        
        for pattern in verbose_patterns:
            cleaned = re.sub(pattern, '.', cleaned, flags=re.IGNORECASE)
        
        cleaned = re.sub(r'\.+', '.', cleaned)
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = cleaned.strip()
        
        if any(word in question.lower() for word in ['poids', 'weight', 'peso']) and len(cleaned) > 100:
            weight_sentence = self._extract_weight_sentence(cleaned)
            if weight_sentence:
                return weight_sentence
        
        return cleaned
    
    def _remove_excessive_advice(self, response: str, language: str = "fr") -> str:
        """Enl√®ve seulement les conseils excessifs (mode standard)"""
        
        cleaned = self._clean_document_references_only(response)
        
        excessive_patterns = [
            r'\.?\s*N\'h√©sitez pas √†[^.]*\.',
            r'\.?\s*Pour des conseils plus personnalis√©s[^.]*\.',
            r'\.?\s*Don\'t hesitate to[^.]*\.',
            r'\.?\s*For more personalized advice[^.]*\.',
            r'\.?\s*No dude en[^.]*\.',
            r'\.?\s*Para consejos m√°s personalizados[^.]*\.',
        ]
        
        for pattern in excessive_patterns:
            cleaned = re.sub(pattern, '.', cleaned, flags=re.IGNORECASE)
        
        return re.sub(r'\.+', '.', cleaned).replace(r'\s+', ' ').strip()
    
    def _clean_document_references_only(self, response_text: str) -> str:
        """Nettoie uniquement les r√©f√©rences aux documents (version originale)"""
        
        if not response_text:
            return response_text
        
        patterns_to_remove = [
            r'selon le document \d+,?\s*',
            r'd\'apr√®s le document \d+,?\s*',
            r'le document \d+ indique que\s*',
            r'comme mentionn√© dans le document \d+,?\s*',
            r'tel que d√©crit dans le document \d+,?\s*',
            r'according to document \d+,?\s*',
            r'as stated in document \d+,?\s*',
            r'document \d+ indicates that\s*',
            r'as mentioned in document \d+,?\s*',
            r'seg√∫n el documento \d+,?\s*',
            r'como se indica en el documento \d+,?\s*',
            r'el documento \d+ menciona que\s*',
            r'\(document \d+\)',
            r'\[document \d+\]',
            r'source:\s*document \d+',
            r'ref:\s*document \d+'
        ]
        
        cleaned_response = response_text
        
        for pattern in patterns_to_remove:
            cleaned_response = re.sub(
                pattern, 
                '', 
                cleaned_response, 
                flags=re.IGNORECASE
            )
        
        cleaned_response = re.sub(r'\s+', ' ', cleaned_response)
        cleaned_response = cleaned_response.strip()
        
        if cleaned_response and cleaned_response[0].islower():
            cleaned_response = cleaned_response[0].upper() + cleaned_response[1:]
        
        return cleaned_response
    
    def _extract_weight_sentence(self, text: str) -> Optional[str]:
        """Extrait la phrase principale contenant l'information de poids"""
        
        weight_patterns = [
            r'[^.]*\d+[^.]*(?:grammes?|g\b|kg|livres?|pounds?)[^.]*\.',
            r'[^.]*(?:entre|between|entre)\s+\d+[^.]*\d+[^.]*\.',
            r'[^.]*(?:poids|weight|peso)[^.]*\d+[^.]*\.'
        ]
        
        for pattern in weight_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                sentence = match.group(0).strip()
                sentence = re.sub(r'^\W+', '', sentence)
                if sentence and sentence[0].islower():
                    sentence = sentence[0].upper() + sentence[1:]
                return sentence
        
        sentences = text.split('.')
        if sentences:
            first_sentence = sentences[0].strip() + '.'
            return first_sentence
        
        return None

# =============================================================================
# üöÄ RESPONSE VERSIONS GENERATOR (CONSERV√â IDENTIQUE)
# =============================================================================

class ResponseVersionsGenerator:
    """G√©n√©rateur de toutes les versions de r√©ponse pour le frontend"""
    
    def __init__(self, existing_processor: ResponseConcisionProcessor):
        self.existing_processor = existing_processor
        self.concision_service = concision_service
        logger.info("üöÄ [ResponseVersions] G√©n√©rateur initialis√© avec syst√®me existant")
    
    async def generate_all_response_versions(
        self, 
        original_response: str, 
        question: str, 
        context: Dict[str, Any],
        requested_level: ConcisionLevel = ConcisionLevel.CONCISE
    ) -> Dict[str, Any]:
        """
        G√©n√®re toutes les versions de r√©ponse en utilisant le syst√®me existant + nouveau
        """
        start_time = time.time()
        
        try:
            logger.info("üöÄ [ResponseVersions] G√©n√©ration toutes versions")
            logger.info(f"   - Question: {question[:50]}...")
            logger.info(f"   - Niveau demand√©: {requested_level}")
            logger.info(f"   - R√©ponse originale: {len(original_response)} caract√®res")
            
            versions = {}
            
            versions["detailed"] = original_response
            
            standard_response = self.existing_processor.apply_concision(
                original_response, 
                question, 
                ConcisionLevel.STANDARD,
                context.get("language", "fr")
            )
            versions["standard"] = standard_response
            
            concise_response = self.existing_processor.apply_concision(
                original_response, 
                question, 
                ConcisionLevel.CONCISE,
                context.get("language", "fr")
            )
            versions["concise"] = concise_response
            
            ultra_concise_response = self.existing_processor.apply_concision(
                original_response, 
                question, 
                ConcisionLevel.ULTRA_CONCISE,
                context.get("language", "fr")
            )
            versions["ultra_concise"] = ultra_concise_response
            
            selected_response = versions.get(requested_level.value, versions["concise"])
            
            generation_time_ms = int((time.time() - start_time) * 1000)
            
            metrics = ConcisionMetrics(
                generation_time_ms=generation_time_ms,
                versions_generated=len(versions),
                cache_hit=False,
                fallback_used=False,
                compression_ratios={
                    level: len(content) / len(original_response) 
                    for level, content in versions.items()
                    if content and len(original_response) > 0
                },
                quality_scores={}
            )
            
            logger.info("‚úÖ [ResponseVersions] Versions g√©n√©r√©es avec syst√®me existant:")
            for level, content in versions.items():
                logger.info(f"   - {level}: {len(content)} caract√®res")
            
            return {
                "response_versions": versions,
                "selected_response": selected_response,
                "concision_metrics": metrics
            }
            
        except Exception as e:
            logger.error(f"‚ùå [ResponseVersions] Erreur g√©n√©ration: {e}")
            
            fallback_versions = {
                "ultra_concise": original_response[:50] + "..." if len(original_response) > 50 else original_response,
                "concise": original_response,
                "standard": original_response,
                "detailed": original_response
            }
            
            return {
                "response_versions": fallback_versions,
                "selected_response": fallback_versions.get(requested_level.value, original_response),
                "concision_metrics": ConcisionMetrics(
                    generation_time_ms=int((time.time() - start_time) * 1000),
                    versions_generated=len(fallback_versions),
                    cache_hit=False,
                    fallback_used=True,
                    compression_ratios={},
                    quality_scores={}
                )
            }

# =============================================================================
# üîÑ RAG CONTEXT ENHANCER (CONSERV√â IDENTIQUE)
# =============================================================================

class RAGContextEnhancer:
    """Am√©liore le contexte conversationnel pour optimiser les requ√™tes RAG"""
    
    def __init__(self):
        self.pronoun_patterns = {
            "fr": [
                r'\b(son|sa|ses|leur|leurs)\s+(poids|√¢ge|croissance|d√©veloppement)',
                r'\b(ils|elles)\s+(p√®sent|grandissent|se d√©veloppent)',
                r'\b(qu\'?est-ce que|quel est)\s+(son|sa|ses|leur)',
                r'\b(combien)\s+(p√®sent-ils|font-ils|mesurent-ils)'
            ],
            "en": [
                r'\b(their|its)\s+(weight|age|growth|development)',
                r'\b(they)\s+(weigh|grow|develop)',
                r'\b(what is|how much is)\s+(their|its)',
                r'\b(how much do they)\s+(weigh|measure)'
            ],
            "es": [
                r'\b(su|sus)\s+(peso|edad|crecimiento|desarrollo)',
                r'\b(ellos|ellas)\s+(pesan|crecen|se desarrollan)',
                r'\b(cu√°l es|cu√°nto es)\s+(su|sus)',
                r'\b(cu√°nto)\s+(pesan|miden)'
            ]
        }
    
    def enhance_question_for_rag(
        self, 
        question: str, 
        conversation_context: str, 
        language: str = "fr"
    ) -> Tuple[str, Dict[str, any]]:
        """Am√©liore une question pour le RAG en utilisant le contexte conversationnel"""
        
        enhancement_info = {
            "pronoun_detected": False,
            "context_entities_used": [],
            "question_enriched": False,
            "original_question": question
        }
        
        has_pronouns = self._detect_contextual_references(question, language)
        if has_pronouns:
            enhancement_info["pronoun_detected"] = True
            logger.info(f"üîç [RAG Context] Pronoms d√©tect√©s dans: '{question}'")
        
        context_entities = self._extract_context_entities(conversation_context)
        if context_entities:
            enhancement_info["context_entities_used"] = list(context_entities.keys())
            logger.info(f"üìä [RAG Context] Entit√©s contextuelles: {context_entities}")
        
        enriched_question = question
        
        if has_pronouns and context_entities:
            enriched_question = self._build_enriched_question(
                question, context_entities, language
            )
            enhancement_info["question_enriched"] = True
            logger.info(f"‚ú® [RAG Context] Question enrichie: '{enriched_question}'")
        
        if context_entities or has_pronouns:
            technical_context = self._build_technical_context(context_entities, language)
            if technical_context:
                enriched_question += f"\n\nContexte technique: {technical_context}"
        
        return enriched_question, enhancement_info
    
    def _detect_contextual_references(self, question: str, language: str) -> bool:
        """D√©tecte si la question contient des pronoms/r√©f√©rences contextuelles"""
        
        patterns = self.pronoun_patterns.get(language, self.pronoun_patterns["fr"])
        question_lower = question.lower()
        
        for pattern in patterns:
            if re.search(pattern, question_lower, re.IGNORECASE):
                logger.debug(f"üéØ [RAG Context] Pattern trouv√©: {pattern}")
                return True
        
        return False
    
    def _extract_context_entities(self, context: str) -> Dict[str, str]:
        """Extrait les entit√©s importantes du contexte conversationnel"""
        
        if not context:
            return {}
        
        entities = {}
        context_lower = context.lower()
        
        breed_patterns = [
            r'race[:\s]+([a-zA-Z0-9\s]+?)(?:\n|,|\.|\s|$)',
            r'breed[:\s]+([a-zA-Z0-9\s]+?)(?:\n|,|\.|\s|$)',
            r'(ross\s*308|cobb\s*500|hubbard|arbor\s*acres)',
            r'poulets?\s+(ross\s*308|cobb\s*500)',
            r'chickens?\s+(ross\s*308|cobb\s*500)'
        ]
        
        for pattern in breed_patterns:
            match = re.search(pattern, context_lower, re.IGNORECASE)
            if match:
                entities["breed"] = match.group(1).strip()
                break
        
        sex_patterns = [
            r'sexe[:\s]+([a-zA-Z\s]+?)(?:\n|,|\.|\s|$)',
            r'sex[:\s]+([a-zA-Z\s]+?)(?:\n|,|\.|\s|$)',
            r'\b(m√¢les?|femelles?|males?|females?|mixte|mixed)\b'
        ]
        
        for pattern in sex_patterns:
            match = re.search(pattern, context_lower, re.IGNORECASE)
            if match:
                entities["sex"] = match.group(1).strip()
                break
        
        age_patterns = [
            r'√¢ge[:\s]+(\d+\s*(?:jour|semaine|day|week)s?)',
            r'age[:\s]+(\d+\s*(?:jour|semaine|day|week)s?)',
            r'(\d+)\s*(?:jour|day)s?',
            r'(\d+)\s*(?:semaine|week)s?'
        ]
        
        for pattern in age_patterns:
            match = re.search(pattern, context_lower, re.IGNORECASE)
            if match:
                entities["age"] = match.group(1).strip()
                break
        
        return entities
    
    def _build_enriched_question(
        self, 
        question: str, 
        context_entities: Dict[str, str], 
        language: str
    ) -> str:
        """Construit une question enrichie en rempla√ßant les pronoms par les entit√©s contextuelles"""
        
        enriched = question
        
        templates = {
            "fr": {
                "breed_sex_age": "Pour des {breed} {sex} de {age}",
                "breed_age": "Pour des {breed} de {age}",
                "breed_sex": "Pour des {breed} {sex}",
                "breed_only": "Pour des {breed}",
                "age_only": "Pour des poulets de {age}"
            },
            "en": {
                "breed_sex_age": "For {breed} {sex} chickens at {age}",
                "breed_age": "For {breed} chickens at {age}",
                "breed_sex": "For {breed} {sex} chickens",
                "breed_only": "For {breed} chickens", 
                "age_only": "For chickens at {age}"
            },
            "es": {
                "breed_sex_age": "Para pollos {breed} {sex} de {age}",
                "breed_age": "Para pollos {breed} de {age}",
                "breed_sex": "Para pollos {breed} {sex}",
                "breed_only": "Para pollos {breed}",
                "age_only": "Para pollos de {age}"
            }
        }
        
        template_set = templates.get(language, templates["fr"])
        
        context_prefix = ""
        if "breed" in context_entities and "sex" in context_entities and "age" in context_entities:
            context_prefix = template_set["breed_sex_age"].format(
                breed=context_entities["breed"],
                sex=context_entities["sex"],
                age=context_entities["age"]
            )
        elif "breed" in context_entities and "age" in context_entities:
            context_prefix = template_set["breed_age"].format(
                breed=context_entities["breed"],
                age=context_entities["age"]
            )
        elif "breed" in context_entities and "sex" in context_entities:
            context_prefix = template_set["breed_sex"].format(
                breed=context_entities["breed"],
                sex=context_entities["sex"]
            )
        elif "breed" in context_entities:
            context_prefix = template_set["breed_only"].format(
                breed=context_entities["breed"]
            )
        elif "age" in context_entities:
            context_prefix = template_set["age_only"].format(age=context_entities["age"])
        
        if context_prefix:
            if any(word in question.lower() for word in ["son", "sa", "ses", "leur", "leurs", "their", "its", "su", "sus"]):
                enriched = f"{context_prefix}, {question.lower()}"
            else:
                enriched = f"{context_prefix}: {question}"
        
        return enriched
    
    def _build_technical_context(self, entities: Dict[str, str], language: str) -> str:
        """Construit un contexte technique pour aider le RAG"""
        
        if not entities:
            return ""
        
        context_parts = []
        
        if "breed" in entities:
            context_parts.append(f"Race: {entities['breed']}")
        
        if "sex" in entities:
            context_parts.append(f"Sexe: {entities['sex']}")
        
        if "age" in entities:
            context_parts.append(f"√Çge: {entities['age']}")
        
        return " | ".join(context_parts)

# =============================================================================
# üîß NOUVELLES FONCTIONS: VALIDATION ROBUSTE ET AUTO-CLARIFICATION SIMPLIFI√âE
# =============================================================================

def validate_dynamic_questions(questions: List[str], user_question: str = "", language: str = "fr") -> Tuple[float, List[str]]:
    """
    üîß NOUVEAU: Valide la qualit√© des questions g√©n√©r√©es
    
    Returns:
        Tuple[float, List[str]]: (score_qualit√©, questions_filtr√©es)
    """
    
    if not questions or not isinstance(questions, list):
        logger.warning("üîß [Question Validation] Aucune question fournie ou format incorrect")
        return 0.0, []
    
    # Mots-cl√©s g√©n√©riques √† filtrer
    generic_keywords = {
        "fr": ["exemple", "par exemple", "etc", "quelque chose", "g√©n√©ralement", "souvent"],
        "en": ["example", "for example", "etc", "something", "generally", "often"],
        "es": ["ejemplo", "por ejemplo", "etc", "algo", "generalmente", "a menudo"]
    }
    
    keywords = generic_keywords.get(language, generic_keywords["fr"])
    
    # Filtrage basique : enlever questions vagues ou g√©n√©riques
    filtered = []
    for question in questions:
        if not isinstance(question, str):
            continue
            
        question = question.strip()
        
        # Tests de qualit√©
        if (len(question) > 15 and 
            len(question) < 200 and
            not any(keyword in question.lower() for keyword in keywords) and
            question not in filtered):
            
            # Ajouter point d'interrogation si manquant
            if not question.endswith('?'):
                question += ' ?'
                
            filtered.append(question)
    
    # Limiter √† 4 questions maximum
    filtered = filtered[:4]
    
    # Calculer score de qualit√©
    if questions:
        score = len(filtered) / max(len(questions), 1)
    else:
        score = 0.0
    
    logger.info(f"üîß [Question Validation] Score: {score:.2f}, Questions filtr√©es: {len(filtered)}/{len(questions)}")
    
    return score, filtered

def auto_clarify_if_needed(question: str, conversation_context: str, language: str = "fr") -> Optional[Dict[str, Any]]:
    """
    üîß NOUVEAU: Fonction centralis√©e pour l'auto-clarification
    
    Returns:
        Dict si clarification n√©cessaire, None sinon
    """
    
    # Calculer score de compl√©tude de base
    completeness_score = _calculate_basic_completeness_score(question, conversation_context, language)
    
    logger.info(f"üîß [Auto Clarify] Score compl√©tude: {completeness_score:.2f}")
    
    # Seuil pour d√©clencher clarification
    if completeness_score < 0.5:
        logger.info("üîß [Auto Clarify] Clarification n√©cessaire - g√©n√©ration questions")
        
        try:
            # Tenter g√©n√©ration dynamique avec GPT
            questions = _generate_clarification_questions_with_fallback(question, language)
            
            if questions:
                return {
                    "type": "clarification_needed",
                    "message": _get_clarification_intro_message(language),
                    "questions": questions,
                    "completeness_score": completeness_score,
                    "generation_method": "auto_clarification"
                }
        except Exception as e:
            logger.error(f"‚ùå [Auto Clarify] Erreur g√©n√©ration questions: {e}")
    
    return None

def _calculate_basic_completeness_score(question: str, conversation_context: str, language: str = "fr") -> float:
    """
    üîß NOUVEAU: Calcule un score de compl√©tude simplifi√© (0.0 √† 1.0)
    """
    
    score = 0.0
    
    # Score de base selon la longueur
    question_length = len(question.strip())
    if question_length > 50:
        score += 0.3
    elif question_length > 25:
        score += 0.2
    
    # Pr√©sence de race sp√©cifique
    specific_breeds = ["ross 308", "cobb 500", "hubbard", "arbor acres"]
    if any(breed in question.lower() for breed in specific_breeds):
        score += 0.3
    elif any(word in question.lower() for word in ["poulet", "chicken", "pollo"]):
        score += 0.1
    
    # Pr√©sence d'√¢ge
    age_patterns = [r'\d+\s*(?:jour|day|d√≠a)s?', r'\d+\s*(?:semaine|week|semana)s?']
    if any(re.search(pattern, question.lower()) for pattern in age_patterns):
        score += 0.2
    
    # Pr√©sence de donn√©es num√©riques
    if re.search(r'\d+', question):
        score += 0.1
    
    # Contexte conversationnel disponible
    if conversation_context and len(conversation_context.strip()) > 50:
        score += 0.1
    
    return min(score, 1.0)

def _generate_clarification_questions_with_fallback(question: str, language: str = "fr") -> List[str]:
    """
    üîß NOUVEAU: G√©n√®re questions avec fallback si GPT √©choue
    """
    
    try:
        # Import dynamique pour √©viter erreurs de d√©pendance
        from .question_clarification_system import generate_dynamic_clarification_questions_with_validation
        
        # Tenter g√©n√©ration dynamique avec validation
        questions, validation_metadata = generate_dynamic_clarification_questions_with_validation(question, language)
        
        # V√©rifier qualit√©
        score, filtered_questions = validate_dynamic_questions(questions, question, language)
        
        if score >= 0.5 and filtered_questions:
            logger.info(f"‚úÖ [Clarification Generation] Questions GPT valid√©es: {len(filtered_questions)}")
            return filtered_questions
        else:
            logger.warning(f"‚ö†Ô∏è [Clarification Generation] Score trop bas ({score:.2f}) - fallback")
            
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è [Clarification Generation] Erreur GPT: {e} - fallback")
    
    # Fallback : questions basiques selon le type
    return _get_fallback_questions_by_type(question, language)

def _get_fallback_questions_by_type(question: str, language: str = "fr") -> List[str]:
    """
    üîß NOUVEAU: Questions de fallback selon le type de question d√©tect√©
    """
    
    question_lower = question.lower()
    
    # D√©tection type de question
    is_weight = any(word in question_lower for word in ["poids", "weight", "peso"])
    is_health = any(word in question_lower for word in ["maladie", "disease", "mort", "death"])
    is_growth = any(word in question_lower for word in ["croissance", "growth", "d√©veloppement"])
    
    fallback_questions = {
        "fr": {
            "weight": [
                "Quelle race ou souche sp√©cifique √©levez-vous (Ross 308, Cobb 500, etc.) ?",
                "Quel √¢ge ont actuellement vos poulets (en jours pr√©cis) ?",
                "S'agit-il de m√¢les, femelles, ou d'un troupeau mixte ?"
            ],
            "health": [
                "Quelle race ou souche √©levez-vous ?",
                "Quel √¢ge ont vos volailles actuellement ?",
                "Quels sympt√¥mes sp√©cifiques observez-vous ?"
            ],
            "growth": [
                "Quelle race ou souche sp√©cifique √©levez-vous ?",
                "Quel √¢ge ont-ils actuellement en jours ?",
                "Quelles sont les conditions d'√©levage actuelles ?"
            ],
            "general": [
                "Pouvez-vous pr√©ciser la race ou souche de vos volailles ?",
                "Quel √¢ge ont actuellement vos animaux ?",
                "Dans quel contexte d'√©levage vous trouvez-vous ?"
            ]
        },
        "en": {
            "weight": [
                "What specific breed or strain are you raising (Ross 308, Cobb 500, etc.)?",
                "What is the current age of your chickens (in precise days)?",
                "Are these males, females, or a mixed flock?"
            ],
            "health": [
                "What breed or strain are you raising?",
                "What is the current age of your poultry?",
                "What specific symptoms are you observing?"
            ],
            "growth": [
                "What specific breed or strain are you raising?",
                "What is their current age in days?",
                "What are the current housing conditions?"
            ],
            "general": [
                "Could you specify the breed or strain of your poultry?",
                "What age are your animals currently?",
                "What farming context are you in?"
            ]
        },
        "es": {
            "weight": [
                "¬øQu√© raza o cepa espec√≠fica est√° criando (Ross 308, Cobb 500, etc.)?",
                "¬øCu√°l es la edad actual de sus pollos (en d√≠as precisos)?",
                "¬øSon machos, hembras, o un lote mixto?"
            ],
            "health": [
                "¬øQu√© raza o cepa est√° criando?",
                "¬øCu√°l es la edad actual de sus aves?",
                "¬øQu√© s√≠ntomas espec√≠ficos est√° observando?"
            ],
            "growth": [
                "¬øQu√© raza o cepa espec√≠fica est√° criando?",
                "¬øCu√°l es su edad actual en d√≠as?",
                "¬øCu√°les son las condiciones actuales de alojamiento?"
            ],
            "general": [
                "¬øPodr√≠a especificar la raza o cepa de sus aves?",
                "¬øQu√© edad tienen actualmente sus animales?",
                "¬øEn qu√© contexto de cr√≠a se encuentra?"
            ]
        }
    }
    
    lang_questions = fallback_questions.get(language, fallback_questions["fr"])
    
    # S√©lectionner type appropri√©
    if is_weight:
        return lang_questions["weight"]
    elif is_health:
        return lang_questions["health"]
    elif is_growth:
        return lang_questions["growth"]
    else:
        return lang_questions["general"]

def _get_clarification_intro_message(language: str = "fr") -> str:
    """
    üîß NOUVEAU: Message d'introduction pour la clarification
    """
    
    messages = {
        "fr": "Votre question manque de contexte. Voici quelques questions pour mieux vous aider :",
        "en": "Your question lacks context. Here are some questions to better help you:",
        "es": "Su pregunta carece de contexto. Aqu√≠ hay algunas preguntas para ayudarle mejor:"
    }
    
    return messages.get(language, messages["fr"])

# =============================================================================
# üîÑ EXPERT SERVICE PRINCIPAL AVEC AUTO-CLARIFICATION INT√âGR√âE
# =============================================================================

class ExpertService:
    """Service principal pour le syst√®me expert avec auto-clarification simplifi√©e int√©gr√©e"""
    
    def __init__(self):
        self.integrations = IntegrationsManager()
        self.rag_enhancer = RAGContextEnhancer()
        self.enhancement_service = APIEnhancementService()
        
        self.concision_processor = ResponseConcisionProcessor()
        
        self.response_versions_generator = ResponseVersionsGenerator(
            existing_processor=self.concision_processor
        )
        
        logger.info("‚úÖ [Expert Service] Service expert initialis√© avec auto-clarification simplifi√©e")
    
    def get_current_user_dependency(self):
        """Retourne la d√©pendance pour l'authentification"""
        return self.integrations.get_current_user_dependency()
    
    async def process_expert_question(
        self,
        request_data: EnhancedQuestionRequest,
        request: Request,
        current_user: Optional[Dict[str, Any]] = None,
        start_time: float = None
    ) -> EnhancedExpertResponse:
        """
        üöÄ MODIFI√â: M√©thode principale avec auto-clarification int√©gr√©e
        """
        
        if start_time is None:
            start_time = time.time()
        
        try:
            logger.info("üöÄ [ExpertService] Traitement question avec auto-clarification")
            
            concision_level = getattr(request_data, 'concision_level', ConcisionLevel.CONCISE)
            generate_all_versions = getattr(request_data, 'generate_all_versions', True)
            semantic_dynamic_mode = getattr(request_data, 'semantic_dynamic_mode', False)
            
            logger.info(f"üöÄ [ResponseVersions] Param√®tres: level={concision_level}, generate_all={generate_all_versions}")
            logger.info(f"üÜï [Semantic Dynamic] Mode: {semantic_dynamic_mode}")
            
            base_response = await self._process_question_with_auto_clarification(
                request_data, request, current_user, start_time, semantic_dynamic_mode
            )
            
            if generate_all_versions and base_response.response:
                try:
                    logger.info("üöÄ [ResponseVersions] G√©n√©ration de toutes les versions")
                    
                    versions_result = await self.response_versions_generator.generate_all_response_versions(
                        original_response=base_response.response,
                        question=request_data.text,
                        context={
                            "language": request_data.language,
                            "user_id": current_user.get("id") if current_user else None,
                            "conversation_id": request_data.conversation_id
                        },
                        requested_level=concision_level
                    )
                    
                    base_response.response_versions = versions_result["response_versions"]
                    base_response.response = versions_result["selected_response"]
                    base_response.concision_metrics = versions_result["concision_metrics"]
                    
                    logger.info("‚úÖ [ResponseVersions] Versions ajout√©es √† la r√©ponse")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è [ResponseVersions] Erreur g√©n√©ration versions: {e}")
                    base_response.response_versions = None
            else:
                logger.info("üöÄ [ResponseVersions] G√©n√©ration versions d√©sactiv√©e")
                base_response.response_versions = None
            
            return base_response
            
        except Exception as e:
            logger.error(f"‚ùå [ExpertService] Erreur traitement avec auto-clarification: {e}")
            raise
    
    async def _process_question_with_auto_clarification(
        self,
        request_data: EnhancedQuestionRequest,
        request: Request,
        current_user: Optional[Dict[str, Any]] = None,
        start_time: float = None,
        semantic_dynamic_mode: bool = False
    ) -> EnhancedExpertResponse:
        """
        üîß MODIFI√â: Logique avec auto-clarification int√©gr√©e au d√©but
        """
        
        processing_steps = []
        ai_enhancements_used = []
        debug_info = {}
        performance_breakdown = {"start": int(time.time() * 1000)}
        
        processing_steps.append("initialization")
        
        # === AUTHENTIFICATION ===
        if current_user is None and self.integrations.auth_available:
            raise HTTPException(status_code=401, detail="Authentification requise")
        
        user_id = self._extract_user_id(current_user, request_data, request)
        user_email = current_user.get("email") if current_user else None
        request_ip = request.client.host if request.client else "unknown"
        
        processing_steps.append("authentication")
        performance_breakdown["auth_complete"] = int(time.time() * 1000)
        
        # === GESTION CONVERSATION ID ===
        conversation_id = self._get_or_create_conversation_id(request_data)
        
        # === VALIDATION QUESTION ===
        question_text = request_data.text.strip()
        if not question_text:
            raise HTTPException(status_code=400, detail="Question text is required")
        
        processing_steps.append("question_validation")
        
        # === M√âMOIRE CONVERSATIONNELLE ===
        conversation_context = None
        
        if self.integrations.intelligent_memory_available:
            try:
                conversation_context = self.integrations.add_message_to_conversation(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    message=question_text,
                    role="user",
                    language=request_data.language,
                    message_type="clarification_response" if request_data.is_clarification_response else "question"
                )
                
                ai_enhancements_used.append("intelligent_memory")
                processing_steps.append("memory_storage")
                logger.info(f"üíæ [Expert Service] Message ajout√© √† la m√©moire: {question_text[:50]}...")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [Expert Service] Erreur m√©moire: {e}")
        
        performance_breakdown["memory_complete"] = int(time.time() * 1000)
        
        # üîß NOUVEAU: AUTO-CLARIFICATION INT√âGR√âE (si pas d√©j√† une r√©ponse de clarification)
        if not request_data.is_clarification_response:
            conversation_context_str = ""
            if conversation_context and hasattr(conversation_context, 'get_context_for_rag'):
                conversation_context_str = conversation_context.get_context_for_rag(max_chars=500)
            
            clarification_result = auto_clarify_if_needed(
                question_text, conversation_context_str, request_data.language
            )
            
            if clarification_result:
                logger.info("üîß [Auto Clarification] Clarification d√©clench√©e automatiquement")
                
                processing_steps.append("auto_clarification_triggered")
                ai_enhancements_used.append("auto_clarification_system")
                
                # Formater les questions
                if len(clarification_result["questions"]) == 1:
                    formatted_questions = clarification_result["questions"][0]
                else:
                    formatted_questions = "\n".join([f"‚Ä¢ {q}" for q in clarification_result["questions"]])
                
                response_text = f"{clarification_result['message']}\n\n{formatted_questions}\n\nCela m'aidera √† vous donner une r√©ponse plus pr√©cise ! üêî"
                
                response_time_ms = int((time.time() - start_time) * 1000)
                
                return EnhancedExpertResponse(
                    question=question_text,
                    response=response_text,
                    conversation_id=conversation_id,
                    rag_used=False,
                    rag_score=None,
                    timestamp=datetime.now().isoformat(),
                    language=request_data.language,
                    response_time_ms=response_time_ms,
                    mode="auto_clarification_triggered",
                    user=user_email,
                    logged=True,
                    validation_passed=True,
                    clarification_result={
                        "clarification_requested": True,
                        "clarification_type": "auto_triggered",
                        "completeness_score": clarification_result.get("completeness_score", 0.0),
                        "questions_generated": len(clarification_result["questions"]),
                        "generation_method": clarification_result.get("generation_method", "auto_clarification"),
                        "automatic_trigger": True
                    },
                    processing_steps=processing_steps,
                    ai_enhancements_used=ai_enhancements_used,
                    dynamic_clarification=DynamicClarification(
                        original_question=question_text,
                        clarification_questions=clarification_result["questions"],
                        confidence=0.8,
                        generation_method="auto_clarification_system",
                        generation_time_ms=response_time_ms,
                        fallback_used=False
                    )
                )
        
        # === VALIDATION AGRICOLE ===
        validation_result = await self._validate_agricultural_question(
            question_text, request_data.language, user_id, request_ip, conversation_id
        )
        
        processing_steps.append("agricultural_validation")
        performance_breakdown["validation_complete"] = int(time.time() * 1000)
        
        if not validation_result.is_valid:
            return self._create_rejection_response(
                question_text, validation_result, conversation_id, 
                user_email, request_data.language, start_time,
                processing_steps, ai_enhancements_used, None
            )
        
        # === SYST√àME DE CLARIFICATION INTELLIGENT + S√âMANTIQUE DYNAMIQUE ===
        clarification_result = await self._handle_clarification_corrected_with_semantic_dynamic(
            request_data, question_text, user_id, conversation_id,
            processing_steps, ai_enhancements_used, semantic_dynamic_mode
        )
        
        if clarification_result:
            return clarification_result
        
        # D√©tection vagueness apr√®s clarifications sp√©cialis√©es
        vagueness_result = None
        if request_data.enable_vagueness_detection:
            vagueness_result = self.enhancement_service.detect_vagueness(
                question_text, request_data.language
            )
            
            ai_enhancements_used.append("vagueness_detection")
            performance_breakdown["vagueness_check"] = int(time.time() * 1000)
            
            if vagueness_result.is_vague and vagueness_result.vagueness_score > 0.6:
                logger.info(f"üéØ [Expert Service] Question floue d√©tect√©e (score: {vagueness_result.vagueness_score})")
                return self._create_vagueness_response(
                    vagueness_result, question_text, conversation_id, 
                    request_data.language, start_time, processing_steps, ai_enhancements_used
                )
        
        performance_breakdown["clarification_complete"] = int(time.time() * 1000)
        
        # === TRAITEMENT EXPERT AVEC RAG-FIRST + AM√âLIORATIONS + TAXONOMIC FILTERING ===
        expert_result = await self._process_expert_response_enhanced_corrected_with_taxonomy(
            question_text, request_data, request, current_user,
            conversation_id, processing_steps, ai_enhancements_used,
            debug_info, performance_breakdown, vagueness_result
        )
        
        # ‚úÖ CONSERV√â: APPLICATION DU SYST√àME DE CONCISION EXISTANT
        if expert_result["answer"] and self.concision_processor.config.ENABLE_CONCISE_RESPONSES:
            
            user_concision_preference = getattr(request_data, 'concision_level', None)
            
            original_answer = expert_result["answer"]
            processed_answer = self.concision_processor.process_response(
                response=original_answer,
                question=question_text,
                concision_level=user_concision_preference,
                language=request_data.language
            )
            
            if processed_answer != original_answer:
                expert_result["answer"] = processed_answer
                expert_result["original_answer"] = original_answer
                expert_result["concision_applied"] = True
                ai_enhancements_used.append("response_concision")
                processing_steps.append("concision_processing")
                
                logger.info(f"‚úÇÔ∏è [Expert Service] Concision appliqu√©e: {len(original_answer)} ‚Üí {len(processed_answer)} chars")
            else:
                expert_result["concision_applied"] = False
        
        performance_breakdown["concision_complete"] = int(time.time() * 1000)
        
        # === ENREGISTREMENT R√âPONSE ===
        if self.integrations.intelligent_memory_available and expert_result["answer"]:
            try:
                self.integrations.add_message_to_conversation(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    message=expert_result["answer"],
                    role="assistant",
                    language=request_data.language,
                    message_type="response"
                )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [Expert Service] Erreur enregistrement r√©ponse: {e}")
        
        processing_steps.append("response_storage")
        performance_breakdown["final"] = int(time.time() * 1000)
        
        # === CONSTRUCTION R√âPONSE FINALE AM√âLIOR√âE ===
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return self._build_final_enhanced_response(
            question_text, expert_result["answer"], conversation_id,
            user_email, request_data.language, response_time_ms,
            expert_result, validation_result, conversation_context,
            processing_steps, ai_enhancements_used, request_data,
            debug_info, performance_breakdown
        )
    
    # ===========================================================================================
    # ‚úÖ TOUTES LES M√âTHODES EXISTANTES CONSERV√âES IDENTIQUES + üè∑Ô∏è M√âTHODE TAXONOMIQUE
    # ===========================================================================================
    
    async def _process_expert_response_enhanced_corrected_with_taxonomy(
        self, question_text: str, request_data: EnhancedQuestionRequest,
        request: Request, current_user: Optional[Dict], conversation_id: str,
        processing_steps: list, ai_enhancements_used: list,
        debug_info: Dict, performance_breakdown: Dict, vagueness_result = None
    ) -> Dict[str, Any]:
        """
        üè∑Ô∏è VERSION CONSERV√âE: RAG parfaitement corrig√© avec m√©moire intelligente + FILTRAGE TAXONOMIQUE
        """
        
        # === 1. R√âCUP√âRATION FORC√âE DU CONTEXTE CONVERSATIONNEL ===
        conversation_context_str = ""
        extracted_entities = {}
        
        if self.integrations.intelligent_memory_available:
            try:
                context_obj = self.integrations.get_conversation_context(conversation_id)
                if context_obj:
                    conversation_context_str = context_obj.get_context_for_rag(max_chars=800)
                    
                    if request_data.is_clarification_response or request_data.original_question:
                        if request_data.original_question:
                            conversation_context_str = f"Question originale: {request_data.original_question}. " + conversation_context_str
                        
                        if hasattr(context_obj, 'messages'):
                            for msg in reversed(context_obj.messages[-5:]):
                                if msg.role == "user" and any(word in msg.message.lower() for word in ["ross", "cobb", "hubbard", "m√¢le", "femelle", "male", "female"]):
                                    conversation_context_str += f" | Clarification: {msg.message}"
                                    break
                    
                    if hasattr(context_obj, 'consolidated_entities'):
                        extracted_entities = context_obj.consolidated_entities.to_dict()
                    
                    logger.info(f"üß† [Expert Service] Contexte enrichi r√©cup√©r√©: {conversation_context_str[:150]}...")
                    ai_enhancements_used.append("intelligent_memory_context_retrieval")
                else:
                    logger.warning(f"‚ö†Ô∏è [Expert Service] Aucun contexte trouv√© pour: {conversation_id}")
                    
            except Exception as e:
                logger.error(f"‚ùå [Expert Service] Erreur r√©cup√©ration contexte: {e}")
        
        performance_breakdown["context_retrieved"] = int(time.time() * 1000)
        
        # === 2. AM√âLIORATION INTELLIGENTE DE LA QUESTION ===
        enriched_question, enhancement_info = self.rag_enhancer.enhance_question_for_rag(
            question=question_text,
            conversation_context=conversation_context_str,
            language=request_data.language
        )
        
        if request_data.original_question and request_data.is_clarification_response:
            logger.info(f"‚ú® [Expert Service] Question d√©j√† enrichie par clarification: {question_text[:100]}...")
            ai_enhancements_used.append("clarification_based_enrichment")
        
        if enhancement_info["question_enriched"]:
            ai_enhancements_used.append("intelligent_question_enhancement")
            logger.info(f"‚ú® [Expert Service] Question am√©lior√©e: {enriched_question[:150]}...")
        
        if enhancement_info["pronoun_detected"]:
            ai_enhancements_used.append("contextual_pronoun_resolution")
            logger.info(f"üéØ [Expert Service] Pronoms contextuels r√©solus: {enhancement_info['context_entities_used']}")
        
        processing_steps.append("intelligent_question_enhancement")
        performance_breakdown["question_enhanced"] = int(time.time() * 1000)
        
        # === 3. üè∑Ô∏è FILTRAGE TAXONOMIQUE INTELLIGENT ===
        from .api_enhancement_service import infer_taxonomy_from_entities, enhance_rag_query_with_taxonomy
        
        taxonomy = infer_taxonomy_from_entities(extracted_entities)
        enhanced_question_with_taxonomy, rag_filters = enhance_rag_query_with_taxonomy(
            enriched_question, extracted_entities, request_data.language
        )
        
        logger.info(f"üè∑Ô∏è [Taxonomy Filter] Taxonomie d√©tect√©e: {taxonomy}")
        if rag_filters:
            logger.info(f"üè∑Ô∏è [Taxonomy Filter] Filtres RAG: {rag_filters}")
            ai_enhancements_used.append("taxonomic_document_filtering")
        
        processing_steps.append("taxonomic_analysis_and_filtering")
        performance_breakdown["taxonomy_analysis"] = int(time.time() * 1000)
        
        # === 4. V√âRIFICATION RAG DISPONIBLE ===
        app = request.app
        process_rag = getattr(app.state, 'process_question_with_rag', None)
        
        if not process_rag:
            logger.error("‚ùå [Expert Service] Syst√®me RAG indisponible - Erreur critique")
            raise HTTPException(
                status_code=503, 
                detail="Service RAG indisponible - Le syst√®me expert n√©cessite l'acc√®s √† la base documentaire"
            )
        
        # === 5. APPEL RAG AVEC CONTEXTE FORC√â + FILTRAGE TAXONOMIQUE ===
        try:
            logger.info("üîç [Expert Service] Appel RAG avec contexte intelligent + taxonomie...")
            
            if request_data.debug_mode:
                debug_info["original_question"] = question_text
                debug_info["enriched_question"] = enriched_question
                debug_info["enriched_question_with_taxonomy"] = enhanced_question_with_taxonomy
                debug_info["conversation_context"] = conversation_context_str
                debug_info["enhancement_info"] = enhancement_info
                debug_info["taxonomy_detected"] = taxonomy
                debug_info["rag_filters"] = rag_filters
            
            result = None
            rag_call_method = "unknown"
            
            rag_context = extract_context_from_entities(extracted_entities)
            rag_context["lang"] = request_data.language
            rag_context["taxonomy"] = taxonomy
            
            # Tentative 1: Avec param√®tre context + filtres taxonomiques si support√©
            try:
                structured_question = build_structured_prompt(
                    documents="[DOCUMENTS_WILL_BE_INSERTED_BY_RAG]",
                    question=enhanced_question_with_taxonomy,
                    context=rag_context
                )
                
                logger.debug(f"üîç [Prompt Final RAG] Contexte: {rag_context}")
                logger.debug(f"üè∑Ô∏è [Prompt Final RAG] Taxonomie: {taxonomy}")
                logger.debug(f"üîç [Prompt Final RAG]\n{structured_question[:500]}...")
                
                rag_params = {
                    "question": structured_question,
                    "user": current_user,
                    "language": request_data.language,
                    "speed_mode": request_data.speed_mode,
                    "context": conversation_context_str
                }
                
                if rag_filters:
                    try:
                        rag_params["filters"] = rag_filters
                        result = await process_rag(**rag_params)
                        rag_call_method = "context_parameter_structured_with_taxonomy"
                        logger.info("‚úÖ [Expert Service] RAG appel√© avec prompt structur√© + contexte + filtres taxonomiques")
                    except TypeError:
                        del rag_params["filters"]
                        result = await process_rag(**rag_params)
                        rag_call_method = "context_parameter_structured_taxonomy_fallback"
                        logger.info("‚úÖ [Expert Service] RAG appel√© avec prompt structur√© + contexte (filtres taxonomiques non support√©s)")
                else:
                    result = await process_rag(**rag_params)
                    rag_call_method = "context_parameter_structured"
                    logger.info("‚úÖ [Expert Service] RAG appel√© avec prompt structur√© + contexte")
                    
            except TypeError as te:
                logger.info(f"‚ÑπÔ∏è [Expert Service] Param√®tre context non support√©: {te}")
                
                if conversation_context_str:
                    structured_question = build_structured_prompt(
                        documents="[DOCUMENTS_WILL_BE_INSERTED_BY_RAG]",
                        question=enhanced_question_with_taxonomy,
                        context=rag_context
                    )
                    
                    logger.debug(f"üîç [Prompt Final RAG - Inject√© + Taxonomie]\n{structured_question[:500]}...")
                    
                    contextual_question = f"{structured_question}\n\nContexte: {conversation_context_str}"
                    result = await process_rag(
                        question=contextual_question,
                        user=current_user,
                        language=request_data.language,
                        speed_mode=request_data.speed_mode
                    )
                    rag_call_method = "context_injected_structured_with_taxonomy"
                    logger.info("‚úÖ [Expert Service] RAG appel√© avec prompt structur√© + contexte inject√© + taxonomie")
                else:
                    structured_question = build_structured_prompt(
                        documents="[DOCUMENTS_WILL_BE_INSERTED_BY_RAG]",
                        question=enhanced_question_with_taxonomy,
                        context=rag_context
                    )
                    
                    logger.debug(f"üîç [Prompt Final RAG - Seul + Taxonomie]\n{structured_question[:500]}...")
                    
                    result = await process_rag(
                        question=structured_question,
                        user=current_user,
                        language=request_data.language,
                        speed_mode=request_data.speed_mode
                    )
                    rag_call_method = "structured_with_taxonomy_only"
                    logger.info("‚úÖ [Expert Service] RAG appel√© avec prompt structur√© + taxonomie seul")
            
            performance_breakdown["rag_complete"] = int(time.time() * 1000)
            
            # === 6. TRAITEMENT R√âSULTAT RAG ===
            answer = str(result.get("response", ""))
            
            answer = self.concision_processor._clean_document_references_only(answer)
            
            rag_score = result.get("score", 0.0)
            original_mode = result.get("mode", "rag_processing")
            
            quality_check = self._validate_rag_response_quality(
                answer, enhanced_question_with_taxonomy, enhancement_info
            )
            
            if not quality_check["valid"]:
                logger.warning(f"‚ö†Ô∏è [Expert Service] Qualit√© RAG insuffisante: {quality_check['reason']}")
                ai_enhancements_used.append("quality_validation_failed")
            
            logger.info(f"‚úÖ [Expert Service] RAG r√©ponse re√ßue: {len(answer)} caract√®res, score: {rag_score}")
            
            mode = f"enhanced_contextual_{original_mode}_{rag_call_method}_corrected_with_concision_and_response_versions_and_taxonomy_and_semantic_dynamic_and_auto_clarification_simplified"
            
            processing_steps.append("mandatory_rag_with_intelligent_context_and_taxonomy")
            
            return {
                "answer": answer,
                "rag_used": True,
                "rag_score": rag_score,
                "mode": mode,
                "context_used": bool(conversation_context_str),
                "question_enriched": enhancement_info["question_enriched"] or bool(request_data.original_question),
                "enhancement_info": enhancement_info,
                "quality_check": quality_check,
                "extracted_entities": extracted_entities,
                "rag_call_method": rag_call_method,
                "taxonomy_used": taxonomy,
                "taxonomy_filters_applied": bool(rag_filters)
            }
            
        except Exception as rag_error:
            logger.error(f"‚ùå [Expert Service] Erreur critique RAG: {rag_error}")
            processing_steps.append("rag_error")
            
            error_details = {
                "error": "Erreur RAG",
                "message": "Impossible d'interroger la base documentaire",
                "question_original": question_text,
                "question_enriched": enriched_question,
                "question_with_taxonomy": enhanced_question_with_taxonomy,
                "context_available": bool(conversation_context_str),
                "taxonomy_detected": taxonomy,
                "technical_error": str(rag_error)
            }
            
            raise HTTPException(status_code=503, detail=error_details)
    
    # ===========================================================================================
    # ‚úÖ TOUTES LES AUTRES M√âTHODES EXISTANTES CONSERV√âES IDENTIQUES
    # ===========================================================================================
    
    async def _handle_clarification_corrected_with_semantic_dynamic(
        self, request_data, question_text, user_id, conversation_id, 
        processing_steps, ai_enhancements_used, semantic_dynamic_mode: bool = False
    ):
        """
        ‚úÖ SYST√àME DE CLARIFICATION PARFAITEMENT CORRIG√â + MODE S√âMANTIQUE DYNAMIQUE
        üÜï NOUVEAU: Support du mode s√©mantique dynamique
        """
        
        # 1. ‚úÖ TRAITEMENT DES R√âPONSES DE CLARIFICATION CORRIG√â
        if request_data.is_clarification_response:
            return await self._process_clarification_response_corrected(
                request_data, question_text, conversation_id,
                processing_steps, ai_enhancements_used
            )
        
        # üÜï NOUVEAU: V√©rifier si mode s√©mantique dynamique activ√©
        if semantic_dynamic_mode and self.integrations.enhanced_clarification_available:
            logger.info(f"üÜï [Semantic Dynamic] Mode activ√© pour: '{question_text[:50]}...'")
            
            try:
                from .question_clarification_system import analyze_question_for_clarification_semantic_dynamic
                
                clarification_result = await analyze_question_for_clarification_semantic_dynamic(
                    question=question_text,
                    language=request_data.language,
                    user_id=user_id,
                    conversation_id=conversation_id,
                    conversation_context={}
                )
                
                if clarification_result.needs_clarification:
                    logger.info(f"üÜï [Semantic Dynamic] {len(clarification_result.questions)} questions g√©n√©r√©es")
                    processing_steps.append("semantic_dynamic_clarification_triggered")
                    ai_enhancements_used.append("semantic_dynamic_clarification")
                    
                    return self._create_semantic_dynamic_clarification_response(
                        question_text, clarification_result, request_data.language, conversation_id
                    )
                else:
                    logger.info(f"‚úÖ [Semantic Dynamic] Question claire, pas de clarification n√©cessaire")
                
            except Exception as e:
                logger.error(f"‚ùå [Semantic Dynamic] Erreur mode s√©mantique: {e}")
        
        # 2. D√âTECTION QUESTIONS N√âCESSITANT CLARIFICATION (mode normal)
        clarification_needed = self._detect_performance_question_needing_clarification(
            question_text, request_data.language
        )
        
        if not clarification_needed:
            return None
        
        logger.info(f"üéØ [Expert Service] Clarification n√©cessaire: {clarification_needed['type']}")
        processing_steps.append("automatic_clarification_triggered")
        ai_enhancements_used.append("smart_performance_clarification")
        
        # 3. ‚úÖ SAUVEGARDE FORC√âE AVEC M√âMOIRE INTELLIGENTE
        if self.integrations.intelligent_memory_available:
            try:
                from .conversation_memory_enhanced import mark_question_for_clarification
                
                question_id = mark_question_for_clarification(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    original_question=question_text,
                    language=request_data.language
                )
                
                logger.info(f"üíæ [Expert Service] Question originale marqu√©e: {question_id}")
                processing_steps.append("original_question_marked")
                ai_enhancements_used.append("intelligent_memory_clarification_marking")
                
            except Exception as e:
                logger.error(f"‚ùå [Expert Service] Erreur marquage question: {e}")
                self.integrations.add_message_to_conversation(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    message=f"ORIGINAL_QUESTION_FOR_CLARIFICATION: {question_text}",
                    role="system",
                    language=request_data.language,
                    message_type="original_question_marker"
                )
        
        # 4. G√©n√©rer la demande de clarification
        clarification_response = self._generate_performance_clarification_response(
            question_text, clarification_needed, request_data.language, conversation_id
        )
        
        return clarification_response
    
    def _create_semantic_dynamic_clarification_response(
        self, question: str, clarification_result, language: str, conversation_id: str
    ) -> EnhancedExpertResponse:
        """üÜï NOUVEAU: Cr√©e une r√©ponse de clarification s√©mantique dynamique"""
        
        # Formater les questions de clarification
        if clarification_result.questions:
            if len(clarification_result.questions) == 1:
                response_text = f"‚ùì Pour mieux comprendre votre situation et vous aider efficacement :\n\n{clarification_result.questions[0]}"
            else:
                formatted_questions = "\n".join([f"‚Ä¢ {q}" for q in clarification_result.questions])
                response_text = f"‚ùì Pour mieux comprendre votre situation et vous aider efficacement :\n\n{formatted_questions}"
            
            response_text += "\n\nCela me permettra de vous donner les conseils les plus pertinents ! üêî"
        else:
            response_text = "‚ùì Pouvez-vous pr√©ciser votre question pour que je puisse mieux vous aider ?"
        
        return EnhancedExpertResponse(
            question=question,
            response=response_text,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=int(clarification_result.processing_time_ms),
            mode="semantic_dynamic_clarification",
            user=None,
            logged=True,
            validation_passed=True,
            clarification_result={
                "clarification_requested": True,
                "clarification_type": "semantic_dynamic",
                "questions_generated": len(clarification_result.questions) if clarification_result.questions else 0,
                "confidence": clarification_result.confidence_score,
                "model_used": clarification_result.model_used,
                "generation_time_ms": clarification_result.processing_time_ms
            },
            processing_steps=["semantic_dynamic_clarification_triggered"],
            ai_enhancements_used=["semantic_dynamic_clarification", "gpt_question_generation"],
            dynamic_clarification=DynamicClarification(
                original_question=question,
                clarification_questions=clarification_result.questions or [],
                confidence=clarification_result.confidence_score
            )
        )
    
    async def _process_clarification_response_corrected(
        self, request_data, question_text, conversation_id, 
        processing_steps, ai_enhancements_used
    ):
        """‚úÖ TRAITEMENT DES R√âPONSES DE CLARIFICATION - VERSION CORRIG√âE FINALE (CONSERV√â)"""
        
        original_question = request_data.original_question
        clarification_context = request_data.clarification_context
        
        if (not original_question or not clarification_context) and self.integrations.intelligent_memory_available:
            try:
                from .conversation_memory_enhanced import find_original_question
                
                original_msg = find_original_question(conversation_id)
                
                if original_msg:
                    original_question = original_msg.message
                    clarification_context = {
                        "missing_information": ["breed", "sex"],
                        "clarification_type": "performance_breed_sex"
                    }
                    logger.info(f"‚úÖ [Expert Service] Question originale r√©cup√©r√©e: {original_question}")
                    ai_enhancements_used.append("intelligent_memory_original_question_recovery")
                else:
                    logger.warning("‚ö†Ô∏è [Expert Service] Question originale non trouv√©e dans la m√©moire")
                    
            except Exception as e:
                logger.error(f"‚ùå [Expert Service] Erreur r√©cup√©ration question originale: {e}")
        
        if not original_question:
            logger.warning("‚ö†Ô∏è [Expert Service] Fallback: cr√©ation question par d√©faut")
            original_question = "Quel est le poids de r√©f√©rence pour ces poulets ?"
            clarification_context = {
                "missing_information": ["breed", "sex"],
                "clarification_type": "performance_breed_sex_fallback"
            }
        
        missing_info = clarification_context.get("missing_information", [])
        
        validation = validate_clarification_completeness(
            question_text, missing_info, request_data.language
        )
        
        if not validation["is_complete"]:
            logger.info(f"üîÑ [Expert Service] Clarification incompl√®te: {validation['still_missing']}")
            return self._generate_follow_up_clarification(
                question_text, validation, request_data.language, conversation_id
            )
        
        breed = validation["extracted_info"].get("breed")
        sex = validation["extracted_info"].get("sex")
        
        age_info = self._extract_age_from_original_question(original_question, request_data.language)
        
        enriched_original_question = self._build_complete_enriched_question(
            original_question, breed, sex, age_info, request_data.language
        )
        
        logger.info(f"‚úÖ [Expert Service] Question COMPL√àTEMENT enrichie: {enriched_original_question}")
        
        request_data.text = enriched_original_question
        request_data.is_clarification_response = False
        request_data.original_question = original_question
        
        processing_steps.append("clarification_processed_successfully_with_age")
        ai_enhancements_used.append("breed_sex_age_extraction_complete")
        ai_enhancements_used.append("complete_question_enrichment")
        ai_enhancements_used.append("forced_question_replacement_with_age")
        
        return None
    
    def _detect_performance_question_needing_clarification(
        self, question: str, language: str = "fr"
    ) -> Optional[Dict[str, Any]]:
        """D√©tection am√©lior√©e des questions techniques n√©cessitant race/sexe (CONSERV√â)"""
        
        question_lower = question.lower()
        
        weight_age_patterns = {
            "fr": [
                r'(?:poids|p√®se)\s+.*?(\d+)\s*(?:jour|semaine)s?',
                r'(\d+)\s*(?:jour|semaine)s?.*?(?:poids|p√®se)',
                r'(?:quel|combien)\s+.*?(?:poids|p√®se).*?(\d+)',
                r'(?:croissance|d√©veloppement).*?(\d+)\s*(?:jour|semaine)',
                r'(\d+)\s*(?:jour|semaine).*?(?:normal|r√©f√©rence|standard)'
            ],
            "en": [
                r'(?:weight|weigh)\s+.*?(\d+)\s*(?:day|week)s?',
                r'(\d+)\s*(?:day|week)s?.*?(?:weight|weigh)',
                r'(?:what|how much)\s+.*?(?:weight|weigh).*?(\d+)',
                r'(?:growth|development).*?(\d+)\s*(?:day|week)',
                r'(\d+)\s*(?:day|week).*?(?:normal|reference|standard)'
            ],
            "es": [
                r'(?:peso|pesa)\s+.*?(\d+)\s*(?:d√≠a|semana)s?',
                r'(\d+)\s*(?:d√≠a|semana)s?.*?(?:peso|pesa)',
                r'(?:cu√°l|cu√°nto)\s+.*?(?:peso|pesa).*?(\d+)',
                r'(?:crecimiento|desarrollo).*?(\d+)\s*(?:d√≠a|semana)',
                r'(\d+)\s*(?:d√≠a|semana).*?(?:normal|referencia|est√°ndar)'
            ]
        }
        
        patterns = weight_age_patterns.get(language, weight_age_patterns["fr"])
        
        age_detected = None
        for pattern in patterns:
            match = re.search(pattern, question_lower)
            if match:
                age_detected = match.group(1)
                break
        
        if not age_detected:
            return None
        
        breed_patterns = [
            r'\b(ross\s*308|ross\s*708|cobb\s*500|cobb\s*700|hubbard|arbor\s*acres)\b',
            r'\b(broiler|poulet|chicken|pollo)\s+(ross|cobb|hubbard)',
            r'\brace\s*[:\-]?\s*(ross|cobb|hubbard)'
        ]
        
        sex_patterns = [
            r'\b(m√¢le|male|macho)s?\b',
            r'\b(femelle|female|hembra)s?\b',
            r'\b(coq|hen|poule|gallina)\b',
            r'\b(mixte|mixed|misto)\b'
        ]
        
        has_breed = any(re.search(pattern, question_lower, re.IGNORECASE) for pattern in breed_patterns)
        has_sex = any(re.search(pattern, question_lower, re.IGNORECASE) for pattern in sex_patterns)
        
        if not has_breed and not has_sex:
            return {
                "type": "performance_question_missing_breed_sex",
                "age_detected": age_detected,
                "question_type": "weight_performance",
                "missing_info": ["breed", "sex"],
                "confidence": 0.95
            }
        
        elif not has_breed or not has_sex:
            missing = []
            if not has_breed:
                missing.append("breed")
            if not has_sex:
                missing.append("sex")
            
            return {
                "type": "performance_question_partial_info",
                "age_detected": age_detected,
                "question_type": "weight_performance", 
                "missing_info": missing,
                "confidence": 0.8
            }
        
        return None

    def _generate_performance_clarification_response(
        self, question: str, clarification_info: Dict, language: str, conversation_id: str
    ) -> EnhancedExpertResponse:
        """G√©n√®re la demande de clarification optimis√©e (CONSERV√â)"""
        
        age = clarification_info.get("age_detected", "X")
        missing_info = clarification_info.get("missing_info", [])
        
        clarification_messages = {
            "fr": {
                "both_missing": f"Pour vous donner le poids de r√©f√©rence exact d'un poulet de {age} jours, j'ai besoin de :\n\n‚Ä¢ **Race/souche** : Ross 308, Cobb 500, Hubbard, etc.\n‚Ä¢ **Sexe** : M√¢les, femelles, ou troupeau mixte\n\nPouvez-vous pr√©ciser ces informations ?",
                "breed_missing": f"Pour le poids exact √† {age} jours, quelle est la **race/souche** (Ross 308, Cobb 500, Hubbard, etc.) ?",
                "sex_missing": f"Pour le poids exact √† {age} jours, s'agit-il de **m√¢les, femelles, ou d'un troupeau mixte** ?"
            },
            "en": {
                "both_missing": f"To give you the exact reference weight for a {age}-day chicken, I need:\n\n‚Ä¢ **Breed/strain**: Ross 308, Cobb 500, Hubbard, etc.\n‚Ä¢ **Sex**: Males, females, or mixed flock\n\nCould you specify this information?",
                "breed_missing": f"For the exact weight at {age} days, what is the **breed/strain** (Ross 308, Cobb 500, Hubbard, etc.)?",
                "sex_missing": f"For the exact weight at {age} days, are these **males, females, or a mixed flock**?"
            },
            "es": {
                "both_missing": f"Para darle el peso de referencia exacto de un pollo de {age} d√≠as, necesito:\n\n‚Ä¢ **Raza/cepa**: Ross 308, Cobb 500, Hubbard, etc.\n‚Ä¢ **Sexo**: Machos, hembras, o lote mixto\n\n¬øPodr√≠a especificar esta informaci√≥n?",
                "breed_missing": f"Para el peso exacto a los {age} d√≠as, ¬øcu√°l es la **raza/cepa** (Ross 308, Cobb 500, Hubbard, etc.)?",
                "sex_missing": f"Para el peso exacto a los {age} d√≠as, ¬øson **machos, hembras, o un lote mixto**?"
            }
        }
        
        messages = clarification_messages.get(language, clarification_messages["fr"])
        
        if len(missing_info) >= 2:
            response_text = messages["both_missing"]
        elif "breed" in missing_info:
            response_text = messages["breed_missing"]
        else:
            response_text = messages["sex_missing"]
        
        examples = {
            "fr": "\n\n**Exemples de r√©ponses :**\n‚Ä¢ \"Ross 308 m√¢les\"\n‚Ä¢ \"Cobb 500 femelles\"\n‚Ä¢ \"Hubbard troupeau mixte\"",
            "en": "\n\n**Example responses:**\n‚Ä¢ \"Ross 308 males\"\n‚Ä¢ \"Cobb 500 females\"\n‚Ä¢ \"Hubbard mixed flock\"",
            "es": "\n\n**Ejemplos de respuestas:**\n‚Ä¢ \"Ross 308 machos\"\n‚Ä¢ \"Cobb 500 hembras\"\n‚Ä¢ \"Hubbard lote mixto\""
        }
        
        response_text += examples.get(language, examples["fr"])
        
        return EnhancedExpertResponse(
            question=question,
            response=response_text,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=50,
            mode="smart_performance_clarification_corrected",
            user=None,
            logged=True,
            validation_passed=True,
            clarification_result={
                "clarification_requested": True,
                "clarification_type": "performance_breed_sex",
                "missing_information": missing_info,
                "age_detected": age,
                "confidence": clarification_info.get("confidence", 0.9)
            },
            processing_steps=["smart_clarification_triggered"],
            ai_enhancements_used=["performance_question_detection", "targeted_clarification"]
        )
    
    def _generate_follow_up_clarification(
        self, question: str, validation: Dict, language: str, conversation_id: str
    ) -> EnhancedExpertResponse:
        """G√©n√®re une clarification de suivi si premi√®re r√©ponse incompl√®te (CONSERV√â)"""
        
        still_missing = validation["still_missing"]
        
        messages = {
            "fr": {
                "breed": "Il me manque encore la **race/souche**. Ross 308, Cobb 500, ou autre ?",
                "sex": "Il me manque encore le **sexe**. M√¢les, femelles, ou troupeau mixte ?",  
                "both": "Il me manque encore la **race et le sexe**. Exemple : \"Ross 308 m√¢les\""
            },
            "en": {
                "breed": "I still need the **breed/strain**. Ross 308, Cobb 500, or other?",
                "sex": "I still need the **sex**. Males, females, or mixed flock?",
                "both": "I still need the **breed and sex**. Example: \"Ross 308 males\""
            },
            "es": {
                "breed": "A√∫n necesito la **raza/cepa**. ¬øRoss 308, Cobb 500, u otra?",
                "sex": "A√∫n necesito el **sexo**. ¬øMachos, hembras, o lote mixto?",
                "both": "A√∫n necesito la **raza y sexo**. Ejemplo: \"Ross 308 machos\""
            }
        }
        
        lang_messages = messages.get(language, messages["fr"])
        
        if len(still_missing) >= 2:
            message = lang_messages["both"]
        elif "breed" in still_missing:
            message = lang_messages["breed"]
        else:
            message = lang_messages["sex"]
        
        return EnhancedExpertResponse(
            question=question,
            response=message,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=30,
            mode="follow_up_clarification_corrected",
            user=None,
            logged=True,
            validation_passed=True,
            clarification_result={
                "clarification_requested": True,
                "clarification_type": "follow_up_incomplete",
                "still_missing_information": still_missing,
                "confidence": validation["confidence"]
            },
            processing_steps=["follow_up_clarification_triggered"],
            ai_enhancements_used=["incomplete_clarification_handling"]
        )
    
    # === M√âTHODES D'ENRICHISSEMENT COMPLET (CONSERV√âES IDENTIQUES) ===
    
    def _extract_age_from_original_question(self, original_question: str, language: str = "fr") -> Dict[str, Any]:
        """Extrait l'√¢ge depuis la question originale (CONSERV√â)"""
        
        age_info = {"days": None, "weeks": None, "text": None, "detected": False}
        
        if not original_question:
            return age_info
        
        question_lower = original_question.lower()
        
        age_patterns = {
            "fr": [
                (r'(\d+)\s*jours?', "days"),
                (r'(\d+)\s*semaines?', "weeks"),
                (r'de\s+(\d+)\s*jours?', "days"),
                (r'de\s+(\d+)\s*semaines?', "weeks"),
                (r'√†\s+(\d+)\s*jours?', "days"),
                (r'√†\s+(\d+)\s*semaines?', "weeks")
            ],
            "en": [
                (r'(\d+)\s*days?', "days"),
                (r'(\d+)\s*weeks?', "weeks"),
                (r'of\s+(\d+)\s*days?', "days"),
                (r'of\s+(\d+)\s*weeks?', "weeks"),
                (r'at\s+(\d+)\s*days?', "days"),
                (r'at\s+(\d+)\s*weeks?', "weeks")
            ],
            "es": [
                (r'(\d+)\s*d√≠as?', "days"),
                (r'(\d+)\s*semanas?', "weeks"),
                (r'de\s+(\d+)\s*d√≠as?', "days"),
                (r'de\s+(\d+)\s*semanas?', "weeks"),
                (r'a\s+(\d+)\s*d√≠as?', "days"),
                (r'a\s+(\d+)\s*semanas?', "weeks")
            ]
        }
        
        patterns = age_patterns.get(language, age_patterns["fr"])
        
        for pattern, unit in patterns:
            match = re.search(pattern, question_lower, re.IGNORECASE)
            if match:
                value = int(match.group(1))
                
                if unit == "days":
                    age_info["days"] = value
                    age_info["weeks"] = round(value / 7, 1)
                    age_info["text"] = f"{value} jour{'s' if value > 1 else ''}"
                else:
                    age_info["weeks"] = value
                    age_info["days"] = value * 7
                    age_info["text"] = f"{value} semaine{'s' if value > 1 else ''}"
                
                age_info["detected"] = True
                
                logger.info(f"üïê [Age Extraction] √Çge d√©tect√©: {age_info['text']} ({age_info['days']} jours)")
                break
        
        if not age_info["detected"]:
            logger.warning(f"‚ö†Ô∏è [Age Extraction] Aucun √¢ge d√©tect√© dans: {original_question}")
        
        return age_info

    def _build_complete_enriched_question(
        self, 
        original_question: str, 
        breed: Optional[str], 
        sex: Optional[str], 
        age_info: Dict[str, Any], 
        language: str = "fr"
    ) -> str:
        """Construit une question compl√®tement enrichie (CONSERV√â)"""
        
        templates = {
            "fr": {
                "complete": "Pour des poulets {breed} {sex} de {age}",
                "breed_age": "Pour des poulets {breed} de {age}",
                "sex_age": "Pour des poulets {sex} de {age}",
                "breed_sex": "Pour des poulets {breed} {sex}",
                "age_only": "Pour des poulets de {age}",
                "breed_only": "Pour des poulets {breed}",
                "sex_only": "Pour des poulets {sex}"
            },
            "en": {
                "complete": "For {breed} {sex} chickens at {age}",
                "breed_age": "For {breed} chickens at {age}",
                "sex_age": "For {sex} chickens at {age}",
                "breed_sex": "For {breed} {sex} chickens",
                "age_only": "For chickens at {age}",
                "breed_only": "For {breed} chickens",
                "sex_only": "For {sex} chickens"
            },
            "es": {
                "complete": "Para pollos {breed} {sex} de {age}",
                "breed_age": "Para pollos {breed} de {age}",
                "sex_age": "Para pollos {sex} de {age}",
                "breed_sex": "Para pollos {breed} {sex}",
                "age_only": "Para pollos de {age}",
                "breed_only": "Para pollos {breed}",
                "sex_only": "Para pollos {sex}"
            }
        }
        
        template_set = templates.get(language, templates["fr"])
        
        context_prefix = ""
        age_text = age_info.get("text") if age_info.get("detected") else None
        
        if breed and sex and age_text:
            context_prefix = template_set["complete"].format(
                breed=breed, sex=sex, age=age_text
            )
            logger.info(f"üåü [Complete Enrichment] Contexte COMPLET: {context_prefix}")
            
        elif breed and age_text:
            context_prefix = template_set["breed_age"].format(
                breed=breed, age=age_text
            )
            logger.info(f"üè∑Ô∏è [Breed+Age] Contexte: {context_prefix}")
            
        elif sex and age_text:
            context_prefix = template_set["sex_age"].format(
                sex=sex, age=age_text
            )
            logger.info(f"‚öß [Sex+Age] Contexte: {context_prefix}")
            
        elif breed and sex:
            context_prefix = template_set["breed_sex"].format(
                breed=breed, sex=sex
            )
            logger.info(f"üè∑Ô∏è‚öß [Breed+Sex] Contexte: {context_prefix}")
            
        elif age_text:
            context_prefix = template_set["age_only"].format(age=age_text)
            logger.info(f"üïê [Age Only] Contexte: {context_prefix}")
            
        elif breed:
            context_prefix = template_set["breed_only"].format(breed=breed)
            logger.info(f"üè∑Ô∏è [Breed Only] Contexte: {context_prefix}")
            
        elif sex:
            context_prefix = template_set["sex_only"].format(sex=sex)
            logger.info(f"‚öß [Sex Only] Contexte: {context_prefix}")
        
        if context_prefix:
            original_lower = original_question.lower().strip()
            
            if "quel est" in original_lower or "what is" in original_lower or "cu√°l es" in original_lower:
                enriched_question = f"{context_prefix}, {original_lower}"
            elif "comment" in original_lower or "how" in original_lower or "c√≥mo" in original_lower:
                enriched_question = f"{context_prefix}: {original_question}"
            else:
                enriched_question = f"{context_prefix}: {original_question}"
            
            logger.info(f"‚ú® [Final Enrichment] Question finale: {enriched_question}")
            return enriched_question
        
        else:
            logger.warning("‚ö†Ô∏è [Enrichment] Pas d'enrichissement possible, question originale conserv√©e")
            return original_question
    
    # === M√âTHODES UTILITAIRES AVEC AUTO-CLARIFICATION SIMPLIFI√âE ===
    
    def _create_vagueness_response(
        self, vagueness_result, question_text: str, conversation_id: str,
        language: str, start_time: float, processing_steps: list, ai_enhancements_used: list
    ) -> EnhancedExpertResponse:
        """Cr√©e une r√©ponse sp√©cialis√©e pour questions floues (CONSERV√â)"""
        
        clarification_messages = {
            "fr": f"Votre question semble manquer de pr√©cision. {vagueness_result.suggested_clarification or 'Pouvez-vous √™tre plus sp√©cifique ?'}",
            "en": f"Your question seems to lack precision. {vagueness_result.suggested_clarification or 'Could you be more specific?'}",
            "es": f"Su pregunta parece carecer de precisi√≥n. {vagueness_result.suggested_clarification or '¬øPodr√≠a ser m√°s espec√≠fico?'}"
        }
        
        response_message = clarification_messages.get(language, clarification_messages["fr"])
        
        if vagueness_result.question_clarity in ["very_unclear", "unclear"]:
            examples = {
                "fr": "\n\nExemples de questions pr√©cises:\n‚Ä¢ Quel est le poids normal d'un Ross 308 de 21 jours?\n‚Ä¢ Comment traiter la mortalit√© √©lev√©e chez des poulets de 3 semaines?\n‚Ä¢ Quelle temp√©rature maintenir pour des poussins de 7 jours?",
                "en": "\n\nExamples of precise questions:\n‚Ä¢ What is the normal weight of a 21-day Ross 308?\n‚Ä¢ How to treat high mortality in 3-week-old chickens?\n‚Ä¢ What temperature to maintain for 7-day chicks?",
                "es": "\n\nEjemplos de preguntas precisas:\n‚Ä¢ ¬øCu√°l es el peso normal de un Ross 308 de 21 d√≠as?\n‚Ä¢ ¬øC√≥mo tratar la alta mortalidad en pollos de 3 semanas?\n‚Ä¢ ¬øQu√© temperatura mantener para pollitos de 7 d√≠as?"
            }
            response_message += examples.get(language, examples["fr"])
        
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return EnhancedExpertResponse(
            question=question_text,
            response=response_message,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode="vagueness_clarification",
            user=None,
            logged=True,
            validation_passed=True,
            vagueness_detection=vagueness_result,
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used
        )
    
    def _build_final_enhanced_response(
        self, question_text: str, answer: str, conversation_id: str,
        user_email: Optional[str], language: str, response_time_ms: int,
        expert_result: Dict, validation_result: ValidationResult,
        conversation_context: Any, processing_steps: list,
        ai_enhancements_used: list, request_data: EnhancedQuestionRequest,
        debug_info: Dict, performance_breakdown: Dict
    ) -> EnhancedExpertResponse:
        """Construit la r√©ponse finale avec toutes les am√©liorations + auto-clarification simplifi√©e"""
        
        extracted_entities = expert_result.get("extracted_entities")
        confidence_overall = None
        conversation_state = None
        
        if conversation_context and hasattr(conversation_context, 'consolidated_entities'):
            if not extracted_entities:
                extracted_entities = conversation_context.consolidated_entities.to_dict()
            confidence_overall = conversation_context.consolidated_entities.confidence_overall
            conversation_state = conversation_context.conversation_urgency
        
        final_debug_info = None
        final_performance = None
        
        if request_data.debug_mode:
            final_debug_info = {
                **debug_info,
                "total_processing_time_ms": response_time_ms,
                "ai_enhancements_count": len(ai_enhancements_used),
                "processing_steps_count": len(processing_steps),
                "concision_applied": expert_result.get("concision_applied", False),
                "response_versions_support": True,
                "taxonomy_used": expert_result.get("taxonomy_used"),
                "taxonomy_filters_applied": expert_result.get("taxonomy_filters_applied", False),
                "semantic_dynamic_available": True,
                "auto_clarification_simplified": True
            }
            
            final_performance = performance_breakdown
        
        concision_info = {
            "concision_applied": expert_result.get("concision_applied", False),
            "original_response_available": "original_answer" in expert_result,
            "detected_question_type": None,
            "applied_concision_level": None,
            "response_versions_supported": True
        }
        
        if expert_result.get("concision_applied"):
            concision_info["detected_question_type"] = self.concision_processor.detect_question_type(question_text)
            concision_info["applied_concision_level"] = self.concision_processor.detect_optimal_concision_level(question_text).value
        
        taxonomy_info = {
            "taxonomy_detected": expert_result.get("taxonomy_used", "general"),
            "taxonomy_filters_applied": expert_result.get("taxonomy_filters_applied", False),
            "taxonomy_enhanced_question": bool(expert_result.get("taxonomy_used") != "general")
        }
        
        semantic_dynamic_info = {
            "semantic_dynamic_available": getattr(request_data, 'semantic_dynamic_mode', False),
            "semantic_dynamic_used": "semantic_dynamic_clarification" in ai_enhancements_used,
            "dynamic_questions_generated": any("semantic_dynamic" in step for step in processing_steps)
        }
        
        auto_clarification_info = {
            "auto_clarification_simplified": True,
            "auto_clarification_used": "auto_clarification_system" in ai_enhancements_used,
            "auto_clarification_trigger": "auto_clarification_triggered" in processing_steps,
            "completeness_scoring_used": True,
            "validation_robuste_active": True,
            "fallback_intelligent_active": True
        }
            
        return EnhancedExpertResponse(
            question=str(question_text),
            response=str(answer),
            conversation_id=conversation_id,
            rag_used=expert_result["rag_used"],
            rag_score=expert_result["rag_score"],
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode=expert_result["mode"],
            user=user_email,
            logged=True,
            validation_passed=True,
            validation_confidence=validation_result.confidence,
            reprocessed_after_clarification=request_data.is_clarification_response,
            conversation_state=conversation_state,
            extracted_entities=extracted_entities,
            confidence_overall=confidence_overall,
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used,
            
            document_relevance=expert_result.get("document_relevance"),
            context_coherence=expert_result.get("context_coherence"),
            vagueness_detection=None,
            fallback_details=None,
            response_format_applied=request_data.expected_response_format.value,
            quality_metrics=expert_result.get("quality_metrics"),
            debug_info=final_debug_info,
            performance_breakdown=final_performance,
            
            concision_info=concision_info,
            original_response=expert_result.get("original_answer"),
            
            response_versions=None,
            concision_metrics=None,
            
            taxonomy_info=taxonomy_info,
            
            semantic_dynamic_info=semantic_dynamic_info,
            
            auto_clarification_info=auto_clarification_info
        )
    
    # === M√âTHODES UTILITAIRES IDENTIQUES (CONSERV√âES) ===
    
    def _extract_user_id(self, current_user: Optional[Dict], request_data: EnhancedQuestionRequest, request: Request) -> str:
        if current_user:
            return current_user.get("user_id") or request_data.user_id or "authenticated_user"
        return request_data.user_id or get_user_id_from_request(request)
    
    def _get_or_create_conversation_id(self, request_data: EnhancedQuestionRequest) -> str:
        if request_data.conversation_id and request_data.conversation_id.strip():
            conversation_id = request_data.conversation_id.strip()
            logger.info(f"üîÑ [Expert Service] CONTINUATION: {conversation_id}")
            return conversation_id
        else:
            conversation_id = str(uuid.uuid4())
            logger.info(f"üÜï [Expert Service] NOUVELLE: {conversation_id}")
            return conversation_id
    
    def _validate_rag_response_quality(
        self, answer: str, enriched_question: str, enhancement_info: Dict
    ) -> Dict[str, any]:
        """Valide la qualit√© de la r√©ponse RAG (CONSERV√â)"""
        
        if not answer or len(answer.strip()) < 20:
            return {
                "valid": False,
                "reason": "R√©ponse trop courte",
                "answer_length": len(answer) if answer else 0
            }
        
        negative_responses = [
            "je ne sais pas", "i don't know", "no s√©",
            "pas d'information", "no information", "sin informaci√≥n"
        ]
        
        answer_lower = answer.lower()
        for negative in negative_responses:
            if negative in answer_lower:
                return {
                    "valid": False,
                    "reason": f"R√©ponse n√©gative d√©tect√©e: {negative}",
                    "answer_length": len(answer)
                }
        
        if any(word in enriched_question.lower() for word in ["poids", "weight", "peso"]):
            if not re.search(r'\d+', answer):
                return {
                    "valid": False,
                    "reason": "Question num√©rique mais pas de chiffres dans la r√©ponse",
                    "answer_length": len(answer)
                }
        
        return {
            "valid": True,
            "reason": "R√©ponse valide",
            "answer_length": len(answer)
        }
    
    # === AUTRES M√âTHODES CONSERV√âES ===
    
    async def _validate_agricultural_question(self, question: str, language: str, user_id: str, request_ip: str, conversation_id: str) -> ValidationResult:
        if not self.integrations.agricultural_validator_available:
            return ValidationResult(is_valid=False, rejection_message="Service temporairement indisponible")
        
        try:
            enriched_question = question
            if self.integrations.intelligent_memory_available:
                try:
                    rag_context = self.integrations.get_context_for_rag(conversation_id)
                    if rag_context:
                        enriched_question = f"{question}\n\nContexte: {rag_context}"
                except Exception:
                    pass
            
            validation_result = self.integrations.validate_agricultural_question(
                question=enriched_question, language=language, user_id=user_id, request_ip=request_ip
            )
            
            return ValidationResult(
                is_valid=validation_result.is_valid,
                rejection_message=validation_result.reason or "Question hors domaine agricole",
                confidence=validation_result.confidence
            )
            
        except Exception as e:
            logger.error(f"‚ùå [Expert Service] Erreur validateur: {e}")
            return ValidationResult(is_valid=False, rejection_message="Erreur de validation")
    
    def _create_rejection_response(self, question_text, validation_result, conversation_id, user_email, language, start_time, processing_steps, ai_enhancements_used, vagueness_result=None):
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return EnhancedExpertResponse(
            question=str(question_text),
            response=str(validation_result.rejection_message),
            conversation_id=conversation_id,
            rag_used=False,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode="enhanced_agricultural_validation_rejected",
            user=user_email,
            logged=True,
            validation_passed=False,
            validation_confidence=validation_result.confidence,
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used,
            vagueness_detection=vagueness_result
        )
    
    async def process_feedback(self, feedback_data: FeedbackRequest) -> Dict[str, Any]:
        feedback_updated = False
        
        if feedback_data.conversation_id and self.integrations.logging_available:
            try:
                rating_numeric = {"positive": 1, "negative": -1, "neutral": 0}.get(feedback_data.rating, 0)
                feedback_updated = await self.integrations.update_feedback(feedback_data.conversation_id, rating_numeric)
            except Exception as e:
                logger.error(f"‚ùå [Expert Service] Erreur feedback: {e}")
        
        return {
            "success": True,
            "message": "Feedback enregistr√© avec succ√®s (Enhanced + Auto-Clarification Simplifi√©e + Validation Robuste + Fallback Intelligent)",
            "rating": feedback_data.rating,
            "comment": feedback_data.comment,
            "conversation_id": feedback_data.conversation_id,
            "feedback_updated_in_db": feedback_updated,
            "enhanced_features_used": True,
            "concision_system_active": self.concision_processor.config.ENABLE_CONCISE_RESPONSES,
            "response_versions_supported": True,
            "taxonomic_filtering_active": True,
            "semantic_dynamic_available": True,
            "auto_clarification_simplified": True,
            "validation_robuste_active": True,
            "fallback_intelligent_active": True,
            "timestamp": datetime.now().isoformat()
        }
    
    async def get_suggested_topics(self, language: str) -> Dict[str, Any]:
        lang = language.lower() if language else "fr"
        if lang not in ["fr", "en", "es"]:
            lang = "fr"
        
        topics_by_language = get_enhanced_topics_by_language()
        topics = topics_by_language.get(lang, topics_by_language["fr"])
        
        return {
            "topics": topics,
            "language": lang,
            "count": len(topics),
            "enhanced_features": {
                "vagueness_detection_available": True,
                "context_coherence_available": True,
                "detailed_rag_scoring_available": True,
                "quality_metrics_available": True,
                "smart_clarification_available": True,
                "intelligent_memory_available": self.integrations.intelligent_memory_available,
                
                "response_concision_available": True,
                "concision_levels": [level.value for level in ConcisionLevel],
                "auto_concision_detection": True,
                "concision_enabled": self.concision_processor.config.ENABLE_CONCISE_RESPONSES,
                
                "response_versions_available": True,
                "multiple_concision_levels_generation": True,
                "dynamic_level_switching_support": True,
                "concision_metrics_available": True,
                
                "taxonomic_filtering_available": True,
                "supported_taxonomies": ["broiler", "layer", "swine", "dairy", "general"],
                "automatic_taxonomy_detection": True,
                "taxonomy_based_document_filtering": True,
                
                "semantic_dynamic_clarification_available": True,
                "gpt_question_generation": True,
                "contextual_clarification_questions": True,
                "intelligent_clarification_mode": True,
                
                "auto_clarification_simplified": True,
                "validation_robuste": True,
                "fallback_intelligent": True,
                "scoring_questions_generees": True,
                "integration_visible_expert_services": True
            },
            "system_status": {
                "validation_enabled": self.integrations.is_agricultural_validation_enabled(),
                "enhanced_clarification_enabled": self.integrations.is_enhanced_clarification_enabled(),
                "intelligent_memory_enabled": self.integrations.intelligent_memory_available,
                "api_enhancements_enabled": True,
                "concision_processor_enabled": True,
                "response_versions_generator_enabled": True,
                "taxonomic_filtering_enabled": True,
                "semantic_dynamic_clarification_enabled": True,
                "auto_clarification_simplified_enabled": True,
                "validation_robuste_enabled": True,
                "fallback_intelligent_enabled": True
            },
            
            "concision_config": {
                "default_level": self.concision_processor.config.DEFAULT_CONCISION_LEVEL.value,
                "auto_detect_enabled": True,
                "max_lengths": self.concision_processor.config.MAX_RESPONSE_LENGTH,
                "ultra_concise_keywords": self.concision_processor.config.ULTRA_CONCISE_KEYWORDS,
                "complex_keywords": self.concision_processor.config.COMPLEX_KEYWORDS,
                
                "response_versions_generation": {
                    "enabled": True,
                    "supported_levels": [level.value for level in ConcisionLevel],
                    "metrics_included": True,
                    "cache_supported": False,
                    "fallback_strategy": "simple_truncation"
                }
            },
            
            "taxonomic_config": {
                "enabled": True,
                "supported_categories": {
                    "broiler": ["ross", "cobb", "hubbard", "indian river"],
                    "layer": ["lohmann", "isa", "dekalb", "hy-line", "bovans", "h&n", "shaver"],
                    "swine": ["gestation_day", "parity"],
                    "dairy": ["days_in_milk", "milk_yield_liters"]
                },
                "auto_detection_enabled": True,
                "filter_fallback_enabled": True,
                "question_enhancement_enabled": True
            },
            
            "semantic_dynamic_config": {
                "enabled": True,
                "max_questions_generated": 4,
                "supported_languages": ["fr", "en", "es"],
                "gpt_model_used": "gpt-4o-mini",
                "fallback_questions_available": True,
                "context_aware_generation": True,
                "automatic_mode_detection": True
            },
            
            "auto_clarification_simplified_config": {
                "enabled": True,
                "completeness_score_threshold": 0.5,
                "validation_robuste_enabled": True,
                "fallback_intelligent_enabled": True,
                "scoring_questions_enabled": True,
                "integration_visible": True,
                "centralized_function": "auto_clarify_if_needed",
                "fallback_questions_by_type": True,
                "gpt_error_handling": True
            }
        }

# =============================================================================
# üÜï API ENDPOINT POUR CONTR√îLER LA CONCISION + RESPONSE VERSIONS + TAXONOMIC FILTERING + SEMANTIC DYNAMIC + AUTO CLARIFICATION SIMPLIFI√âE (OPTIONNEL)
# =============================================================================

def create_concision_control_endpoint():
    """
    Endpoint optionnel pour contr√¥ler la concision c√¥t√© backend
    üöÄ MODIFI√â: Ajout support auto-clarification simplifi√©e + validation robuste + fallback intelligent
    """
    
    from fastapi import APIRouter
    from pydantic import BaseModel
    
    router = APIRouter()
    
    class ConcisionSettingsRequest(BaseModel):
        enabled: bool = True
        default_level: ConcisionLevel = ConcisionLevel.CONCISE
        max_lengths: Optional[Dict[str, int]] = None
        enable_response_versions: bool = True
        enable_taxonomic_filtering: bool = True
        enable_semantic_dynamic: bool = True
        enable_auto_clarification_simplified: bool = True
        auto_clarification_threshold: float = 0.5
        enable_validation_robuste: bool = True
        enable_fallback_intelligent: bool = True
    
    class ConcisionSettingsResponse(BaseModel):
        success: bool
        current_settings: Dict[str, Any]
        message: str
    
    @router.post("/concision/settings", response_model=ConcisionSettingsResponse)
    async def update_concision_settings(request: ConcisionSettingsRequest):
        """Mettre √† jour les param√®tres de concision + auto-clarification simplifi√©e du syst√®me"""
        
        try:
            ConcisionConfig.ENABLE_CONCISE_RESPONSES = request.enabled
            ConcisionConfig.DEFAULT_CONCISION_LEVEL = request.default_level
            
            if request.max_lengths:
                ConcisionConfig.MAX_RESPONSE_LENGTH.update(request.max_lengths)
            
            return ConcisionSettingsResponse(
                success=True,
                current_settings={
                    "enabled": ConcisionConfig.ENABLE_CONCISE_RESPONSES,
                    "default_level": ConcisionConfig.DEFAULT_CONCISION_LEVEL.value,
                    "max_lengths": ConcisionConfig.MAX_RESPONSE_LENGTH,
                    "response_versions_enabled": request.enable_response_versions,
                    "taxonomic_filtering_enabled": request.enable_taxonomic_filtering,
                    "semantic_dynamic_enabled": request.enable_semantic_dynamic,
                    "auto_clarification_simplified_enabled": request.enable_auto_clarification_simplified,
                    "auto_clarification_threshold": request.auto_clarification_threshold,
                    "validation_robuste_enabled": request.enable_validation_robuste,
                    "fallback_intelligent_enabled": request.enable_fallback_intelligent
                },
                message="Param√®tres de concision + auto-clarification simplifi√©e + validation robuste + fallback intelligent mis √† jour avec succ√®s"
            )
        except Exception as e:
            return ConcisionSettingsResponse(
                success=False,
                current_settings={},
                message=f"Erreur lors de la mise √† jour: {str(e)}"
            )
    
    @router.get("/concision/settings", response_model=Dict[str, Any])
    async def get_concision_settings():
        """R√©cup√©rer les param√®tres actuels de concision + auto-clarification simplifi√©e"""
        
        return {
            "enabled": ConcisionConfig.ENABLE_CONCISE_RESPONSES,
            "default_level": ConcisionConfig.DEFAULT_CONCISION_LEVEL.value,
            "available_levels": [level.value for level in ConcisionLevel],
            "max_lengths": ConcisionConfig.MAX_RESPONSE_LENGTH,
            "ultra_concise_keywords": ConcisionConfig.ULTRA_CONCISE_KEYWORDS,
            "complex_keywords": ConcisionConfig.COMPLEX_KEYWORDS,
            
            "response_versions": {
                "supported": True,
                "generation_method": "existing_processor_based",
                "cache_enabled": False,
                "fallback_enabled": True,
                "metrics_included": True
            },
            
            "taxonomic_filtering": {
                "supported": True,
                "auto_detection_enabled": True,
                "supported_taxonomies": ["broiler", "layer", "swine", "dairy", "general"],
                "question_enhancement_enabled": True,
                "filter_fallback_enabled": True
            },
            
            "semantic_dynamic": {
                "supported": True,
                "max_questions": 4,
                "supported_languages": ["fr", "en", "es"],
                "gpt_generation_enabled": True,
                "fallback_questions_available": True,
                "contextual_mode_available": True
            },
            
            "auto_clarification_simplified": {
                "supported": True,
                "completeness_evaluation_enabled": True,
                "threshold_configurable": 0.5,
                "validation_robuste_integrated": True,
                "fallback_intelligent_integrated": True,
                "scoring_questions_enabled": True,
                "integration_visible_expert_services": True,
                "centralized_function_available": True,
                "fallback_by_question_type": True
            }
        }
    
    return router

# =============================================================================
# CONFIGURATION FINALE AVEC AUTO-CLARIFICATION SIMPLIFI√âE + VALIDATION ROBUSTE
# =============================================================================

logger.info("üöÄ" * 30)
logger.info("üöÄ [EXPERT SERVICES] VERSION 3.11.0 - AUTO CLARIFICATION SIMPLIFI√âE + VALIDATION ROBUSTE!")
logger.info("üöÄ [MODIFICATIONS APPLIQU√âES] Toutes les am√©liorations demand√©es int√©gr√©es:")
logger.info("   üîß 1. Scoring et validation qualit√© des questions g√©n√©r√©es (validate_dynamic_questions)")
logger.info("   üîß 2. Int√©gration visible dans expert_services.py (auto_clarify_if_needed)")
logger.info("   üîß 3. Fallback lisible si GPT √©choue (_generate_clarification_questions_with_fallback)")
logger.info("üöÄ [INT√âGRATION] Syst√®me complet avec toutes les fonctionnalit√©s:")
logger.info("   ‚úÖ ResponseVersionsGenerator utilise ResponseConcisionProcessor existant")
logger.info("   ‚úÖ G√©n√©ration 4 versions: ultra_concise, concise, standard, detailed")
logger.info("   ‚úÖ S√©lection automatique selon concision_level")
logger.info("   ‚úÖ M√©triques d√©taill√©es avec ConcisionMetrics")
logger.info("   ‚úÖ Compatible avec toute la logique existante")
logger.info("   ‚úÖ Fallback automatique si erreur")
logger.info("   ‚úÖ Support generate_all_versions flag")
logger.info("   üè∑Ô∏è Filtrage taxonomique intelligent des documents")
logger.info("   üè∑Ô∏è D√©tection automatique broiler/layer/swine/dairy")
logger.info("   üè∑Ô∏è Questions enrichies avec contexte taxonomique")
logger.info("   üÜï Mode s√©mantique dynamique de clarification")
logger.info("   üÜï G√©n√©ration GPT de 1-4 questions contextuelles")
logger.info("   üÜï Support param√®tre semantic_dynamic_mode")
logger.info("   üîß NOUVEAU: Auto-clarification simplifi√©e int√©gr√©e dans expert_services")
logger.info("   üîß NOUVEAU: Validation robuste des questions g√©n√©r√©es (score qualit√©)")
logger.info("   üîß NOUVEAU: Fallback intelligent par type de question si GPT √©choue")
logger.info("üöÄ [FONCTIONNALIT√âS NOUVELLES] Modifications demand√©es impl√©ment√©es:")
logger.info("   üîß validate_dynamic_questions(questions, user_question) ‚Üí (score, filtered_questions)")
logger.info("   üîß auto_clarify_if_needed(question, context, language) ‚Üí clarification_result ou None")
logger.info("   üîß _generate_clarification_questions_with_fallback() avec gestion d'erreur compl√®te")
logger.info("   üîß _get_fallback_questions_by_type() pour questions par contexte d√©tect√©")
logger.info("   üîß _calculate_basic_completeness_score() pour √©valuation automatique")
logger.info("   üîß Int√©gration visible dans _process_question_with_auto_clarification()")
logger.info("üöÄ [BACKEND READY] Frontend peut maintenant:")
logger.info("   - Demander concision_level sp√©cifique")
logger.info("   - Recevoir response_versions compl√®tes") 
logger.info("   - Changer niveau dynamiquement c√¥t√© frontend")
logger.info("   - Profiter du cache et performance optimis√©e")
logger.info("   - B√©n√©ficier du filtrage taxonomique automatique")
logger.info("   - Activer le mode s√©mantique dynamique (semantic_dynamic_mode=true)")
logger.info("   - Recevoir questions de clarification intelligentes")
logger.info("   - B√©n√©ficier de l'auto-clarification simplifi√©e si contexte insuffisant")
logger.info("   - Profiter de la validation robuste des questions g√©n√©r√©es")
logger.info("   - Avoir fallback intelligent garanti m√™me si GPT √©choue")
logger.info("üéØ [R√âSULTAT] Question vague ‚Üí Score contexte < 0.5 ‚Üí Auto-clarification avec validation!")
logger.info("üöÄ" * 30)

logger.info("‚úÖ [Expert Service] Services m√©tier EXPERT SYSTEM + AUTO-CLARIFICATION SIMPLIFI√âE + VALIDATION ROBUSTE int√©gr√©")
logger.info("üöÄ [Expert Service] FONCTIONNALIT√âS VERSION 3.11.0:")
logger.info("   - ‚úÖ Syst√®me de concision intelligent multi-niveaux (CONSERV√â)")
logger.info("   - ‚úÖ D√©tection automatique type de question (CONSERV√â)")
logger.info("   - ‚úÖ Nettoyage avanc√© verbosit√© + r√©f√©rences documents (CONSERV√â)")
logger.info("   - ‚úÖ Configuration flexible par type de question (CONSERV√â)")
logger.info("   - ‚úÖ Conservation r√©ponse originale si concision appliqu√©e (CONSERV√â)")
logger.info("   - üöÄ ResponseVersionsGenerator avec syst√®me existant")
logger.info("   - üöÄ G√©n√©ration toutes versions (ultra_concise, concise, standard, detailed)")
logger.info("   - üöÄ ConcisionMetrics avec compression ratios et m√©triques")
logger.info("   - üöÄ Support generate_all_versions flag pour contr√¥le frontend")
logger.info("   - üöÄ S√©lection intelligente version selon concision_level")
logger.info("   - üè∑Ô∏è Filtrage taxonomique intelligent des documents RAG (CONSERV√â)")
logger.info("   - üè∑Ô∏è D√©tection automatique broiler/layer/swine/dairy/general (CONSERV√â)")
logger.info("   - üè∑Ô∏è Enhancement questions avec contexte taxonomique (CONSERV√â)")
logger.info("   - üè∑Ô∏è Filtres RAG adaptatifs selon la taxonomie d√©tect√©e (CONSERV√â)")
logger.info("   - üÜï Mode s√©mantique dynamique de clarification (CONSERV√â)")
logger.info("   - üÜï G√©n√©ration intelligente 1-4 questions via GPT (CONSERV√â)")
logger.info("   - üÜï Support param√®tre semantic_dynamic_mode (CONSERV√â)")
logger.info("   - üîß NOUVEAU: Auto-clarification simplifi√©e int√©gr√©e dans expert_services")
logger.info("   - üîß NOUVEAU: Validation robuste des questions avec scoring qualit√©")
logger.info("   - üîß NOUVEAU: Fallback intelligent si GPT √©choue compl√®tement")
logger.info("   - üîß NOUVEAU: Questions de fallback par type d√©tect√© (poids/sant√©/croissance)")
logger.info("   - üîß NOUVEAU: √âvaluation score de compl√©tude automatique (0.0 √† 1.0)")
logger.info("   - üîß NOUVEAU: Fonction centralis√©e auto_clarify_if_needed()")
logger.info("üîß [Expert Service] FONCTIONNALIT√âS CONSERV√âES:")
logger.info("   - ‚úÖ Syst√®me de clarification intelligent complet")
logger.info("   - ‚úÖ M√©moire conversationnelle enrichie")
logger.info("   - ‚úÖ RAG avec contexte et prompt structur√©")
logger.info("   - ‚úÖ Multi-LLM support et validation agricole")
logger.info("   - ‚úÖ Support multilingue FR/EN/ES")
logger.info("üîß [Expert Service] MODIFICATIONS DEMAND√âES IMPL√âMENT√âES:")
logger.info("   - üéØ 1. validate_dynamic_questions() avec filtrage avanc√© + score qualit√©")
logger.info("   - üéØ 2. Int√©gration visible auto_clarify_if_needed() dans _process_question_with_auto_clarification()")
logger.info("   - üéØ 3. Fallback lisible _generate_clarification_questions_with_fallback() avec gestion try/except")
logger.info("   - üéØ Centralis√© dans bloc auto_clarify_if_needed() pour simplifier")
logger.info("   - üéØ Scoring < 0.5 ‚Üí fallback automatique vers questions par type")
logger.info("   - üéØ Gestion d'erreur compl√®te : import error, GPT error, parsing error")
logger.info("‚ú® [Expert Service] R√âSULTAT FINAL:")
logger.info('   - Question vague: "J\'ai un probl√®me avec mes poulets"')
logger.info('   - √âvaluation compl√©tude: score < 0.5')
logger.info('   - G√©n√©ration GPT: 3-4 questions contextuelles')
logger.info('   - Validation robuste: filtrage + score qualit√©')
logger.info('   - Si √©chec: fallback intelligent par type de question garanti')
logger.info('   - Int√©gration visible dans expert_services pour contr√¥le total')
logger.info("‚úÖ [Expert Service] AUTO-CLARIFICATION SIMPLIFI√âE + VALIDATION ROBUSTE + FALLBACK INTELLIGENT op√©rationnel!")