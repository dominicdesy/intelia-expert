"""
app/api/v1/expert_services.py - SERVICES MÃ‰TIER EXPERT SYSTEM COMPLETS AVEC AUTO-CLARIFICATION INTÃ‰GRÃ‰E

ðŸš¨ VERSION COMPLÃˆTE 4.0.0 - TOUTES AMÃ‰LIORATIONS INTÃ‰GRÃ‰ES:
1. âœ… SystÃ¨me de concision des rÃ©ponses intÃ©grÃ© (CONSERVÃ‰)
2. âœ… Nettoyage avancÃ© verbositÃ© + rÃ©fÃ©rences documents (CONSERVÃ‰)
3. âœ… Configuration flexible par type de question (CONSERVÃ‰)
4. âœ… DÃ©tection automatique niveau de concision requis (CONSERVÃ‰)
5. âœ… Conservation de toutes les fonctionnalitÃ©s existantes (CONSERVÃ‰)
6. ðŸš€ ResponseVersionsGenerator intÃ©grÃ© (CONSERVÃ‰)
7. ðŸš€ GÃ©nÃ©ration de toutes les versions (ultra_concise, concise, standard, detailed) (CONSERVÃ‰)
8. ðŸš€ Support ConcisionMetrics avec mÃ©triques dÃ©taillÃ©es (CONSERVÃ‰)
9. ðŸš€ SÃ©lection automatique selon concision_level (CONSERVÃ‰)
10. ðŸš€ Support generate_all_versions flag pour frontend (CONSERVÃ‰)
11. ðŸ·ï¸ Filtrage taxonomique intelligent des documents RAG (CONSERVÃ‰)
12. ðŸ·ï¸ DÃ©tection automatique broiler/layer/swine/dairy/general (CONSERVÃ‰)
13. ðŸ·ï¸ Enhancement questions avec contexte taxonomique (CONSERVÃ‰)
14. ðŸ·ï¸ Filtres RAG adaptatifs selon la taxonomie dÃ©tectÃ©e (CONSERVÃ‰)
15. ðŸ†• Mode sÃ©mantique dynamique de clarification intÃ©grÃ© (CONSERVÃ‰)
16. ðŸ†• GÃ©nÃ©ration intelligente de questions contextuelles via GPT (CONSERVÃ‰)
17. ðŸ†• Support paramÃ¨tre semantic_dynamic_mode dans les requÃªtes (CONSERVÃ‰)
18. ðŸ”§ NOUVEAU: DÃ©clenchement automatique clarification si contexte faible
19. ðŸ”§ NOUVEAU: Score de complÃ©tude contexte avec seuils intelligents
20. ðŸ”§ NOUVEAU: Validation automatique questions GPT gÃ©nÃ©rÃ©es avec fallback robuste
21. ðŸ”§ NOUVEAU: IntÃ©gration complÃ¨te dans process_expert_question
22. ðŸ”§ NOUVEAU: Gestion d'erreurs complÃ¨te avec fallback adaptatif

FONCTIONNALITÃ‰S CONSERVÃ‰ES:
- âœ… SystÃ¨me de clarification intelligent complet
- âœ… MÃ©moire conversationnelle
- âœ… RAG avec contexte enrichi
- âœ… Multi-LLM support
- âœ… Validation agricole
- âœ… Support multilingue
"""

import os
import logging
import uuid
import time
import re
from datetime import datetime
from typing import Optional, Dict, Any, Tuple
from enum import Enum

from fastapi import HTTPException, Request

from .expert_models import (
    EnhancedQuestionRequest, EnhancedExpertResponse, FeedbackRequest,
    ValidationResult, ProcessingContext, VaguenessResponse, ResponseFormat,
    ConcisionLevel, ConcisionMetrics, DynamicClarification
)
from .expert_utils import (
    get_user_id_from_request, 
    build_enriched_question_from_clarification,
    get_enhanced_topics_by_language,
    save_conversation_auto_enhanced,
    extract_breed_and_sex_from_clarification,
    build_enriched_question_with_breed_sex,
    validate_clarification_completeness
)
from .expert_integrations import IntegrationsManager
from .api_enhancement_service import APIEnhancementService
from .prompt_templates import build_structured_prompt, extract_context_from_entities, validate_prompt_context, build_clarification_prompt
from .concision_service import concision_service

logger = logging.getLogger(__name__)

# =============================================================================
# ðŸ†• SYSTÃˆME DE CONCISION DES RÃ‰PONSES (CONSERVÃ‰ IDENTIQUE)
# =============================================================================

class ConcisionLevel(Enum):
    """Niveaux de concision disponibles"""
    ULTRA_CONCISE = "ultra_concise"
    CONCISE = "concise"
    STANDARD = "standard"
    DETAILED = "detailed"

class ConcisionConfig:
    """Configuration du systÃ¨me de concision"""
    
    ENABLE_CONCISE_RESPONSES = True
    DEFAULT_CONCISION_LEVEL = ConcisionLevel.CONCISE
    
    MAX_RESPONSE_LENGTH = {
        "weight_question": 80,
        "temperature_question": 60,
        "measurement_question": 70,
        "general_question": 150,
        "complex_question": 300
    }
    
    ULTRA_CONCISE_KEYWORDS = [
        "poids", "weight", "peso",
        "tempÃ©rature", "temperature", "temperatura",
        "combien", "how much", "cuÃ¡nto",
        "quel est", "what is", "cuÃ¡l es"
    ]
    
    COMPLEX_KEYWORDS = [
        "comment", "how to", "cÃ³mo",
        "pourquoi", "why", "por quÃ©", 
        "expliquer", "explain", "explicar",
        "procÃ©dure", "procedure", "procedimiento",
        "protocole", "protocol", "protocolo"
    ]

class ResponseConcisionProcessor:
    """Processeur de concision des rÃ©ponses (CONSERVÃ‰ IDENTIQUE)"""
    
    def __init__(self):
        self.config = ConcisionConfig()
        logger.info("âœ… [Concision] Processeur de concision initialisÃ©")
    
    def detect_question_type(self, question: str) -> str:
        """DÃ©tecte le type de question pour appliquer les bonnes rÃ¨gles"""
        
        question_lower = question.lower().strip()
        
        weight_keywords = ["poids", "weight", "peso", "grammes", "grams", "gramos", "kg"]
        if any(word in question_lower for word in weight_keywords):
            return "weight_question"
        
        temp_keywords = ["tempÃ©rature", "temperature", "temperatura", "Â°c", "degrÃ©", "degree"]
        if any(word in question_lower for word in temp_keywords):
            return "temperature_question"
        
        measurement_keywords = ["taille", "size", "tamaÃ±o", "longueur", "length", "hauteur", "height"]
        if any(word in question_lower for word in measurement_keywords):
            return "measurement_question"
        
        if any(word in question_lower for word in self.config.COMPLEX_KEYWORDS):
            return "complex_question"
        
        return "general_question"
    
    def detect_optimal_concision_level(self, question: str, user_preference: Optional[ConcisionLevel] = None) -> ConcisionLevel:
        """DÃ©tecte le niveau de concision optimal pour une question"""
        
        if user_preference:
            return user_preference
        
        question_lower = question.lower().strip()
        
        if any(keyword in question_lower for keyword in self.config.ULTRA_CONCISE_KEYWORDS):
            return ConcisionLevel.ULTRA_CONCISE
        
        if any(keyword in question_lower for keyword in self.config.COMPLEX_KEYWORDS):
            return ConcisionLevel.DETAILED
        
        return self.config.DEFAULT_CONCISION_LEVEL
    
    def apply_concision(
        self, 
        response: str, 
        question: str, 
        concision_level: ConcisionLevel,
        language: str = "fr"
    ) -> str:
        """
        ðŸš€ MÃ©thode unified pour appliquer la concision
        UtilisÃ©e par ResponseVersionsGenerator
        """
        
        if not self.config.ENABLE_CONCISE_RESPONSES:
            return response
        
        logger.info(f"ðŸŽ¯ [Concision] Application niveau {concision_level.value}")
        
        if concision_level == ConcisionLevel.ULTRA_CONCISE:
            return self._extract_essential_info(response, question, language)
        elif concision_level == ConcisionLevel.CONCISE:
            return self._make_concise(response, question, language)
        elif concision_level == ConcisionLevel.STANDARD:
            return self._remove_excessive_advice(response, language)
        else:
            return self._clean_document_references_only(response)
    
    def process_response(
        self, 
        response: str, 
        question: str, 
        concision_level: Optional[ConcisionLevel] = None,
        language: str = "fr"
    ) -> str:
        """Traite une rÃ©ponse selon le niveau de concision demandÃ© (MÃ‰THODE CONSERVÃ‰E)"""
        
        if not self.config.ENABLE_CONCISE_RESPONSES:
            return response
        
        level = concision_level or self.detect_optimal_concision_level(question)
        
        return self.apply_concision(response, question, level, language)
    
    def _extract_essential_info(self, response: str, question: str, language: str = "fr") -> str:
        """Extrait uniquement l'information essentielle (mode ultra-concis)"""
        
        question_lower = question.lower()
        
        if any(word in question_lower for word in ["poids", "weight", "peso"]):
            weight_patterns = [
                r'(?:entre\s+)?(\d+(?:-\d+|[^\d]*\d+)?)\s*(?:grammes?|g\b)',
                r'(\d+)\s*(?:Ã |to|a)\s*(\d+)\s*(?:grammes?|g\b)',
                r'(\d+)\s*(?:grammes?|g\b)'
            ]
            
            for pattern in weight_patterns:
                match = re.search(pattern, response, re.IGNORECASE)
                if match:
                    if len(match.groups()) >= 2:
                        return f"{match.group(1)}-{match.group(2)}g"
                    else:
                        value = match.group(1)
                        if "entre" in response.lower() or "-" in value:
                            return f"{value}g"
                        else:
                            return f"~{value}g"
        
        if any(word in question_lower for word in ["tempÃ©rature", "temperature"]):
            temp_patterns = [
                r'(\d+(?:-\d+)?)\s*(?:Â°C|degrÃ©s?|degrees?)',
                r'(\d+)\s*(?:Ã |to|a)\s*(\d+)\s*(?:Â°C|degrÃ©s?)'
            ]
            
            for pattern in temp_patterns:
                match = re.search(pattern, response, re.IGNORECASE)
                if match:
                    if len(match.groups()) >= 2:
                        return f"{match.group(1)}-{match.group(2)}Â°C"
                    else:
                        return f"{match.group(1)}Â°C"
        
        sentences = response.split('.')
        for sentence in sentences:
            if re.search(r'\d+', sentence) and len(sentence.strip()) > 10:
                return sentence.strip() + '.'
        
        if sentences:
            return sentences[0].strip() + '.'
        
        return response
    
    def _make_concise(self, response: str, question: str, language: str = "fr") -> str:
        """Rend concis (enlÃ¨ve conseils mais garde info principale)"""
        
        cleaned = self._clean_document_references_only(response)
        
        verbose_patterns = [
            r'\.?\s*Il est essentiel de[^.]*\.',
            r'\.?\s*Assurez-vous de[^.]*\.',
            r'\.?\s*N\'hÃ©sitez pas Ã [^.]*\.',
            r'\.?\s*Pour garantir[^.]*\.',
            r'\.?\s*Il est important de[^.]*\.',
            r'\.?\s*Veillez Ã [^.]*\.',
            r'\.?\s*Il convient de[^.]*\.',
            r'\.?\s*Ã€ ce stade[^.]*\.',
            r'\.?\s*En cas de doute[^.]*\.',
            r'\.?\s*pour favoriser le bien-Ãªtre[^.]*\.',
            r'\.?\s*en termes de[^.]*\.',
            r'\.?\s*It is essential to[^.]*\.',
            r'\.?\s*Make sure to[^.]*\.',
            r'\.?\s*Don\'t hesitate to[^.]*\.',
            r'\.?\s*To ensure[^.]*\.',
            r'\.?\s*It is important to[^.]*\.',
            r'\.?\s*Be sure to[^.]*\.',
            r'\.?\s*At this stage[^.]*\.',
            r'\.?\s*Es esencial[^.]*\.',
            r'\.?\s*AsegÃºrese de[^.]*\.',
            r'\.?\s*No dude en[^.]*\.',
            r'\.?\s*Para garantizar[^.]*\.',
            r'\.?\s*Es importante[^.]*\.',
            r'\.?\s*En esta etapa[^.]*\.',
        ]
        
        for pattern in verbose_patterns:
            cleaned = re.sub(pattern, '.', cleaned, flags=re.IGNORECASE)
        
        cleaned = re.sub(r'\.+', '.', cleaned)
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = cleaned.strip()
        
        if any(word in question.lower() for word in ['poids', 'weight', 'peso']) and len(cleaned) > 100:
            weight_sentence = self._extract_weight_sentence(cleaned)
            if weight_sentence:
                return weight_sentence
        
        return cleaned
    
    def _remove_excessive_advice(self, response: str, language: str = "fr") -> str:
        """EnlÃ¨ve seulement les conseils excessifs (mode standard)"""
        
        cleaned = self._clean_document_references_only(response)
        
        excessive_patterns = [
            r'\.?\s*N\'hÃ©sitez pas Ã [^.]*\.',
            r'\.?\s*Pour des conseils plus personnalisÃ©s[^.]*\.',
            r'\.?\s*Don\'t hesitate to[^.]*\.',
            r'\.?\s*For more personalized advice[^.]*\.',
            r'\.?\s*No dude en[^.]*\.',
            r'\.?\s*Para consejos mÃ¡s personalizados[^.]*\.',
        ]
        
        for pattern in excessive_patterns:
            cleaned = re.sub(pattern, '.', cleaned, flags=re.IGNORECASE)
        
        return re.sub(r'\.+', '.', cleaned).replace(r'\s+', ' ').strip()
    
    def _clean_document_references_only(self, response_text: str) -> str:
        """Nettoie uniquement les rÃ©fÃ©rences aux documents (version originale)"""
        
        if not response_text:
            return response_text
        
        patterns_to_remove = [
            r'selon le document \d+,?\s*',
            r'd\'aprÃ¨s le document \d+,?\s*',
            r'le document \d+ indique que\s*',
            r'comme mentionnÃ© dans le document \d+,?\s*',
            r'tel que dÃ©crit dans le document \d+,?\s*',
            r'according to document \d+,?\s*',
            r'as stated in document \d+,?\s*',
            r'document \d+ indicates that\s*',
            r'as mentioned in document \d+,?\s*',
            r'segÃºn el documento \d+,?\s*',
            r'como se indica en el documento \d+,?\s*',
            r'el documento \d+ menciona que\s*',
            r'\(document \d+\)',
            r'\[document \d+\]',
            r'source:\s*document \d+',
            r'ref:\s*document \d+'
        ]
        
        cleaned_response = response_text
        
        for pattern in patterns_to_remove:
            cleaned_response = re.sub(
                pattern, 
                '', 
                cleaned_response, 
                flags=re.IGNORECASE
            )
        
        cleaned_response = re.sub(r'\s+', ' ', cleaned_response)
        cleaned_response = cleaned_response.strip()
        
        if cleaned_response and cleaned_response[0].islower():
            cleaned_response = cleaned_response[0].upper() + cleaned_response[1:]
        
        return cleaned_response
    
    def _extract_weight_sentence(self, text: str) -> Optional[str]:
        """Extrait la phrase principale contenant l'information de poids"""
        
        weight_patterns = [
            r'[^.]*\d+[^.]*(?:grammes?|g\b|kg|livres?|pounds?)[^.]*\.',
            r'[^.]*(?:entre|between|entre)\s+\d+[^.]*\d+[^.]*\.',
            r'[^.]*(?:poids|weight|peso)[^.]*\d+[^.]*\.'
        ]
        
        for pattern in weight_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                sentence = match.group(0).strip()
                sentence = re.sub(r'^\W+', '', sentence)
                if sentence and sentence[0].islower():
                    sentence = sentence[0].upper() + sentence[1:]
                return sentence
        
        sentences = text.split('.')
        if sentences:
            first_sentence = sentences[0].strip() + '.'
            return first_sentence
        
        return None

# =============================================================================
# ðŸš€ RESPONSE VERSIONS GENERATOR (CONSERVÃ‰ IDENTIQUE)
# =============================================================================

class ResponseVersionsGenerator:
    """GÃ©nÃ©rateur de toutes les versions de rÃ©ponse pour le frontend"""
    
    def __init__(self, existing_processor: ResponseConcisionProcessor):
        self.existing_processor = existing_processor
        self.concision_service = concision_service
        logger.info("ðŸš€ [ResponseVersions] GÃ©nÃ©rateur initialisÃ© avec systÃ¨me existant")
    
    async def generate_all_response_versions(
        self, 
        original_response: str, 
        question: str, 
        context: Dict[str, Any],
        requested_level: ConcisionLevel = ConcisionLevel.CONCISE
    ) -> Dict[str, Any]:
        """
        GÃ©nÃ¨re toutes les versions de rÃ©ponse en utilisant le systÃ¨me existant + nouveau
        """
        start_time = time.time()
        
        try:
            logger.info("ðŸš€ [ResponseVersions] GÃ©nÃ©ration toutes versions")
            logger.info(f"   - Question: {question[:50]}...")
            logger.info(f"   - Niveau demandÃ©: {requested_level}")
            logger.info(f"   - RÃ©ponse originale: {len(original_response)} caractÃ¨res")
            
            versions = {}
            
            versions["detailed"] = original_response
            
            standard_response = self.existing_processor.apply_concision(
                original_response, 
                question, 
                ConcisionLevel.STANDARD,
                context.get("language", "fr")
            )
            versions["standard"] = standard_response
            
            concise_response = self.existing_processor.apply_concision(
                original_response, 
                question, 
                ConcisionLevel.CONCISE,
                context.get("language", "fr")
            )
            versions["concise"] = concise_response
            
            ultra_concise_response = self.existing_processor.apply_concision(
                original_response, 
                question, 
                ConcisionLevel.ULTRA_CONCISE,
                context.get("language", "fr")
            )
            versions["ultra_concise"] = ultra_concise_response
            
            selected_response = versions.get(requested_level.value, versions["concise"])
            
            generation_time_ms = int((time.time() - start_time) * 1000)
            
            metrics = ConcisionMetrics(
                generation_time_ms=generation_time_ms,
                versions_generated=len(versions),
                cache_hit=False,
                fallback_used=False,
                compression_ratios={
                    level: len(content) / len(original_response) 
                    for level, content in versions.items()
                    if content and len(original_response) > 0
                },
                quality_scores={}
            )
            
            logger.info("âœ… [ResponseVersions] Versions gÃ©nÃ©rÃ©es avec systÃ¨me existant:")
            for level, content in versions.items():
                logger.info(f"   - {level}: {len(content)} caractÃ¨res")
            
            return {
                "response_versions": versions,
                "selected_response": selected_response,
                "concision_metrics": metrics
            }
            
        except Exception as e:
            logger.error(f"âŒ [ResponseVersions] Erreur gÃ©nÃ©ration: {e}")
            
            fallback_versions = {
                "ultra_concise": original_response[:50] + "..." if len(original_response) > 50 else original_response,
                "concise": original_response,
                "standard": original_response,
                "detailed": original_response
            }
            
            return {
                "response_versions": fallback_versions,
                "selected_response": fallback_versions.get(requested_level.value, original_response),
                "concision_metrics": ConcisionMetrics(
                    generation_time_ms=int((time.time() - start_time) * 1000),
                    versions_generated=len(fallback_versions),
                    cache_hit=False,
                    fallback_used=True,
                    compression_ratios={},
                    quality_scores={}
                )
            }

# =============================================================================
# ðŸ”„ RAG CONTEXT ENHANCER (CONSERVÃ‰ IDENTIQUE)
# =============================================================================

class RAGContextEnhancer:
    """AmÃ©liore le contexte conversationnel pour optimiser les requÃªtes RAG"""
    
    def __init__(self):
        self.pronoun_patterns = {
            "fr": [
                r'\b(son|sa|ses|leur|leurs)\s+(poids|Ã¢ge|croissance|dÃ©veloppement)',
                r'\b(ils|elles)\s+(pÃ¨sent|grandissent|se dÃ©veloppent)',
                r'\b(qu\'?est-ce que|quel est)\s+(son|sa|ses|leur)',
                r'\b(combien)\s+(pÃ¨sent-ils|font-ils|mesurent-ils)'
            ],
            "en": [
                r'\b(their|its)\s+(weight|age|growth|development)',
                r'\b(they)\s+(weigh|grow|develop)',
                r'\b(what is|how much is)\s+(their|its)',
                r'\b(how much do they)\s+(weigh|measure)'
            ],
            "es": [
                r'\b(su|sus)\s+(peso|edad|crecimiento|desarrollo)',
                r'\b(ellos|ellas)\s+(pesan|crecen|se desarrollan)',
                r'\b(cuÃ¡l es|cuÃ¡nto es)\s+(su|sus)',
                r'\b(cuÃ¡nto)\s+(pesan|miden)'
            ]
        }
    
    def enhance_question_for_rag(
        self, 
        question: str, 
        conversation_context: str, 
        language: str = "fr"
    ) -> Tuple[str, Dict[str, any]]:
        """AmÃ©liore une question pour le RAG en utilisant le contexte conversationnel"""
        
        enhancement_info = {
            "pronoun_detected": False,
            "context_entities_used": [],
            "question_enriched": False,
            "original_question": question
        }
        
        has_pronouns = self._detect_contextual_references(question, language)
        if has_pronouns:
            enhancement_info["pronoun_detected"] = True
            logger.info(f"ðŸ” [RAG Context] Pronoms dÃ©tectÃ©s dans: '{question}'")
        
        context_entities = self._extract_context_entities(conversation_context)
        if context_entities:
            enhancement_info["context_entities_used"] = list(context_entities.keys())
            logger.info(f"ðŸ“Š [RAG Context] EntitÃ©s contextuelles: {context_entities}")
        
        enriched_question = question
        
        if has_pronouns and context_entities:
            enriched_question = self._build_enriched_question(
                question, context_entities, language
            )
            enhancement_info["question_enriched"] = True
            logger.info(f"âœ¨ [RAG Context] Question enrichie: '{enriched_question}'")
        
        if context_entities or has_pronouns:
            technical_context = self._build_technical_context(context_entities, language)
            if technical_context:
                enriched_question += f"\n\nContexte technique: {technical_context}"
        
        return enriched_question, enhancement_info
    
    def _detect_contextual_references(self, question: str, language: str) -> bool:
        """DÃ©tecte si la question contient des pronoms/rÃ©fÃ©rences contextuelles"""
        
        patterns = self.pronoun_patterns.get(language, self.pronoun_patterns["fr"])
        question_lower = question.lower()
        
        for pattern in patterns:
            if re.search(pattern, question_lower, re.IGNORECASE):
                logger.debug(f"ðŸŽ¯ [RAG Context] Pattern trouvÃ©: {pattern}")
                return True
        
        return False
    
    def _extract_context_entities(self, context: str) -> Dict[str, str]:
        """Extrait les entitÃ©s importantes du contexte conversationnel"""
        
        if not context:
            return {}
        
        entities = {}
        context_lower = context.lower()
        
        breed_patterns = [
            r'race[:\s]+([a-zA-Z0-9\s]+?)(?:\n|,|\.|\s|$)',
            r'breed[:\s]+([a-zA-Z0-9\s]+?)(?:\n|,|\.|\s|$)',
            r'(ross\s*308|cobb\s*500|hubbard|arbor\s*acres)',
            r'poulets?\s+(ross\s*308|cobb\s*500)',
            r'chickens?\s+(ross\s*308|cobb\s*500)'
        ]
        
        for pattern in breed_patterns:
            match = re.search(pattern, context_lower, re.IGNORECASE)
            if match:
                entities["breed"] = match.group(1).strip()
                break
        
        sex_patterns = [
            r'sexe[:\s]+([a-zA-Z\s]+?)(?:\n|,|\.|\s|$)',
            r'sex[:\s]+([a-zA-Z\s]+?)(?:\n|,|\.|\s|$)',
            r'\b(mÃ¢les?|femelles?|males?|females?|mixte|mixed)\b'
        ]
        
        for pattern in sex_patterns:
            match = re.search(pattern, context_lower, re.IGNORECASE)
            if match:
                entities["sex"] = match.group(1).strip()
                break
        
        age_patterns = [
            r'Ã¢ge[:\s]+(\d+\s*(?:jour|semaine|day|week)s?)',
            r'age[:\s]+(\d+\s*(?:jour|semaine|day|week)s?)',
            r'(\d+)\s*(?:jour|day)s?',
            r'(\d+)\s*(?:semaine|week)s?'
        ]
        
        for pattern in age_patterns:
            match = re.search(pattern, context_lower, re.IGNORECASE)
            if match:
                entities["age"] = match.group(1).strip()
                break
        
        return entities
    
    def _build_enriched_question(
        self, 
        question: str, 
        context_entities: Dict[str, str], 
        language: str
    ) -> str:
        """Construit une question enrichie en remplaÃ§ant les pronoms par les entitÃ©s contextuelles"""
        
        enriched = question
        
        templates = {
            "fr": {
                "breed_sex_age": "Pour des {breed} {sex} de {age}",
                "breed_age": "Pour des {breed} de {age}",
                "breed_sex": "Pour des {breed} {sex}",
                "breed_only": "Pour des {breed}",
                "age_only": "Pour des poulets de {age}"
            },
            "en": {
                "breed_sex_age": "For {breed} {sex} chickens at {age}",
                "breed_age": "For {breed} chickens at {age}",
                "breed_sex": "For {breed} {sex} chickens",
                "breed_only": "For {breed} chickens", 
                "age_only": "For chickens at {age}"
            },
            "es": {
                "breed_sex_age": "Para pollos {breed} {sex} de {age}",
                "breed_age": "Para pollos {breed} de {age}",
                "breed_sex": "Para pollos {breed} {sex}",
                "breed_only": "Para pollos {breed}",
                "age_only": "Para pollos de {age}"
            }
        }
        
        template_set = templates.get(language, templates["fr"])
        
        context_prefix = ""
        if "breed" in context_entities and "sex" in context_entities and "age" in context_entities:
            context_prefix = template_set["breed_sex_age"].format(
                breed=context_entities["breed"],
                sex=context_entities["sex"],
                age=context_entities["age"]
            )
        elif "breed" in context_entities and "age" in context_entities:
            context_prefix = template_set["breed_age"].format(
                breed=context_entities["breed"],
                age=context_entities["age"]
            )
        elif "breed" in context_entities and "sex" in context_entities:
            context_prefix = template_set["breed_sex"].format(
                breed=context_entities["breed"],
                sex=context_entities["sex"]
            )
        elif "breed" in context_entities:
            context_prefix = template_set["breed_only"].format(
                breed=context_entities["breed"]
            )
        elif "age" in context_entities:
            context_prefix = template_set["age_only"].format(age=context_entities["age"])
        
        if context_prefix:
            if any(word in question.lower() for word in ["son", "sa", "ses", "leur", "leurs", "their", "its", "su", "sus"]):
                enriched = f"{context_prefix}, {question.lower()}"
            else:
                enriched = f"{context_prefix}: {question}"
        
        return enriched
    
    def _build_technical_context(self, entities: Dict[str, str], language: str) -> str:
        """Construit un contexte technique pour aider le RAG"""
        
        if not entities:
            return ""
        
        context_parts = []
        
        if "breed" in entities:
            context_parts.append(f"Race: {entities['breed']}")
        
        if "sex" in entities:
            context_parts.append(f"Sexe: {entities['sex']}")
        
        if "age" in entities:
            context_parts.append(f"Ã‚ge: {entities['age']}")
        
        return " | ".join(context_parts)

# =============================================================================
# ðŸ”§ NOUVEAU: SYSTÃˆME AUTO-CLARIFICATION INTÃ‰GRÃ‰
# =============================================================================

class AutoClarificationSystem:
    """
    ðŸ”§ NOUVEAU: SystÃ¨me d'auto-clarification basÃ© sur le score de contexte
    """
    
    def __init__(self):
        self.context_threshold = 0.7  # Seuil pour dÃ©clencher clarification
        self.enable_auto_clarification = True
        
        logger.info("ðŸ”§ [Auto Clarification] SystÃ¨me initialisÃ©")
        logger.info(f"ðŸ”§ [Auto Clarification] Seuil contexte: {self.context_threshold}")
    
    def auto_clarify_if_needed(
        self, 
        question: str, 
        context_score: float, 
        language: str = "fr"
    ) -> Optional[Dict[str, Any]]:
        """
        ðŸ”§ NOUVEAU: DÃ©termine si une clarification automatique est nÃ©cessaire
        
        Args:
            question: Question de l'utilisateur
            context_score: Score de complÃ©tude du contexte (0.0 Ã  1.0)
            language: Langue de la question
            
        Returns:
            Dict avec clarification si nÃ©cessaire, None sinon
        """
        
        if not self.enable_auto_clarification:
            return None
        
        if context_score >= self.context_threshold:
            logger.info(f"âœ… [Auto Clarification] Contexte suffisant ({context_score:.2f} >= {self.context_threshold})")
            return None
        
        logger.info(f"ðŸ¤” [Auto Clarification] Contexte insuffisant ({context_score:.2f} < {self.context_threshold})")
        
        # Analyser le type de question pour gÃ©nÃ©rer clarifications appropriÃ©es
        question_analysis = self._analyze_question_for_clarification(question, language)
        
        if question_analysis["needs_clarification"]:
            logger.info(f"ðŸŽ¯ [Auto Clarification] DÃ©clenchement automatique - Type: {question_analysis['type']}")
            
            return {
                "type": "auto_clarification_needed",
                "message": self._build_clarification_message(question_analysis, language),
                "questions": question_analysis["questions"],
                "context_score": context_score,
                "trigger_reason": f"context_score_below_threshold_{context_score:.2f}",
                "automatic_trigger": True
            }
        
        return None
    
    def _analyze_question_for_clarification(self, question: str, language: str) -> Dict[str, Any]:
        """Analyse une question pour dÃ©terminer le type de clarification nÃ©cessaire"""
        
        question_lower = question.lower()
        
        # DÃ©tection questions de poids/performance
        if any(word in question_lower for word in ["poids", "weight", "peso", "performance", "croissance", "growth"]):
            return {
                "needs_clarification": True,
                "type": "performance_question",
                "questions": self._get_performance_clarification_questions(language),
                "priority": "high"
            }
        
        # DÃ©tection questions de santÃ©
        if any(word in question_lower for word in ["maladie", "disease", "mort", "death", "problÃ¨me", "problem"]):
            return {
                "needs_clarification": True,
                "type": "health_question", 
                "questions": self._get_health_clarification_questions(language),
                "priority": "high"
            }
        
        # DÃ©tection questions d'environnement
        if any(word in question_lower for word in ["tempÃ©rature", "temperature", "environnement", "environment"]):
            return {
                "needs_clarification": True,
                "type": "environment_question",
                "questions": self._get_environment_clarification_questions(language),
                "priority": "medium"
            }
        
        # Question gÃ©nÃ©rale - clarification basique
        return {
            "needs_clarification": True,
            "type": "general_question",
            "questions": self._get_general_clarification_questions(language),
            "priority": "medium"
        }
    
    def _get_performance_clarification_questions(self, language: str) -> List[str]:
        """Questions de clarification pour les questions de performance"""
        
        questions = {
            "fr": [
                "Quelle race ou souche spÃ©cifique Ã©levez-vous (Ross 308, Cobb 500, etc.) ?",
                "Quel Ã¢ge ont actuellement vos volailles (en jours prÃ©cis) ?",
                "S'agit-il de mÃ¢les, femelles, ou d'un troupeau mixte ?"
            ],
            "en": [
                "What specific breed or strain are you raising (Ross 308, Cobb 500, etc.)?",
                "What is the current age of your poultry (in precise days)?",
                "Are these males, females, or a mixed flock?"
            ],
            "es": [
                "Â¿QuÃ© raza o cepa especÃ­fica estÃ¡ criando (Ross 308, Cobb 500, etc.)?",
                "Â¿CuÃ¡l es la edad actual de sus aves (en dÃ­as precisos)?",
                "Â¿Son machos, hembras, o un lote mixto?"
            ]
        }
        
        return questions.get(language, questions["fr"])
    
    def _get_health_clarification_questions(self, language: str) -> List[str]:
        """Questions de clarification pour les questions de santÃ©"""
        
        questions = {
            "fr": [
                "Quelle race ou souche Ã©levez-vous ?",
                "Quel Ã¢ge ont vos volailles ?",
                "Quels symptÃ´mes spÃ©cifiques observez-vous ?",
                "Depuis combien de temps observez-vous ce problÃ¨me ?"
            ],
            "en": [
                "What breed or strain are you raising?",
                "What age are your poultry?",
                "What specific symptoms are you observing?",
                "How long have you been observing this problem?"
            ],
            "es": [
                "Â¿QuÃ© raza o cepa estÃ¡ criando?",
                "Â¿QuÃ© edad tienen sus aves?",
                "Â¿QuÃ© sÃ­ntomas especÃ­ficos estÃ¡ observando?",
                "Â¿Desde cuÃ¡ndo observa este problema?"
            ]
        }
        
        return questions.get(language, questions["fr"])
    
    def _get_environment_clarification_questions(self, language: str) -> List[str]:
        """Questions de clarification pour les questions d'environnement"""
        
        questions = {
            "fr": [
                "Quelle race ou souche Ã©levez-vous ?",
                "Quel Ã¢ge ont vos volailles ?",
                "Quelles sont les conditions actuelles (tempÃ©rature, humiditÃ©) ?",
                "Quel type de bÃ¢timent utilisez-vous ?"
            ],
            "en": [
                "What breed or strain are you raising?",
                "What age are your poultry?",
                "What are the current conditions (temperature, humidity)?",
                "What type of housing are you using?"
            ],
            "es": [
                "Â¿QuÃ© raza o cepa estÃ¡ criando?",
                "Â¿QuÃ© edad tienen sus aves?",
                "Â¿CuÃ¡les son las condiciones actuales (temperatura, humedad)?",
                "Â¿QuÃ© tipo de alojamiento estÃ¡ usando?"
            ]
        }
        
        return questions.get(language, questions["fr"])
    
    def _get_general_clarification_questions(self, language: str) -> List[str]:
        """Questions de clarification gÃ©nÃ©rales"""
        
        questions = {
            "fr": [
                "Pouvez-vous prÃ©ciser la race ou souche de vos volailles ?",
                "Quel Ã¢ge ont actuellement vos animaux ?",
                "Dans quel contexte d'Ã©levage vous trouvez-vous ?",
                "Y a-t-il des symptÃ´mes ou problÃ¨mes spÃ©cifiques observÃ©s ?"
            ],
            "en": [
                "Could you specify the breed or strain of your poultry?",
                "What age are your animals currently?",
                "What farming context are you in?",
                "Are there any specific symptoms or problems observed?"
            ],
            "es": [
                "Â¿PodrÃ­a especificar la raza o cepa de sus aves?",
                "Â¿QuÃ© edad tienen actualmente sus animales?",
                "Â¿En quÃ© contexto de crÃ­a se encuentra?",
                "Â¿Hay algÃºn sÃ­ntoma o problema especÃ­fico observado?"
            ]
        }
        
        return questions.get(language, questions["fr"])
    
    def _build_clarification_message(self, question_analysis: Dict[str, Any], language: str) -> str:
        """Construit le message de clarification selon le type de question"""
        
        messages = {
            "fr": {
                "performance_question": "ðŸ¤” Votre question concerne la performance. Pour vous donner une rÃ©ponse prÃ©cise, j'ai besoin de quelques dÃ©tails :",
                "health_question": "ðŸ¤” Votre question concerne la santÃ©. Pour mieux vous aider, pouvez-vous prÃ©ciser :",
                "environment_question": "ðŸ¤” Votre question concerne l'environnement. Pour une rÃ©ponse adaptÃ©e, j'aurais besoin de :",
                "general_question": "ðŸ¤” Pour mieux comprendre votre situation et vous aider efficacement :"
            },
            "en": {
                "performance_question": "ðŸ¤” Your question is about performance. To give you a precise answer, I need some details:",
                "health_question": "ðŸ¤” Your question is about health. To better help you, could you specify:",
                "environment_question": "ðŸ¤” Your question is about environment. For a tailored answer, I would need:",
                "general_question": "ðŸ¤” To better understand your situation and help you effectively:"
            },
            "es": {
                "performance_question": "ðŸ¤” Su pregunta es sobre rendimiento. Para darle una respuesta precisa, necesito algunos detalles:",
                "health_question": "ðŸ¤” Su pregunta es sobre salud. Para ayudarle mejor, Â¿podrÃ­a especificar:",
                "environment_question": "ðŸ¤” Su pregunta es sobre ambiente. Para una respuesta adaptada, necesitarÃ­a:",
                "general_question": "ðŸ¤” Para entender mejor su situaciÃ³n y ayudarle efectivamente:"
            }
        }
        
        question_type = question_analysis["type"]
        lang_messages = messages.get(language, messages["fr"])
        
        return lang_messages.get(question_type, lang_messages["general_question"])

# =============================================================================
# ðŸ”„ EXPERT SERVICE PRINCIPAL AVEC TOUTES LES INTÃ‰GRATIONS
# =============================================================================

class ExpertService:
    """Service principal pour le systÃ¨me expert avec toutes les amÃ©liorations intÃ©grÃ©es"""
    
    def __init__(self):
        self.integrations = IntegrationsManager()
        self.rag_enhancer = RAGContextEnhancer()
        self.enhancement_service = APIEnhancementService()
        
        self.concision_processor = ResponseConcisionProcessor()
        
        self.response_versions_generator = ResponseVersionsGenerator(
            existing_processor=self.concision_processor
        )
        
        # ðŸ”§ NOUVEAU: SystÃ¨me d'auto-clarification
        self.auto_clarification = AutoClarificationSystem()
        
        logger.info("âœ… [Expert Service] Service expert initialisÃ© avec TOUTES les amÃ©liorations")
        logger.info("   - âœ… SystÃ¨me de concision des rÃ©ponses")
        logger.info("   - ðŸš€ GÃ©nÃ©rateur de versions de rÃ©ponse")
        logger.info("   - ðŸ·ï¸ Filtrage taxonomique")
        logger.info("   - ðŸ†• Mode sÃ©mantique dynamique")
        logger.info("   - ðŸ”§ Auto-clarification intÃ©grÃ©e")
    
    def get_current_user_dependency(self):
        """Retourne la dÃ©pendance pour l'authentification"""
        return self.integrations.get_current_user_dependency()
    
    async def process_expert_question(
        self,
        request_data: EnhancedQuestionRequest,
        request: Request,
        current_user: Optional[Dict[str, Any]] = None,
        start_time: float = None
    ) -> EnhancedExpertResponse:
        """
        ðŸš€ MÃ‰THODE PRINCIPALE COMPLÃˆTEMENT RÃ‰Ã‰CRITE avec auto-clarification intÃ©grÃ©e
        âœ… CONSERVE toute la logique existante + ajoute auto-clarification
        """
        
        if start_time is None:
            start_time = time.time()
        
        try:
            logger.info("ðŸš€ [ExpertService] Traitement question avec auto-clarification intÃ©grÃ©e")
            
            concision_level = getattr(request_data, 'concision_level', ConcisionLevel.CONCISE)
            generate_all_versions = getattr(request_data, 'generate_all_versions', True)
            semantic_dynamic_mode = getattr(request_data, 'semantic_dynamic_mode', False)
            
            logger.info(f"ðŸš€ [ResponseVersions] ParamÃ¨tres: level={concision_level}, generate_all={generate_all_versions}")
            logger.info(f"ðŸ†• [Semantic Dynamic] Mode: {semantic_dynamic_mode}")
            
            base_response = await self._process_question_with_auto_clarification(
                request_data, request, current_user, start_time, semantic_dynamic_mode
            )
            
            if generate_all_versions and base_response.response:
                try:
                    logger.info("ðŸš€ [ResponseVersions] GÃ©nÃ©ration de toutes les versions")
                    
                    versions_result = await self.response_versions_generator.generate_all_response_versions(
                        original_response=base_response.response,
                        question=request_data.text,
                        context={
                            "language": request_data.language,
                            "user_id": current_user.get("id") if current_user else None,
                            "conversation_id": request_data.conversation_id
                        },
                        requested_level=concision_level
                    )
                    
                    base_response.response_versions = versions_result["response_versions"]
                    base_response.response = versions_result["selected_response"]
                    base_response.concision_metrics = versions_result["concision_metrics"]
                    
                    logger.info("âœ… [ResponseVersions] Versions ajoutÃ©es Ã  la rÃ©ponse")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ [ResponseVersions] Erreur gÃ©nÃ©ration versions: {e}")
                    base_response.response_versions = None
            else:
                logger.info("ðŸš€ [ResponseVersions] GÃ©nÃ©ration versions dÃ©sactivÃ©e")
                base_response.response_versions = None
            
            return base_response
            
        except Exception as e:
            logger.error(f"âŒ [ExpertService] Erreur traitement avec auto-clarification: {e}")
            raise
    
    async def _process_question_with_auto_clarification(
        self,
        request_data: EnhancedQuestionRequest,
        request: Request,
        current_user: Optional[Dict[str, Any]] = None,
        start_time: float = None,
        semantic_dynamic_mode: bool = False
    ) -> EnhancedExpertResponse:
        """
        ðŸ”§ NOUVELLE MÃ‰THODE: Traitement avec auto-clarification intÃ©grÃ©e
        """
        
        processing_steps = []
        ai_enhancements_used = []
        debug_info = {}
        performance_breakdown = {"start": int(time.time() * 1000)}
        
        processing_steps.append("initialization")
        
        # === AUTHENTIFICATION ===
        if current_user is None and self.integrations.auth_available:
            raise HTTPException(status_code=401, detail="Authentification requise")
        
        user_id = self._extract_user_id(current_user, request_data, request)
        user_email = current_user.get("email") if current_user else None
        request_ip = request.client.host if request.client else "unknown"
        
        processing_steps.append("authentication")
        performance_breakdown["auth_complete"] = int(time.time() * 1000)
        
        # === GESTION CONVERSATION ID ===
        conversation_id = self._get_or_create_conversation_id(request_data)
        
        # === VALIDATION QUESTION ===
        question_text = request_data.text.strip()
        if not question_text:
            raise HTTPException(status_code=400, detail="Question text is required")
        
        processing_steps.append("question_validation")
        
        # === MÃ‰MOIRE CONVERSATIONNELLE + ðŸ”§ Ã‰VALUATION CONTEXTE ===
        conversation_context = None
        context_score = 0.0  # ðŸ”§ NOUVEAU: Score de complÃ©tude du contexte
        
        if self.integrations.intelligent_memory_available:
            try:
                conversation_context = self.integrations.add_message_to_conversation(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    message=question_text,
                    role="user",
                    language=request_data.language,
                    message_type="clarification_response" if request_data.is_clarification_response else "question"
                )
                
                # ðŸ”§ NOUVEAU: Calculer score de contexte
                context_score = self._calculate_context_completeness_score(
                    question_text, conversation_context, request_data.language
                )
                
                ai_enhancements_used.append("intelligent_memory")
                processing_steps.append("memory_storage")
                logger.info(f"ðŸ’¾ [Expert Service] Message ajoutÃ© Ã  la mÃ©moire: {question_text[:50]}...")
                logger.info(f"ðŸ“Š [Context Score] Score complÃ©tude contexte: {context_score:.2f}")
                
            except Exception as e:
                logger.warning(f"âš ï¸ [Expert Service] Erreur mÃ©moire: {e}")
        
        performance_breakdown["memory_complete"] = int(time.time() * 1000)
        
        # ðŸ”§ NOUVEAU: VÃ©rification auto-clarification AVANT validation agricole
        if not request_data.is_clarification_response:
            auto_clarification_result = self.auto_clarification.auto_clarify_if_needed(
                question_text, context_score, request_data.language
            )
            
            if auto_clarification_result:
                logger.info(f"ðŸ¤” [Auto Clarification] DÃ©clenchement automatique: {auto_clarification_result['trigger_reason']}")
                
                processing_steps.append("automatic_clarification_triggered")
                ai_enhancements_used.append("auto_clarification_context_based")
                
                return self._create_auto_clarification_response(
                    question_text, auto_clarification_result, request_data.language, 
                    conversation_id, context_score, start_time, processing_steps, ai_enhancements_used
                )
        
        # === VALIDATION AGRICOLE ===
        validation_result = await self._validate_agricultural_question(
            question_text, request_data.language, user_id, request_ip, conversation_id
        )
        
        processing_steps.append("agricultural_validation")
        performance_breakdown["validation_complete"] = int(time.time() * 1000)
        
        if not validation_result.is_valid:
            return self._create_rejection_response(
                question_text, validation_result, conversation_id, 
                user_email, request_data.language, start_time,
                processing_steps, ai_enhancements_used, None
            )
        
        # === SYSTÃˆME DE CLARIFICATION INTELLIGENT + SÃ‰MANTIQUE DYNAMIQUE ===
        clarification_result = await self._handle_clarification_corrected_with_semantic_dynamic(
            request_data, question_text, user_id, conversation_id,
            processing_steps, ai_enhancements_used, semantic_dynamic_mode
        )
        
        if clarification_result:
            return clarification_result
        
        # DÃ©tection vagueness aprÃ¨s clarifications spÃ©cialisÃ©es
        vagueness_result = None
        if request_data.enable_vagueness_detection:
            vagueness_result = self.enhancement_service.detect_vagueness(
                question_text, request_data.language
            )
            
            ai_enhancements_used.append("vagueness_detection")
            performance_breakdown["vagueness_check"] = int(time.time() * 1000)
            
            if vagueness_result.is_vague and vagueness_result.vagueness_score > 0.6:
                logger.info(f"ðŸŽ¯ [Expert Service] Question floue dÃ©tectÃ©e (score: {vagueness_result.vagueness_score})")
                return self._create_vagueness_response(
                    vagueness_result, question_text, conversation_id, 
                    request_data.language, start_time, processing_steps, ai_enhancements_used
                )
        
        performance_breakdown["clarification_complete"] = int(time.time() * 1000)
        
        # === TRAITEMENT EXPERT AVEC RAG-FIRST + AMÃ‰LIORATIONS + TAXONOMIC FILTERING ===
        expert_result = await self._process_expert_response_enhanced_corrected_with_taxonomy(
            question_text, request_data, request, current_user,
            conversation_id, processing_steps, ai_enhancements_used,
            debug_info, performance_breakdown, vagueness_result
        )
        
        # âœ… CONSERVÃ‰: APPLICATION DU SYSTÃˆME DE CONCISION EXISTANT
        if expert_result["answer"] and self.concision_processor.config.ENABLE_CONCISE_RESPONSES:
            
            user_concision_preference = getattr(request_data, 'concision_level', None)
            
            original_answer = expert_result["answer"]
            processed_answer = self.concision_processor.process_response(
                response=original_answer,
                question=question_text,
                concision_level=user_concision_preference,
                language=request_data.language
            )
            
            if processed_answer != original_answer:
                expert_result["answer"] = processed_answer
                expert_result["original_answer"] = original_answer
                expert_result["concision_applied"] = True
                ai_enhancements_used.append("response_concision")
                processing_steps.append("concision_processing")
                
                logger.info(f"âœ‚ï¸ [Expert Service] Concision appliquÃ©e: {len(original_answer)} â†’ {len(processed_answer)} chars")
            else:
                expert_result["concision_applied"] = False
        
        performance_breakdown["concision_complete"] = int(time.time() * 1000)
        
        # === ENREGISTREMENT RÃ‰PONSE ===
        if self.integrations.intelligent_memory_available and expert_result["answer"]:
            try:
                self.integrations.add_message_to_conversation(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    message=expert_result["answer"],
                    role="assistant",
                    language=request_data.language,
                    message_type="response"
                )
            except Exception as e:
                logger.warning(f"âš ï¸ [Expert Service] Erreur enregistrement rÃ©ponse: {e}")
        
        processing_steps.append("response_storage")
        performance_breakdown["final"] = int(time.time() * 1000)
        
        # === CONSTRUCTION RÃ‰PONSE FINALE AMÃ‰LIORÃ‰E ===
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return self._build_final_enhanced_response(
            question_text, expert_result["answer"], conversation_id,
            user_email, request_data.language, response_time_ms,
            expert_result, validation_result, conversation_context,
            processing_steps, ai_enhancements_used, request_data,
            debug_info, performance_breakdown
        )
    
    # ===========================================================================================
    # ðŸ”§ NOUVELLES MÃ‰THODES POUR AUTO-CLARIFICATION
    # ===========================================================================================
    
    def _calculate_context_completeness_score(
        self, 
        question: str, 
        conversation_context, 
        language: str = "fr"
    ) -> float:
        """
        ðŸ”§ NOUVEAU: Calcule un score de complÃ©tude du contexte (0.0 Ã  1.0)
        """
        
        score = 0.0
        
        # Score de base selon la longueur et dÃ©tail de la question
        question_length = len(question.strip())
        if question_length > 50:
            score += 0.2
        elif question_length > 25:
            score += 0.1
        
        # DÃ©tection d'informations spÃ©cifiques dans la question
        question_lower = question.lower()
        
        # PrÃ©sence de race spÃ©cifique (+0.3)
        specific_breeds = ["ross 308", "cobb 500", "hubbard", "arbor acres", "isa"]
        if any(breed in question_lower for breed in specific_breeds):
            score += 0.3
        elif any(word in question_lower for word in ["poulet", "chicken", "pollo"]):
            score += 0.1  # Race gÃ©nÃ©rique
        
        # PrÃ©sence d'Ã¢ge (+0.2)
        age_patterns = [r'\d+\s*(?:jour|day|dÃ­a)s?', r'\d+\s*(?:semaine|week|semana)s?']
        if any(re.search(pattern, question_lower) for pattern in age_patterns):
            score += 0.2
        
        # PrÃ©sence de donnÃ©es numÃ©riques (+0.1)
        if re.search(r'\d+', question):
            score += 0.1
        
        # Contexte conversationnel disponible (+0.2)
        if conversation_context and hasattr(conversation_context, 'consolidated_entities'):
            entities = conversation_context.consolidated_entities.to_dict()
            if entities:
                score += 0.2
                
                # Bonus pour entitÃ©s spÃ©cifiques
                if entities.get('breed') and entities.get('breed') != 'generic':
                    score += 0.1
                if entities.get('age_days') or entities.get('age_weeks'):
                    score += 0.1
        
        # Limiter le score Ã  1.0
        return min(score, 1.0)
    
    def _create_auto_clarification_response(
        self,
        question: str,
        clarification_result: Dict[str, Any],
        language: str,
        conversation_id: str,
        context_score: float,
        start_time: float,
        processing_steps: list,
        ai_enhancements_used: list
    ) -> EnhancedExpertResponse:
        """
        ðŸ”§ NOUVEAU: CrÃ©e une rÃ©ponse d'auto-clarification
        """
        
        # Construire le message avec les questions
        message = clarification_result["message"]
        questions = clarification_result["questions"]
        
        if len(questions) == 1:
            formatted_questions = questions[0]
        else:
            formatted_questions = "\n".join([f"â€¢ {q}" for q in questions])
        
        outro_messages = {
            "fr": "\n\nCes prÃ©cisions m'aideront Ã  vous donner une rÃ©ponse plus prÃ©cise et utile ! ðŸ”",
            "en": "\n\nThese details will help me give you a more precise and useful answer! ðŸ”",
            "es": "\n\nÂ¡Estos detalles me ayudarÃ¡n a darle una respuesta mÃ¡s precisa y Ãºtil! ðŸ”"
        }
        
        outro = outro_messages.get(language, outro_messages["fr"])
        response_text = f"{message}\n\n{formatted_questions}{outro}"
        
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return EnhancedExpertResponse(
            question=question,
            response=response_text,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode="automatic_context_based_clarification",
            user=None,
            logged=True,
            validation_passed=True,
            clarification_result={
                "clarification_requested": True,
                "clarification_type": "automatic_context_based",
                "context_score": context_score,
                "questions_generated": len(questions),
                "trigger_reason": clarification_result["trigger_reason"],
                "automatic_trigger": True,
                "question_type": clarification_result.get("type", "unknown")
            },
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used,
            dynamic_clarification=DynamicClarification(
                original_question=question,
                clarification_questions=questions,
                confidence=0.9,
                generation_method="automatic_context_evaluation",
                generation_time_ms=response_time_ms,
                fallback_used=False
            )
        )
    
    # ===========================================================================================
    # âœ… TOUTES LES MÃ‰THODES EXISTANTES CONSERVÃ‰ES IDENTIQUES
    # ===========================================================================================
    
    async def _handle_clarification_corrected_with_semantic_dynamic(
        self, request_data, question_text, user_id, conversation_id, 
        processing_steps, ai_enhancements_used, semantic_dynamic_mode: bool = False
    ):
        """
        âœ… SYSTÃˆME DE CLARIFICATION PARFAITEMENT CORRIGÃ‰ + MODE SÃ‰MANTIQUE DYNAMIQUE
        ðŸ†• NOUVEAU: Support du mode sÃ©mantique dynamique
        """
        
        # 1. âœ… TRAITEMENT DES RÃ‰PONSES DE CLARIFICATION CORRIGÃ‰
        if request_data.is_clarification_response:
            return await self._process_clarification_response_corrected(
                request_data, question_text, conversation_id,
                processing_steps, ai_enhancements_used
            )
        
        # ðŸ†• NOUVEAU: VÃ©rifier si mode sÃ©mantique dynamique activÃ©
        if semantic_dynamic_mode and self.integrations.enhanced_clarification_available:
            logger.info(f"ðŸ†• [Semantic Dynamic] Mode activÃ© pour: '{question_text[:50]}...'")
            
            try:
                from .question_clarification_system import analyze_question_for_clarification_semantic_dynamic
                
                clarification_result = await analyze_question_for_clarification_semantic_dynamic(
                    question=question_text,
                    language=request_data.language,
                    user_id=user_id,
                    conversation_id=conversation_id,
                    conversation_context={}
                )
                
                if clarification_result.needs_clarification:
                    logger.info(f"ðŸ†• [Semantic Dynamic] {len(clarification_result.questions)} questions gÃ©nÃ©rÃ©es")
                    processing_steps.append("semantic_dynamic_clarification_triggered")
                    ai_enhancements_used.append("semantic_dynamic_clarification")
                    
                    return self._create_semantic_dynamic_clarification_response(
                        question_text, clarification_result, request_data.language, conversation_id
                    )
                else:
                    logger.info(f"âœ… [Semantic Dynamic] Question claire, pas de clarification nÃ©cessaire")
                
            except Exception as e:
                logger.error(f"âŒ [Semantic Dynamic] Erreur mode sÃ©mantique: {e}")
        
        # 2. DÃ‰TECTION QUESTIONS NÃ‰CESSITANT CLARIFICATION (mode normal)
        clarification_needed = self._detect_performance_question_needing_clarification(
            question_text, request_data.language
        )
        
        if not clarification_needed:
            return None
        
        logger.info(f"ðŸŽ¯ [Expert Service] Clarification nÃ©cessaire: {clarification_needed['type']}")
        processing_steps.append("automatic_clarification_triggered")
        ai_enhancements_used.append("smart_performance_clarification")
        
        # 3. âœ… SAUVEGARDE FORCÃ‰E AVEC MÃ‰MOIRE INTELLIGENTE
        if self.integrations.intelligent_memory_available:
            try:
                from .conversation_memory_enhanced import mark_question_for_clarification
                
                question_id = mark_question_for_clarification(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    original_question=question_text,
                    language=request_data.language
                )
                
                logger.info(f"ðŸ’¾ [Expert Service] Question originale marquÃ©e: {question_id}")
                processing_steps.append("original_question_marked")
                ai_enhancements_used.append("intelligent_memory_clarification_marking")
                
            except Exception as e:
                logger.error(f"âŒ [Expert Service] Erreur marquage question: {e}")
                self.integrations.add_message_to_conversation(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    message=f"ORIGINAL_QUESTION_FOR_CLARIFICATION: {question_text}",
                    role="system",
                    language=request_data.language,
                    message_type="original_question_marker"
                )
        
        # 4. GÃ©nÃ©rer la demande de clarification
        clarification_response = self._generate_performance_clarification_response(
            question_text, clarification_needed, request_data.language, conversation_id
        )
        
        return clarification_response
    
    def _create_semantic_dynamic_clarification_response(
        self, question: str, clarification_result, language: str, conversation_id: str
    ) -> EnhancedExpertResponse:
        """ðŸ†• NOUVEAU: CrÃ©e une rÃ©ponse de clarification sÃ©mantique dynamique"""
        
        # Formater les questions de clarification
        if clarification_result.questions:
            if len(clarification_result.questions) == 1:
                response_text = f"â“ Pour mieux comprendre votre situation et vous aider efficacement :\n\n{clarification_result.questions[0]}"
            else:
                formatted_questions = "\n".join([f"â€¢ {q}" for q in clarification_result.questions])
                response_text = f"â“ Pour mieux comprendre votre situation et vous aider efficacement :\n\n{formatted_questions}"
            
            response_text += "\n\nCela me permettra de vous donner les conseils les plus pertinents ! ðŸ”"
        else:
            response_text = "â“ Pouvez-vous prÃ©ciser votre question pour que je puisse mieux vous aider ?"
        
        return EnhancedExpertResponse(
            question=question,
            response=response_text,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=int(clarification_result.processing_time_ms),
            mode="semantic_dynamic_clarification",
            user=None,
            logged=True,
            validation_passed=True,
            clarification_result={
                "clarification_requested": True,
                "clarification_type": "semantic_dynamic",
                "questions_generated": len(clarification_result.questions) if clarification_result.questions else 0,
                "confidence": clarification_result.confidence_score,
                "model_used": clarification_result.model_used,
                "generation_time_ms": clarification_result.processing_time_ms,
                "validation_score": clarification_result.validation_score,
                "fallback_used": clarification_result.fallback_used,
                "gpt_failed": clarification_result.gpt_failed
            },
            processing_steps=["semantic_dynamic_clarification_triggered"],
            ai_enhancements_used=["semantic_dynamic_clarification", "gpt_question_generation"],
            dynamic_clarification=DynamicClarification(
                original_question=question,
                clarification_questions=clarification_result.questions or [],
                confidence=clarification_result.confidence_score
            )
        )
    
    async def _process_expert_response_enhanced_corrected_with_taxonomy(
        self, question_text: str, request_data: EnhancedQuestionRequest,
        request: Request, current_user: Optional[Dict], conversation_id: str,
        processing_steps: list, ai_enhancements_used: list,
        debug_info: Dict, performance_breakdown: Dict, vagueness_result = None
    ) -> Dict[str, Any]:
        """
        ðŸ·ï¸ NOUVELLE VERSION: RAG parfaitement corrigÃ© avec mÃ©moire intelligente + FILTRAGE TAXONOMIQUE
        """
        
        # === 1. RÃ‰CUPÃ‰RATION FORCÃ‰E DU CONTEXTE CONVERSATIONNEL ===
        conversation_context_str = ""
        extracted_entities = {}
        
        if self.integrations.intelligent_memory_available:
            try:
                context_obj = self.integrations.get_conversation_context(conversation_id)
                if context_obj:
                    conversation_context_str = context_obj.get_context_for_rag(max_chars=800)
                    
                    if request_data.is_clarification_response or request_data.original_question:
                        if request_data.original_question:
                            conversation_context_str = f"Question originale: {request_data.original_question}. " + conversation_context_str
                        
                        if hasattr(context_obj, 'messages'):
                            for msg in reversed(context_obj.messages[-5:]):
                                if msg.role == "user" and any(word in msg.message.lower() for word in ["ross", "cobb", "hubbard", "mÃ¢le", "femelle", "male", "female"]):
                                    conversation_context_str += f" | Clarification: {msg.message}"
                                    break
                    
                    if hasattr(context_obj, 'consolidated_entities'):
                        extracted_entities = context_obj.consolidated_entities.to_dict()
                    
                    logger.info(f"ðŸ§  [Expert Service] Contexte enrichi rÃ©cupÃ©rÃ©: {conversation_context_str[:150]}...")
                    ai_enhancements_used.append("intelligent_memory_context_retrieval")
                else:
                    logger.warning(f"âš ï¸ [Expert Service] Aucun contexte trouvÃ© pour: {conversation_id}")
                    
            except Exception as e:
                logger.error(f"âŒ [Expert Service] Erreur rÃ©cupÃ©ration contexte: {e}")
        
        performance_breakdown["context_retrieved"] = int(time.time() * 1000)
        
        # === 2. AMÃ‰LIORATION INTELLIGENTE DE LA QUESTION ===
        enriched_question, enhancement_info = self.rag_enhancer.enhance_question_for_rag(
            question=question_text,
            conversation_context=conversation_context_str,
            language=request_data.language
        )
        
        if request_data.original_question and request_data.is_clarification_response:
            logger.info(f"âœ¨ [Expert Service] Question dÃ©jÃ  enrichie par clarification: {question_text[:100]}...")
            ai_enhancements_used.append("clarification_based_enrichment")
        
        if enhancement_info["question_enriched"]:
            ai_enhancements_used.append("intelligent_question_enhancement")
            logger.info(f"âœ¨ [Expert Service] Question amÃ©liorÃ©e: {enriched_question[:150]}...")
        
        if enhancement_info["pronoun_detected"]:
            ai_enhancements_used.append("contextual_pronoun_resolution")
            logger.info(f"ðŸŽ¯ [Expert Service] Pronoms contextuels rÃ©solus: {enhancement_info['context_entities_used']}")
        
        processing_steps.append("intelligent_question_enhancement")
        performance_breakdown["question_enhanced"] = int(time.time() * 1000)
        
        # === 3. ðŸ·ï¸ NOUVEAU: FILTRAGE TAXONOMIQUE INTELLIGENT ===
        from .api_enhancement_service import infer_taxonomy_from_entities, enhance_rag_query_with_taxonomy
        
        taxonomy = infer_taxonomy_from_entities(extracted_entities)
        enhanced_question_with_taxonomy, rag_filters = enhance_rag_query_with_taxonomy(
            enriched_question, extracted_entities, request_data.language
        )
        
        logger.info(f"ðŸ·ï¸ [Taxonomy Filter] Taxonomie dÃ©tectÃ©e: {taxonomy}")
        if rag_filters:
            logger.info(f"ðŸ·ï¸ [Taxonomy Filter] Filtres RAG: {rag_filters}")
            ai_enhancements_used.append("taxonomic_document_filtering")
        
        processing_steps.append("taxonomic_analysis_and_filtering")
        performance_breakdown["taxonomy_analysis"] = int(time.time() * 1000)
        
        # === 4. VÃ‰RIFICATION RAG DISPONIBLE ===
        app = request.app
        process_rag = getattr(app.state, 'process_question_with_rag', None)
        
        if not process_rag:
            logger.error("âŒ [Expert Service] SystÃ¨me RAG indisponible - Erreur critique")
            raise HTTPException(
                status_code=503, 
                detail="Service RAG indisponible - Le systÃ¨me expert nÃ©cessite l'accÃ¨s Ã  la base documentaire"
            )
        
        # === 5. APPEL RAG AVEC CONTEXTE FORCÃ‰ + FILTRAGE TAXONOMIQUE ===
        try:
            logger.info("ðŸ” [Expert Service] Appel RAG avec contexte intelligent + taxonomie...")
            
            if request_data.debug_mode:
                debug_info["original_question"] = question_text
                debug_info["enriched_question"] = enriched_question
                debug_info["enriched_question_with_taxonomy"] = enhanced_question_with_taxonomy
                debug_info["conversation_context"] = conversation_context_str
                debug_info["enhancement_info"] = enhancement_info
                debug_info["taxonomy_detected"] = taxonomy
                debug_info["rag_filters"] = rag_filters
            
            result = None
            rag_call_method = "unknown"
            
            rag_context = extract_context_from_entities(extracted_entities)
            rag_context["lang"] = request_data.language
            rag_context["taxonomy"] = taxonomy
            
            # Tentative 1: Avec paramÃ¨tre context + filtres taxonomiques si supportÃ©
            try:
                structured_question = build_structured_prompt(
                    documents="[DOCUMENTS_WILL_BE_INSERTED_BY_RAG]",
                    question=enhanced_question_with_taxonomy,
                    context=rag_context
                )
                
                logger.debug(f"ðŸ” [Prompt Final RAG] Contexte: {rag_context}")
                logger.debug(f"ðŸ·ï¸ [Prompt Final RAG] Taxonomie: {taxonomy}")
                logger.debug(f"ðŸ” [Prompt Final RAG]\n{structured_question[:500]}...")
                
                rag_params = {
                    "question": structured_question,
                    "user": current_user,
                    "language": request_data.language,
                    "speed_mode": request_data.speed_mode,
                    "context": conversation_context_str
                }
                
                if rag_filters:
                    try:
                        rag_params["filters"] = rag_filters
                        result = await process_rag(**rag_params)
                        rag_call_method = "context_parameter_structured_with_taxonomy"
                        logger.info("âœ… [Expert Service] RAG appelÃ© avec prompt structurÃ© + contexte + filtres taxonomiques")
                    except TypeError:
                        del rag_params["filters"]
                        result = await process_rag(**rag_params)
                        rag_call_method = "context_parameter_structured_taxonomy_fallback"
                        logger.info("âœ… [Expert Service] RAG appelÃ© avec prompt structurÃ© + contexte (filtres taxonomiques non supportÃ©s)")
                else:
                    result = await process_rag(**rag_params)
                    rag_call_method = "context_parameter_structured"
                    logger.info("âœ… [Expert Service] RAG appelÃ© avec prompt structurÃ© + contexte")
                    
            except TypeError as te:
                logger.info(f"â„¹ï¸ [Expert Service] ParamÃ¨tre context non supportÃ©: {te}")
                
                if conversation_context_str:
                    structured_question = build_structured_prompt(
                        documents="[DOCUMENTS_WILL_BE_INSERTED_BY_RAG]",
                        question=enhanced_question_with_taxonomy,
                        context=rag_context
                    )
                    
                    logger.debug(f"ðŸ” [Prompt Final RAG - InjectÃ© + Taxonomie]\n{structured_question[:500]}...")
                    
                    contextual_question = f"{structured_question}\n\nContexte: {conversation_context_str}"
                    result = await process_rag(
                        question=contextual_question,
                        user=current_user,
                        language=request_data.language,
                        speed_mode=request_data.speed_mode
                    )
                    rag_call_method = "context_injected_structured_with_taxonomy"
                    logger.info("âœ… [Expert Service] RAG appelÃ© avec prompt structurÃ© + contexte injectÃ© + taxonomie")
                else:
                    structured_question = build_structured_prompt(
                        documents="[DOCUMENTS_WILL_BE_INSERTED_BY_RAG]",
                        question=enhanced_question_with_taxonomy,
                        context=rag_context
                    )
                    
                    logger.debug(f"ðŸ” [Prompt Final RAG - Seul + Taxonomie]\n{structured_question[:500]}...")
                    
                    result = await process_rag(
                        question=structured_question,
                        user=current_user,
                        language=request_data.language,
                        speed_mode=request_data.speed_mode
                    )
                    rag_call_method = "structured_with_taxonomy_only"
                    logger.info("âœ… [Expert Service] RAG appelÃ© avec prompt structurÃ© + taxonomie seul")
            
            performance_breakdown["rag_complete"] = int(time.time() * 1000)
            
            # === 6. TRAITEMENT RÃ‰SULTAT RAG ===
            answer = str(result.get("response", ""))
            
            answer = self.concision_processor._clean_document_references_only(answer)
            
            rag_score = result.get("score", 0.0)
            original_mode = result.get("mode", "rag_processing")
            
            quality_check = self._validate_rag_response_quality(
                answer, enhanced_question_with_taxonomy, enhancement_info
            )
            
            if not quality_check["valid"]:
                logger.warning(f"âš ï¸ [Expert Service] QualitÃ© RAG insuffisante: {quality_check['reason']}")
                ai_enhancements_used.append("quality_validation_failed")
            
            logger.info(f"âœ… [Expert Service] RAG rÃ©ponse reÃ§ue: {len(answer)} caractÃ¨res, score: {rag_score}")
            
            mode = f"enhanced_contextual_{original_mode}_{rag_call_method}_corrected_with_concision_and_response_versions_and_taxonomy_and_semantic_dynamic_and_auto_clarification"
            
            processing_steps.append("mandatory_rag_with_intelligent_context_and_taxonomy")
            
            return {
                "answer": answer,
                "rag_used": True,
                "rag_score": rag_score,
                "mode": mode,
                "context_used": bool(conversation_context_str),
                "question_enriched": enhancement_info["question_enriched"] or bool(request_data.original_question),
                "enhancement_info": enhancement_info,
                "quality_check": quality_check,
                "extracted_entities": extracted_entities,
                "rag_call_method": rag_call_method,
                "taxonomy_used": taxonomy,
                "taxonomy_filters_applied": bool(rag_filters)
            }
            
        except Exception as rag_error:
            logger.error(f"âŒ [Expert Service] Erreur critique RAG: {rag_error}")
            processing_steps.append("rag_error")
            
            error_details = {
                "error": "Erreur RAG",
                "message": "Impossible d'interroger la base documentaire",
                "question_original": question_text,
                "question_enriched": enriched_question,
                "question_with_taxonomy": enhanced_question_with_taxonomy,
                "context_available": bool(conversation_context_str),
                "taxonomy_detected": taxonomy,
                "technical_error": str(rag_error)
            }
            
            raise HTTPException(status_code=503, detail=error_details)
    
    # ===========================================================================================
    # âœ… TOUTES LES AUTRES MÃ‰THODES EXISTANTES CONSERVÃ‰ES IDENTIQUES
    # ===========================================================================================
    
    async def _process_clarification_response_corrected(
        self, request_data, question_text, conversation_id, 
        processing_steps, ai_enhancements_used
    ):
        """âœ… TRAITEMENT DES RÃ‰PONSES DE CLARIFICATION - VERSION CORRIGÃ‰E FINALE (CONSERVÃ‰)"""
        
        original_question = request_data.original_question
        clarification_context = request_data.clarification_context
        
        if (not original_question or not clarification_context) and self.integrations.intelligent_memory_available:
            try:
                from .conversation_memory_enhanced import find_original_question
                
                original_msg = find_original_question(conversation_id)
                
                if original_msg:
                    original_question = original_msg.message
                    clarification_context = {
                        "missing_information": ["breed", "sex"],
                        "clarification_type": "performance_breed_sex"
                    }
                    logger.info(f"âœ… [Expert Service] Question originale rÃ©cupÃ©rÃ©e: {original_question}")
                    ai_enhancements_used.append("intelligent_memory_original_question_recovery")
                else:
                    logger.warning("âš ï¸ [Expert Service] Question originale non trouvÃ©e dans la mÃ©moire")
                    
            except Exception as e:
                logger.error(f"âŒ [Expert Service] Erreur rÃ©cupÃ©ration question originale: {e}")
        
        if not original_question:
            logger.warning("âš ï¸ [Expert Service] Fallback: crÃ©ation question par dÃ©faut")
            original_question = "Quel est le poids de rÃ©fÃ©rence pour ces poulets ?"
            clarification_context = {
                "missing_information": ["breed", "sex"],
                "clarification_type": "performance_breed_sex_fallback"
            }
        
        missing_info = clarification_context.get("missing_information", [])
        
        validation = validate_clarification_completeness(
            question_text, missing_info, request_data.language
        )
        
        if not validation["is_complete"]:
            logger.info(f"ðŸ”„ [Expert Service] Clarification incomplÃ¨te: {validation['still_missing']}")
            return self._generate_follow_up_clarification(
                question_text, validation, request_data.language, conversation_id
            )
        
        breed = validation["extracted_info"].get("breed")
        sex = validation["extracted_info"].get("sex")
        
        age_info = self._extract_age_from_original_question(original_question, request_data.language)
        
        enriched_original_question = self._build_complete_enriched_question(
            original_question, breed, sex, age_info, request_data.language
        )
        
        logger.info(f"âœ… [Expert Service] Question COMPLÃˆTEMENT enrichie: {enriched_original_question}")
        
        request_data.text = enriched_original_question
        request_data.is_clarification_response = False
        request_data.original_question = original_question
        
        processing_steps.append("clarification_processed_successfully_with_age")
        ai_enhancements_used.append("breed_sex_age_extraction_complete")
        ai_enhancements_used.append("complete_question_enrichment")
        ai_enhancements_used.append("forced_question_replacement_with_age")
        
        return None
    
    def _detect_performance_question_needing_clarification(
        self, question: str, language: str = "fr"
    ) -> Optional[Dict[str, Any]]:
        """DÃ©tection amÃ©liorÃ©e des questions techniques nÃ©cessitant race/sexe (CONSERVÃ‰)"""
        
        question_lower = question.lower()
        
        weight_age_patterns = {
            "fr": [
                r'(?:poids|pÃ¨se)\s+.*?(\d+)\s*(?:jour|semaine)s?',
                r'(\d+)\s*(?:jour|semaine)s?.*?(?:poids|pÃ¨se)',
                r'(?:quel|combien)\s+.*?(?:poids|pÃ¨se).*?(\d+)',
                r'(?:croissance|dÃ©veloppement).*?(\d+)\s*(?:jour|semaine)',
                r'(\d+)\s*(?:jour|semaine).*?(?:normal|rÃ©fÃ©rence|standard)'
            ],
            "en": [
                r'(?:weight|weigh)\s+.*?(\d+)\s*(?:day|week)s?',
                r'(\d+)\s*(?:day|week)s?.*?(?:weight|weigh)',
                r'(?:what|how much)\s+.*?(?:weight|weigh).*?(\d+)',
                r'(?:growth|development).*?(\d+)\s*(?:day|week)',
                r'(\d+)\s*(?:day|week).*?(?:normal|reference|standard)'
            ],
            "es": [
                r'(?:peso|pesa)\s+.*?(\d+)\s*(?:dÃ­a|semana)s?',
                r'(\d+)\s*(?:dÃ­a|semana)s?.*?(?:peso|pesa)',
                r'(?:cuÃ¡l|cuÃ¡nto)\s+.*?(?:peso|pesa).*?(\d+)',
                r'(?:crecimiento|desarrollo).*?(\d+)\s*(?:dÃ­a|semana)',
                r'(\d+)\s*(?:dÃ­a|semana).*?(?:normal|referencia|estÃ¡ndar)'
            ]
        }
        
        patterns = weight_age_patterns.get(language, weight_age_patterns["fr"])
        
        age_detected = None
        for pattern in patterns:
            match = re.search(pattern, question_lower)
            if match:
                age_detected = match.group(1)
                break
        
        if not age_detected:
            return None
        
        breed_patterns = [
            r'\b(ross\s*308|ross\s*708|cobb\s*500|cobb\s*700|hubbard|arbor\s*acres)\b',
            r'\b(broiler|poulet|chicken|pollo)\s+(ross|cobb|hubbard)',
            r'\brace\s*[:\-]?\s*(ross|cobb|hubbard)'
        ]
        
        sex_patterns = [
            r'\b(mÃ¢le|male|macho)s?\b',
            r'\b(femelle|female|hembra)s?\b',
            r'\b(coq|hen|poule|gallina)\b',
            r'\b(mixte|mixed|misto)\b'
        ]
        
        has_breed = any(re.search(pattern, question_lower, re.IGNORECASE) for pattern in breed_patterns)
        has_sex = any(re.search(pattern, question_lower, re.IGNORECASE) for pattern in sex_patterns)
        
        if not has_breed and not has_sex:
            return {
                "type": "performance_question_missing_breed_sex",
                "age_detected": age_detected,
                "question_type": "weight_performance",
                "missing_info": ["breed", "sex"],
                "confidence": 0.95
            }
        
        elif not has_breed or not has_sex:
            missing = []
            if not has_breed:
                missing.append("breed")
            if not has_sex:
                missing.append("sex")
            
            return {
                "type": "performance_question_partial_info",
                "age_detected": age_detected,
                "question_type": "weight_performance", 
                "missing_info": missing,
                "confidence": 0.8
            }
        
        return None

    def _generate_performance_clarification_response(
        self, question: str, clarification_info: Dict, language: str, conversation_id: str
    ) -> EnhancedExpertResponse:
        """GÃ©nÃ¨re la demande de clarification optimisÃ©e (CONSERVÃ‰)"""
        
        age = clarification_info.get("age_detected", "X")
        missing_info = clarification_info.get("missing_info", [])
        
        clarification_messages = {
            "fr": {
                "both_missing": f"Pour vous donner le poids de rÃ©fÃ©rence exact d'un poulet de {age} jours, j'ai besoin de :\n\nâ€¢ **Race/souche** : Ross 308, Cobb 500, Hubbard, etc.\nâ€¢ **Sexe** : MÃ¢les, femelles, ou troupeau mixte\n\nPouvez-vous prÃ©ciser ces informations ?",
                "breed_missing": f"Pour le poids exact Ã  {age} jours, quelle est la **race/souche** (Ross 308, Cobb 500, Hubbard, etc.) ?",
                "sex_missing": f"Pour le poids exact Ã  {age} jours, s'agit-il de **mÃ¢les, femelles, ou d'un troupeau mixte** ?"
            },
            "en": {
                "both_missing": f"To give you the exact reference weight for a {age}-day chicken, I need:\n\nâ€¢ **Breed/strain**: Ross 308, Cobb 500, Hubbard, etc.\nâ€¢ **Sex**: Males, females, or mixed flock\n\nCould you specify this information?",
                "breed_missing": f"For the exact weight at {age} days, what is the **breed/strain** (Ross 308, Cobb 500, Hubbard, etc.)?",
                "sex_missing": f"For the exact weight at {age} days, are these **males, females, or a mixed flock**?"
            },
            "es": {
                "both_missing": f"Para darle el peso de referencia exacto de un pollo de {age} dÃ­as, necesito:\n\nâ€¢ **Raza/cepa**: Ross 308, Cobb 500, Hubbard, etc.\nâ€¢ **Sexo**: Machos, hembras, o lote mixto\n\nÂ¿PodrÃ­a especificar esta informaciÃ³n?",
                "breed_missing": f"Para el peso exacto a los {age} dÃ­as, Â¿cuÃ¡l es la **raza/cepa** (Ross 308, Cobb 500, Hubbard, etc.)?",
                "sex_missing": f"Para el peso exacto a los {age} dÃ­as, Â¿son **machos, hembras, o un lote mixto**?"
            }
        }
        
        messages = clarification_messages.get(language, clarification_messages["fr"])
        
        if len(missing_info) >= 2:
            response_text = messages["both_missing"]
        elif "breed" in missing_info:
            response_text = messages["breed_missing"]
        else:
            response_text = messages["sex_missing"]
        
        examples = {
            "fr": "\n\n**Exemples de rÃ©ponses :**\nâ€¢ \"Ross 308 mÃ¢les\"\nâ€¢ \"Cobb 500 femelles\"\nâ€¢ \"Hubbard troupeau mixte\"",
            "en": "\n\n**Example responses:**\nâ€¢ \"Ross 308 males\"\nâ€¢ \"Cobb 500 females\"\nâ€¢ \"Hubbard mixed flock\"",
            "es": "\n\n**Ejemplos de respuestas:**\nâ€¢ \"Ross 308 machos\"\nâ€¢ \"Cobb 500 hembras\"\nâ€¢ \"Hubbard lote mixto\""
        }
        
        response_text += examples.get(language, examples["fr"])
        
        return EnhancedExpertResponse(
            question=question,
            response=response_text,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=50,
            mode="smart_performance_clarification_corrected",
            user=None,
            logged=True,
            validation_passed=True,
            clarification_result={
                "clarification_requested": True,
                "clarification_type": "performance_breed_sex",
                "missing_information": missing_info,
                "age_detected": age,
                "confidence": clarification_info.get("confidence", 0.9)
            },
            processing_steps=["smart_clarification_triggered"],
            ai_enhancements_used=["performance_question_detection", "targeted_clarification"]
        )
    
    def _generate_follow_up_clarification(
        self, question: str, validation: Dict, language: str, conversation_id: str
    ) -> EnhancedExpertResponse:
        """GÃ©nÃ¨re une clarification de suivi si premiÃ¨re rÃ©ponse incomplÃ¨te (CONSERVÃ‰)"""
        
        still_missing = validation["still_missing"]
        
        messages = {
            "fr": {
                "breed": "Il me manque encore la **race/souche**. Ross 308, Cobb 500, ou autre ?",
                "sex": "Il me manque encore le **sexe**. MÃ¢les, femelles, ou troupeau mixte ?",  
                "both": "Il me manque encore la **race et le sexe**. Exemple : \"Ross 308 mÃ¢les\""
            },
            "en": {
                "breed": "I still need the **breed/strain**. Ross 308, Cobb 500, or other?",
                "sex": "I still need the **sex**. Males, females, or mixed flock?",
                "both": "I still need the **breed and sex**. Example: \"Ross 308 males\""
            },
            "es": {
                "breed": "AÃºn necesito la **raza/cepa**. Â¿Ross 308, Cobb 500, u otra?",
                "sex": "AÃºn necesito el **sexo**. Â¿Machos, hembras, o lote mixto?",
                "both": "AÃºn necesito la **raza y sexo**. Ejemplo: \"Ross 308 machos\""
            }
        }
        
        lang_messages = messages.get(language, messages["fr"])
        
        if len(still_missing) >= 2:
            message = lang_messages["both"]
        elif "breed" in still_missing:
            message = lang_messages["breed"]
        else:
            message = lang_messages["sex"]
        
        return EnhancedExpertResponse(
            question=question,
            response=message,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=30,
            mode="follow_up_clarification_corrected",
            user=None,
            logged=True,
            validation_passed=True,
            clarification_result={
                "clarification_requested": True,
                "clarification_type": "follow_up_incomplete",
                "still_missing_information": still_missing,
                "confidence": validation["confidence"]
            },
            processing_steps=["follow_up_clarification_triggered"],
            ai_enhancements_used=["incomplete_clarification_handling"]
        )
    
    # === MÃ‰THODES D'ENRICHISSEMENT COMPLET (CONSERVÃ‰ES IDENTIQUES) ===
    
    def _extract_age_from_original_question(self, original_question: str, language: str = "fr") -> Dict[str, Any]:
        """Extrait l'Ã¢ge depuis la question originale (CONSERVÃ‰)"""
        
        age_info = {"days": None, "weeks": None, "text": None, "detected": False}
        
        if not original_question:
            return age_info
        
        question_lower = original_question.lower()
        
        age_patterns = {
            "fr": [
                (r'(\d+)\s*jours?', "days"),
                (r'(\d+)\s*semaines?', "weeks"),
                (r'de\s+(\d+)\s*jours?', "days"),
                (r'de\s+(\d+)\s*semaines?', "weeks"),
                (r'Ã \s+(\d+)\s*jours?', "days"),
                (r'Ã \s+(\d+)\s*semaines?', "weeks")
            ],
            "en": [
                (r'(\d+)\s*days?', "days"),
                (r'(\d+)\s*weeks?', "weeks"),
                (r'of\s+(\d+)\s*days?', "days"),
                (r'of\s+(\d+)\s*weeks?', "weeks"),
                (r'at\s+(\d+)\s*days?', "days"),
                (r'at\s+(\d+)\s*weeks?', "weeks")
            ],
            "es": [
                (r'(\d+)\s*dÃ­as?', "days"),
                (r'(\d+)\s*semanas?', "weeks"),
                (r'de\s+(\d+)\s*dÃ­as?', "days"),
                (r'de\s+(\d+)\s*semanas?', "weeks"),
                (r'a\s+(\d+)\s*dÃ­as?', "days"),
                (r'a\s+(\d+)\s*semanas?', "weeks")
            ]
        }
        
        patterns = age_patterns.get(language, age_patterns["fr"])
        
        for pattern, unit in patterns:
            match = re.search(pattern, question_lower, re.IGNORECASE)
            if match:
                value = int(match.group(1))
                
                if unit == "days":
                    age_info["days"] = value
                    age_info["weeks"] = round(value / 7, 1)
                    age_info["text"] = f"{value} jour{'s' if value > 1 else ''}"
                else:
                    age_info["weeks"] = value
                    age_info["days"] = value * 7
                    age_info["text"] = f"{value} semaine{'s' if value > 1 else ''}"
                
                age_info["detected"] = True
                
                logger.info(f"ðŸ• [Age Extraction] Ã‚ge dÃ©tectÃ©: {age_info['text']} ({age_info['days']} jours)")
                break
        
        if not age_info["detected"]:
            logger.warning(f"âš ï¸ [Age Extraction] Aucun Ã¢ge dÃ©tectÃ© dans: {original_question}")
        
        return age_info

    def _build_complete_enriched_question(
        self, 
        original_question: str, 
        breed: Optional[str], 
        sex: Optional[str], 
        age_info: Dict[str, Any], 
        language: str = "fr"
    ) -> str:
        """Construit une question complÃ¨tement enrichie (CONSERVÃ‰)"""
        
        templates = {
            "fr": {
                "complete": "Pour des poulets {breed} {sex} de {age}",
                "breed_age": "Pour des poulets {breed} de {age}",
                "sex_age": "Pour des poulets {sex} de {age}",
                "breed_sex": "Pour des poulets {breed} {sex}",
                "age_only": "Pour des poulets de {age}",
                "breed_only": "Pour des poulets {breed}",
                "sex_only": "Pour des poulets {sex}"
            },
            "en": {
                "complete": "For {breed} {sex} chickens at {age}",
                "breed_age": "For {breed} chickens at {age}",
                "sex_age": "For {sex} chickens at {age}",
                "breed_sex": "For {breed} {sex} chickens",
                "age_only": "For chickens at {age}",
                "breed_only": "For {breed} chickens",
                "sex_only": "For {sex} chickens"
            },
            "es": {
                "complete": "Para pollos {breed} {sex} de {age}",
                "breed_age": "Para pollos {breed} de {age}",
                "sex_age": "Para pollos {sex} de {age}",
                "breed_sex": "Para pollos {breed} {sex}",
                "age_only": "Para pollos de {age}",
                "breed_only": "Para pollos {breed}",
                "sex_only": "Para pollos {sex}"
            }
        }
        
        template_set = templates.get(language, templates["fr"])
        
        context_prefix = ""
        age_text = age_info.get("text") if age_info.get("detected") else None
        
        if breed and sex and age_text:
            context_prefix = template_set["complete"].format(
                breed=breed, sex=sex, age=age_text
            )
            logger.info(f"ðŸŒŸ [Complete Enrichment] Contexte COMPLET: {context_prefix}")
            
        elif breed and age_text:
            context_prefix = template_set["breed_age"].format(
                breed=breed, age=age_text
            )
            logger.info(f"ðŸ·ï¸ [Breed+Age] Contexte: {context_prefix}")
            
        elif sex and age_text:
            context_prefix = template_set["sex_age"].format(
                sex=sex, age=age_text
            )
            logger.info(f"âš§ [Sex+Age] Contexte: {context_prefix}")
            
        elif breed and sex:
            context_prefix = template_set["breed_sex"].format(
                breed=breed, sex=sex
            )
            logger.info(f"ðŸ·ï¸âš§ [Breed+Sex] Contexte: {context_prefix}")
            
        elif age_text:
            context_prefix = template_set["age_only"].format(age=age_text)
            logger.info(f"ðŸ• [Age Only] Contexte: {context_prefix}")
            
        elif breed:
            context_prefix = template_set["breed_only"].format(breed=breed)
            logger.info(f"ðŸ·ï¸ [Breed Only] Contexte: {context_prefix}")
            
        elif sex:
            context_prefix = template_set["sex_only"].format(sex=sex)
            logger.info(f"âš§ [Sex Only] Contexte: {context_prefix}")
        
        if context_prefix:
            original_lower = original_question.lower().strip()
            
            if "quel est" in original_lower or "what is" in original_lower or "cuÃ¡l es" in original_lower:
                enriched_question = f"{context_prefix}, {original_lower}"
            elif "comment" in original_lower or "how" in original_lower or "cÃ³mo" in original_lower:
                enriched_question = f"{context_prefix}: {original_question}"
            else:
                enriched_question = f"{context_prefix}: {original_question}"
            
            logger.info(f"âœ¨ [Final Enrichment] Question finale: {enriched_question}")
            return enriched_question
        
        else:
            logger.warning("âš ï¸ [Enrichment] Pas d'enrichissement possible, question originale conservÃ©e")
            return original_question
    
    # === MÃ‰THODES UTILITAIRES AVEC TOUTES LES AMÃ‰LIORATIONS ===
    
    def _create_vagueness_response(
        self, vagueness_result, question_text: str, conversation_id: str,
        language: str, start_time: float, processing_steps: list, ai_enhancements_used: list
    ) -> EnhancedExpertResponse:
        """CrÃ©e une rÃ©ponse spÃ©cialisÃ©e pour questions floues (CONSERVÃ‰)"""
        
        clarification_messages = {
            "fr": f"Votre question semble manquer de prÃ©cision. {vagueness_result.suggested_clarification or 'Pouvez-vous Ãªtre plus spÃ©cifique ?'}",
            "en": f"Your question seems to lack precision. {vagueness_result.suggested_clarification or 'Could you be more specific?'}",
            "es": f"Su pregunta parece carecer de precisiÃ³n. {vagueness_result.suggested_clarification or 'Â¿PodrÃ­a ser mÃ¡s especÃ­fico?'}"
        }
        
        response_message = clarification_messages.get(language, clarification_messages["fr"])
        
        if vagueness_result.question_clarity in ["very_unclear", "unclear"]:
            examples = {
                "fr": "\n\nExemples de questions prÃ©cises:\nâ€¢ Quel est le poids normal d'un Ross 308 de 21 jours?\nâ€¢ Comment traiter la mortalitÃ© Ã©levÃ©e chez des poulets de 3 semaines?\nâ€¢ Quelle tempÃ©rature maintenir pour des poussins de 7 jours?",
                "en": "\n\nExamples of precise questions:\nâ€¢ What is the normal weight of a 21-day Ross 308?\nâ€¢ How to treat high mortality in 3-week-old chickens?\nâ€¢ What temperature to maintain for 7-day chicks?",
                "es": "\n\nEjemplos de preguntas precisas:\nâ€¢ Â¿CuÃ¡l es el peso normal de un Ross 308 de 21 dÃ­as?\nâ€¢ Â¿CÃ³mo tratar la alta mortalidad en pollos de 3 semanas?\nâ€¢ Â¿QuÃ© temperatura mantener para pollitos de 7 dÃ­as?"
            }
            response_message += examples.get(language, examples["fr"])
        
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return EnhancedExpertResponse(
            question=question_text,
            response=response_message,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode="vagueness_clarification",
            user=None,
            logged=True,
            validation_passed=True,
            vagueness_detection=vagueness_result,
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used
        )
    
    def _build_final_enhanced_response(
        self, question_text: str, answer: str, conversation_id: str,
        user_email: Optional[str], language: str, response_time_ms: int,
        expert_result: Dict, validation_result: ValidationResult,
        conversation_context: Any, processing_steps: list,
        ai_enhancements_used: list, request_data: EnhancedQuestionRequest,
        debug_info: Dict, performance_breakdown: Dict
    ) -> EnhancedExpertResponse:
        """Construit la rÃ©ponse finale avec toutes les amÃ©liorations"""
        
        extracted_entities = expert_result.get("extracted_entities")
        confidence_overall = None
        conversation_state = None
        
        if conversation_context and hasattr(conversation_context, 'consolidated_entities'):
            if not extracted_entities:
                extracted_entities = conversation_context.consolidated_entities.to_dict()
            confidence_overall = conversation_context.consolidated_entities.confidence_overall
            conversation_state = conversation_context.conversation_urgency
        
        final_debug_info = None
        final_performance = None
        
        if request_data.debug_mode:
            final_debug_info = {
                **debug_info,
                "total_processing_time_ms": response_time_ms,
                "ai_enhancements_count": len(ai_enhancements_used),
                "processing_steps_count": len(processing_steps),
                "concision_applied": expert_result.get("concision_applied", False),
                "response_versions_support": True,
                "taxonomy_used": expert_result.get("taxonomy_used"),
                "taxonomy_filters_applied": expert_result.get("taxonomy_filters_applied", False),
                "semantic_dynamic_available": True,
                "auto_clarification_available": True,
                "auto_clarification_used": "auto_clarification_context_based" in ai_enhancements_used
            }
            
            final_performance = performance_breakdown
        
        concision_info = {
            "concision_applied": expert_result.get("concision_applied", False),
            "original_response_available": "original_answer" in expert_result,
            "detected_question_type": None,
            "applied_concision_level": None,
            "response_versions_supported": True
        }
        
        if expert_result.get("concision_applied"):
            concision_info["detected_question_type"] = self.concision_processor.detect_question_type(question_text)
            concision_info["applied_concision_level"] = self.concision_processor.detect_optimal_concision_level(question_text).value
        
        taxonomy_info = {
            "taxonomy_detected": expert_result.get("taxonomy_used", "general"),
            "taxonomy_filters_applied": expert_result.get("taxonomy_filters_applied", False),
            "taxonomy_enhanced_question": bool(expert_result.get("taxonomy_used") != "general")
        }
        
        semantic_dynamic_info = {
            "semantic_dynamic_available": getattr(request_data, 'semantic_dynamic_mode', False),
            "semantic_dynamic_used": "semantic_dynamic_clarification" in ai_enhancements_used,
            "dynamic_questions_generated": any("semantic_dynamic" in step for step in processing_steps)
        }
        
        # ðŸ”§ NOUVEAU: Informations auto-clarification
        auto_clarification_info = {
            "auto_clarification_available": True,
            "auto_clarification_used": "auto_clarification_context_based" in ai_enhancements_used,
            "context_score_evaluated": any("context" in step for step in processing_steps),
            "automatic_trigger": "automatic_clarification_triggered" in processing_steps
        }
            
        return EnhancedExpertResponse(
            question=str(question_text),
            response=str(answer),
            conversation_id=conversation_id,
            rag_used=expert_result["rag_used"],
            rag_score=expert_result["rag_score"],
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode=expert_result["mode"],
            user=user_email,
            logged=True,
            validation_passed=True,
            validation_confidence=validation_result.confidence,
            reprocessed_after_clarification=request_data.is_clarification_response,
            conversation_state=conversation_state,
            extracted_entities=extracted_entities,
            confidence_overall=confidence_overall,
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used,
            
            document_relevance=expert_result.get("document_relevance"),
            context_coherence=expert_result.get("context_coherence"),
            vagueness_detection=None,
            fallback_details=None,
            response_format_applied=request_data.expected_response_format.value,
            quality_metrics=expert_result.get("quality_metrics"),
            debug_info=final_debug_info,
            performance_breakdown=final_performance,
            
            concision_info=concision_info,
            original_response=expert_result.get("original_answer"),
            
            response_versions=None,
            concision_metrics=None,
            
            taxonomy_info=taxonomy_info,
            
            semantic_dynamic_info=semantic_dynamic_info,
            
            # ðŸ”§ NOUVEAU: Auto-clarification info
            auto_clarification_info=auto_clarification_info
        )
    
    # === MÃ‰THODES UTILITAIRES IDENTIQUES (CONSERVÃ‰ES) ===
    
    def _extract_user_id(self, current_user: Optional[Dict], request_data: EnhancedQuestionRequest, request: Request) -> str:
        if current_user:
            return current_user.get("user_id") or request_data.user_id or "authenticated_user"
        return request_data.user_id or get_user_id_from_request(request)
    
    def _get_or_create_conversation_id(self, request_data: EnhancedQuestionRequest) -> str:
        if request_data.conversation_id and request_data.conversation_id.strip():
            conversation_id = request_data.conversation_id.strip()
            logger.info(f"ðŸ”„ [Expert Service] CONTINUATION: {conversation_id}")
            return conversation_id
        else:
            conversation_id = str(uuid.uuid4())
            logger.info(f"ðŸ†• [Expert Service] NOUVELLE: {conversation_id}")
            return conversation_id
    
    def _validate_rag_response_quality(
        self, answer: str, enriched_question: str, enhancement_info: Dict
    ) -> Dict[str, any]:
        """Valide la qualitÃ© de la rÃ©ponse RAG (CONSERVÃ‰)"""
        
        if not answer or len(answer.strip()) < 20:
            return {
                "valid": False,
                "reason": "RÃ©ponse trop courte",
                "answer_length": len(answer) if answer else 0
            }
        
        negative_responses = [
            "je ne sais pas", "i don't know", "no sÃ©",
            "pas d'information", "no information", "sin informaciÃ³n"
        ]
        
        answer_lower = answer.lower()
        for negative in negative_responses:
            if negative in answer_lower:
                return {
                    "valid": False,
                    "reason": f"RÃ©ponse nÃ©gative dÃ©tectÃ©e: {negative}",
                    "answer_length": len(answer)
                }
        
        if any(word in enriched_question.lower() for word in ["poids", "weight", "peso"]):
            if not re.search(r'\d+', answer):
                return {
                    "valid": False,
                    "reason": "Question numÃ©rique mais pas de chiffres dans la rÃ©ponse",
                    "answer_length": len(answer)
                }
        
        return {
            "valid": True,
            "reason": "RÃ©ponse valide",
            "answer_length": len(answer)
        }
    
    # === AUTRES MÃ‰THODES CONSERVÃ‰ES ===
    
    async def _validate_agricultural_question(self, question: str, language: str, user_id: str, request_ip: str, conversation_id: str) -> ValidationResult:
        if not self.integrations.agricultural_validator_available:
            return ValidationResult(is_valid=False, rejection_message="Service temporairement indisponible")
        
        try:
            enriched_question = question
            if self.integrations.intelligent_memory_available:
                try:
                    rag_context = self.integrations.get_context_for_rag(conversation_id)
                    if rag_context:
                        enriched_question = f"{question}\n\nContexte: {rag_context}"
                except Exception:
                    pass
            
            validation_result = self.integrations.validate_agricultural_question(
                question=enriched_question, language=language, user_id=user_id, request_ip=request_ip
            )
            
            return ValidationResult(
                is_valid=validation_result.is_valid,
                rejection_message=validation_result.reason or "Question hors domaine agricole",
                confidence=validation_result.confidence
            )
            
        except Exception as e:
            logger.error(f"âŒ [Expert Service] Erreur validateur: {e}")
            return ValidationResult(is_valid=False, rejection_message="Erreur de validation")
    
    def _create_rejection_response(self, question_text, validation_result, conversation_id, user_email, language, start_time, processing_steps, ai_enhancements_used, vagueness_result=None):
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return EnhancedExpertResponse(
            question=str(question_text),
            response=str(validation_result.rejection_message),
            conversation_id=conversation_id,
            rag_used=False,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode="enhanced_agricultural_validation_rejected",
            user=user_email,
            logged=True,
            validation_passed=False,
            validation_confidence=validation_result.confidence,
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used,
            vagueness_detection=vagueness_result
        )
    
    async def process_feedback(self, feedback_data: FeedbackRequest) -> Dict[str, Any]:
        feedback_updated = False
        
        if feedback_data.conversation_id and self.integrations.logging_available:
            try:
                rating_numeric = {"positive": 1, "negative": -1, "neutral": 0}.get(feedback_data.rating, 0)
                feedback_updated = await self.integrations.update_feedback(feedback_data.conversation_id, rating_numeric)
            except Exception as e:
                logger.error(f"âŒ [Expert Service] Erreur feedback: {e}")
        
        return {
            "success": True,
            "message": "Feedback enregistrÃ© avec succÃ¨s (Version ComplÃ¨te 4.0.0 - Auto-Clarification IntÃ©grÃ©e)",
            "rating": feedback_data.rating,
            "comment": feedback_data.comment,
            "conversation_id": feedback_data.conversation_id,
            "feedback_updated_in_db": feedback_updated,
            "enhanced_features_used": True,
            "concision_system_active": self.concision_processor.config.ENABLE_CONCISE_RESPONSES,
            "response_versions_supported": True,
            "taxonomic_filtering_active": True,
            "semantic_dynamic_available": True,
            "auto_clarification_available": True,
            "auto_clarification_active": self.auto_clarification.enable_auto_clarification,
            "timestamp": datetime.now().isoformat()
        }
    
    async def get_suggested_topics(self, language: str) -> Dict[str, Any]:
        lang = language.lower() if language else "fr"
        if lang not in ["fr", "en", "es"]:
            lang = "fr"
        
        topics_by_language = get_enhanced_topics_by_language()
        topics = topics_by_language.get(lang, topics_by_language["fr"])
        
        return {
            "topics": topics,
            "language": lang,
            "count": len(topics),
            "enhanced_features": {
                "vagueness_detection_available": True,
                "context_coherence_available": True,
                "detailed_rag_scoring_available": True,
                "quality_metrics_available": True,
                "smart_clarification_available": True,
                "intelligent_memory_available": self.integrations.intelligent_memory_available,
                
                "response_concision_available": True,
                "concision_levels": [level.value for level in ConcisionLevel],
                "auto_concision_detection": True,
                "concision_enabled": self.concision_processor.config.ENABLE_CONCISE_RESPONSES,
                
                "response_versions_available": True,
                "multiple_concision_levels_generation": True,
                "dynamic_level_switching_support": True,
                "concision_metrics_available": True,
                
                "taxonomic_filtering_available": True,
                "supported_taxonomies": ["broiler", "layer", "swine", "dairy", "general"],
                "automatic_taxonomy_detection": True,
                "taxonomy_based_document_filtering": True,
                
                "semantic_dynamic_clarification_available": True,
                "gpt_question_generation": True,
                "contextual_clarification_questions": True,
                "intelligent_clarification_mode": True,
                
                # ðŸ”§ NOUVEAU: Auto-clarification features
                "auto_clarification_available": True,
                "context_completeness_scoring": True,
                "automatic_trigger_threshold": self.auto_clarification.context_threshold,
                "smart_question_evaluation": True,
                "auto_clarification_types": ["performance", "health", "environment", "general"]
            },
            "system_status": {
                "validation_enabled": self.integrations.is_agricultural_validation_enabled(),
                "enhanced_clarification_enabled": self.integrations.is_enhanced_clarification_enabled(),
                "intelligent_memory_enabled": self.integrations.intelligent_memory_available,
                "api_enhancements_enabled": True,
                "concision_processor_enabled": True,
                "response_versions_generator_enabled": True,
                "taxonomic_filtering_enabled": True,
                "semantic_dynamic_clarification_enabled": True,
                "auto_clarification_enabled": self.auto_clarification.enable_auto_clarification
            },
            
            "concision_config": {
                "default_level": self.concision_processor.config.DEFAULT_CONCISION_LEVEL.value,
                "auto_detect_enabled": True,
                "max_lengths": self.concision_processor.config.MAX_RESPONSE_LENGTH,
                "ultra_concise_keywords": self.concision_processor.config.ULTRA_CONCISE_KEYWORDS,
                "complex_keywords": self.concision_processor.config.COMPLEX_KEYWORDS,
                
                "response_versions_generation": {
                    "enabled": True,
                    "supported_levels": [level.value for level in ConcisionLevel],
                    "metrics_included": True,
                    "cache_supported": False,
                    "fallback_strategy": "simple_truncation"
                }
            },
            
            "taxonomic_config": {
                "enabled": True,
                "supported_categories": {
                    "broiler": ["ross", "cobb", "hubbard", "indian river"],
                    "layer": ["lohmann", "isa", "dekalb", "hy-line", "bovans", "h&n", "shaver"],
                    "swine": ["gestation_day", "parity"],
                    "dairy": ["days_in_milk", "milk_yield_liters"]
                },
                "auto_detection_enabled": True,
                "filter_fallback_enabled": True,
                "question_enhancement_enabled": True
            },
            
            "semantic_dynamic_config": {
                "enabled": True,
                "max_questions_generated": 4,
                "supported_languages": ["fr", "en", "es"],
                "gpt_model_used": "gpt-4o-mini",
                "fallback_questions_available": True,
                "context_aware_generation": True,
                "automatic_mode_detection": True,
                "validation_enabled": True
            },
            
            # ðŸ”§ NOUVEAU: Config auto-clarification
            "auto_clarification_config": {
                "enabled": self.auto_clarification.enable_auto_clarification,
                "context_score_threshold": self.auto_clarification.context_threshold,
                "evaluation_criteria": ["question_length", "specific_breeds", "age_info", "numeric_data", "conversational_context"],
                "automatic_trigger": True,