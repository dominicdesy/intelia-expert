"""
app/api/v1/expert_services.py - SERVICES M√âTIER EXPERT SYSTEM

Logique m√©tier principale pour le syst√®me expert
VERSION FINALE CORRIG√âE : RAG-First + Syst√®me de clarification intelligent
"""

import os
import logging
import uuid
import time
import re
from datetime import datetime
from typing import Optional, Dict, Any, Tuple

from fastapi import HTTPException, Request

from .expert_models import (
    EnhancedQuestionRequest, EnhancedExpertResponse, FeedbackRequest,
    ValidationResult, ProcessingContext, VaguenessResponse, ResponseFormat
)
from .expert_utils import (
    get_user_id_from_request, 
    build_enriched_question_from_clarification,
    get_enhanced_topics_by_language,
    save_conversation_auto_enhanced,
    extract_breed_and_sex_from_clarification,
    build_enriched_question_with_breed_sex,
    validate_clarification_completeness
)
from .expert_integrations import IntegrationsManager
from .api_enhancement_service import APIEnhancementService

logger = logging.getLogger(__name__)

class RAGContextEnhancer:
    """Am√©liore le contexte conversationnel pour optimiser les requ√™tes RAG"""
    
    def __init__(self):
        # Patterns pour d√©tecter les r√©f√©rences contextuelles
        self.pronoun_patterns = {
            "fr": [
                r'\b(son|sa|ses|leur|leurs)\s+(poids|√¢ge|croissance|d√©veloppement)',
                r'\b(ils|elles)\s+(p√®sent|grandissent|se d√©veloppent)',
                r'\b(qu\'?est-ce que|quel est)\s+(son|sa|ses|leur)',
                r'\b(combien)\s+(p√®sent-ils|font-ils|mesurent-ils)'
            ],
            "en": [
                r'\b(their|its)\s+(weight|age|growth|development)',
                r'\b(they)\s+(weigh|grow|develop)',
                r'\b(what is|how much is)\s+(their|its)',
                r'\b(how much do they)\s+(weigh|measure)'
            ],
            "es": [
                r'\b(su|sus)\s+(peso|edad|crecimiento|desarrollo)',
                r'\b(ellos|ellas)\s+(pesan|crecen|se desarrollan)',
                r'\b(cu√°l es|cu√°nto es)\s+(su|sus)',
                r'\b(cu√°nto)\s+(pesan|miden)'
            ]
        }
    
    def enhance_question_for_rag(
        self, 
        question: str, 
        conversation_context: str, 
        language: str = "fr"
    ) -> Tuple[str, Dict[str, any]]:
        """Am√©liore une question pour le RAG en utilisant le contexte conversationnel"""
        
        enhancement_info = {
            "pronoun_detected": False,
            "context_entities_used": [],
            "question_enriched": False,
            "original_question": question
        }
        
        # 1. D√©tecter les pronoms/r√©f√©rences contextuelles
        has_pronouns = self._detect_contextual_references(question, language)
        if has_pronouns:
            enhancement_info["pronoun_detected"] = True
            logger.info(f"üîç [RAG Context] Pronoms d√©tect√©s dans: '{question}'")
        
        # 2. Extraire entit√©s du contexte
        context_entities = self._extract_context_entities(conversation_context)
        if context_entities:
            enhancement_info["context_entities_used"] = list(context_entities.keys())
            logger.info(f"üìä [RAG Context] Entit√©s contextuelles: {context_entities}")
        
        # 3. Enrichir la question si n√©cessaire
        enriched_question = question
        
        if has_pronouns and context_entities:
            enriched_question = self._build_enriched_question(
                question, context_entities, language
            )
            enhancement_info["question_enriched"] = True
            logger.info(f"‚ú® [RAG Context] Question enrichie: '{enriched_question}'")
        
        # 4. Ajouter contexte technique si pertinent
        if context_entities or has_pronouns:
            technical_context = self._build_technical_context(context_entities, language)
            if technical_context:
                enriched_question += f"\n\nContexte technique: {technical_context}"
        
        return enriched_question, enhancement_info
    
    def _detect_contextual_references(self, question: str, language: str) -> bool:
        """D√©tecte si la question contient des pronoms/r√©f√©rences contextuelles"""
        
        patterns = self.pronoun_patterns.get(language, self.pronoun_patterns["fr"])
        question_lower = question.lower()
        
        for pattern in patterns:
            if re.search(pattern, question_lower, re.IGNORECASE):
                logger.debug(f"üéØ [RAG Context] Pattern trouv√©: {pattern}")
                return True
        
        return False
    
    def _extract_context_entities(self, context: str) -> Dict[str, str]:
        """Extrait les entit√©s importantes du contexte conversationnel"""
        
        if not context:
            return {}
        
        entities = {}
        context_lower = context.lower()
        
        # Extraire race
        breed_patterns = [
            r'race[:\s]+([a-zA-Z0-9\s]+?)(?:\n|,|\.|\s|$)',
            r'breed[:\s]+([a-zA-Z0-9\s]+?)(?:\n|,|\.|\s|$)',
            r'(ross\s*308|cobb\s*500|hubbard|arbor\s*acres)',
            r'poulets?\s+(ross\s*308|cobb\s*500)',
            r'chickens?\s+(ross\s*308|cobb\s*500)'
        ]
        
        for pattern in breed_patterns:
            match = re.search(pattern, context_lower, re.IGNORECASE)
            if match:
                entities["breed"] = match.group(1).strip()
                break
        
        # Extraire √¢ge
        age_patterns = [
            r'√¢ge[:\s]+(\d+\s*(?:jour|semaine|day|week)s?)',
            r'age[:\s]+(\d+\s*(?:jour|semaine|day|week)s?)',
            r'(\d+)\s*(?:jour|day)s?',
            r'(\d+)\s*(?:semaine|week)s?'
        ]
        
        for pattern in age_patterns:
            match = re.search(pattern, context_lower, re.IGNORECASE)
            if match:
                entities["age"] = match.group(1).strip()
                break
        
        return entities
    
    def _build_enriched_question(
        self, 
        question: str, 
        context_entities: Dict[str, str], 
        language: str
    ) -> str:
        """Construit une question enrichie en rempla√ßant les pronoms par les entit√©s contextuelles"""
        
        enriched = question
        
        # Templates par langue
        templates = {
            "fr": {
                "breed_age": "Pour des {breed} de {age}",
                "breed_only": "Pour des {breed}",
                "age_only": "Pour des poulets de {age}"
            },
            "en": {
                "breed_age": "For {breed} chickens at {age}",
                "breed_only": "For {breed} chickens", 
                "age_only": "For chickens at {age}"
            },
            "es": {
                "breed_age": "Para pollos {breed} de {age}",
                "breed_only": "Para pollos {breed}",
                "age_only": "Para pollos de {age}"
            }
        }
        
        template_set = templates.get(language, templates["fr"])
        
        # Construire le pr√©fixe contextuel
        context_prefix = ""
        if "breed" in context_entities and "age" in context_entities:
            context_prefix = template_set["breed_age"].format(
                breed=context_entities["breed"],
                age=context_entities["age"]
            )
        elif "breed" in context_entities:
            context_prefix = template_set["breed_only"].format(
                breed=context_entities["breed"]
            )
        elif "age" in context_entities:
            context_prefix = template_set["age_only"].format(
                age=context_entities["age"]
            )
        
        if context_prefix:
            # Remplacer ou pr√©fixer selon la structure de la question
            if any(word in question.lower() for word in ["son", "sa", "ses", "leur", "leurs", "their", "its", "su", "sus"]):
                enriched = f"{context_prefix}, {question.lower()}"
            else:
                enriched = f"{context_prefix}: {question}"
        
        return enriched
    
    def _build_technical_context(self, entities: Dict[str, str], language: str) -> str:
        """Construit un contexte technique pour aider le RAG"""
        
        if not entities:
            return ""
        
        context_parts = []
        
        if "breed" in entities:
            context_parts.append(f"Race: {entities['breed']}")
        
        if "age" in entities:
            context_parts.append(f"√Çge: {entities['age']}")
        
        return " | ".join(context_parts)

class ExpertService:
    """Service principal pour le syst√®me expert avec toutes les am√©liorations"""
    
    def __init__(self):
        self.integrations = IntegrationsManager()
        self.rag_enhancer = RAGContextEnhancer()
        self.enhancement_service = APIEnhancementService()  # ‚úÖ NOUVEAU SERVICE
        logger.info("‚úÖ [Expert Service] Service expert initialis√© avec am√©liorations compl√®tes")
    
    def get_current_user_dependency(self):
        """Retourne la d√©pendance pour l'authentification"""
        return self.integrations.get_current_user_dependency()
    
    async def process_expert_question(
        self,
        request_data: EnhancedQuestionRequest,
        request: Request,
        current_user: Optional[Dict[str, Any]],
        start_time: float
    ) -> EnhancedExpertResponse:
        """Traite une question expert avec toutes les fonctionnalit√©s am√©lior√©es"""
        
        processing_steps = []
        ai_enhancements_used = []
        debug_info = {}
        performance_breakdown = {"start": int(time.time() * 1000)}
        
        # Initialisation
        processing_steps.append("initialization")
        
        # === AUTHENTIFICATION ===
        if current_user is None and self.integrations.auth_available:
            raise HTTPException(status_code=401, detail="Authentification requise")
        
        user_id = self._extract_user_id(current_user, request_data, request)
        user_email = current_user.get("email") if current_user else None
        request_ip = request.client.host if request.client else "unknown"
        
        processing_steps.append("authentication")
        performance_breakdown["auth_complete"] = int(time.time() * 1000)
        
        # === GESTION CONVERSATION ID ===
        conversation_id = self._get_or_create_conversation_id(request_data)
        
        # === VALIDATION QUESTION ===
        question_text = request_data.text.strip()
        if not question_text:
            raise HTTPException(status_code=400, detail="Question text is required")
        
        processing_steps.append("question_validation")
        
        # ‚úÖ NOUVEAU: D√âTECTION DE QUESTIONS FLOUES (AVANT TOUT TRAITEMENT)
        vagueness_result = None
        if request_data.enable_vagueness_detection:
            vagueness_result = self.enhancement_service.detect_vagueness(
                question_text, request_data.language
            )
            
            ai_enhancements_used.append("vagueness_detection")
            performance_breakdown["vagueness_check"] = int(time.time() * 1000)
            
            # ‚úÖ CORRECTION: R√©duire le seuil de 0.7 √† 0.6 pour d√©clencher plus facilement
            if vagueness_result.is_vague and vagueness_result.vagueness_score > 0.6:  # ‚Üê √©tait 0.7
                logger.info(f"üéØ [Expert Service] Question floue d√©tect√©e (score: {vagueness_result.vagueness_score})")
                return self._create_vagueness_response(
                    vagueness_result, question_text, conversation_id, 
                    request_data.language, start_time, processing_steps, ai_enhancements_used
                )
        
        # === ENREGISTREMENT DANS M√âMOIRE INTELLIGENTE ===
        conversation_context = None
        if self.integrations.intelligent_memory_available:
            try:
                conversation_context = self.integrations.add_message_to_conversation(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    message=question_text,
                    role="user",
                    language=request_data.language,
                    message_type="clarification_response" if request_data.is_clarification_response else "question"
                )
                ai_enhancements_used.append("intelligent_memory")
                processing_steps.append("memory_storage")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [Expert Service] Erreur m√©moire: {e}")
        
        performance_breakdown["memory_complete"] = int(time.time() * 1000)
        
        # === VALIDATION AGRICOLE ===
        validation_result = await self._validate_agricultural_question(
            question_text, request_data.language, user_id, request_ip, conversation_id
        )
        
        processing_steps.append("agricultural_validation")
        performance_breakdown["validation_complete"] = int(time.time() * 1000)
        
        if not validation_result.is_valid:
            return self._create_rejection_response(
                question_text, validation_result, conversation_id, 
                user_email, request_data.language, start_time,
                processing_steps, ai_enhancements_used, vagueness_result
            )
        
        # === ‚úÖ SYST√àME DE CLARIFICATION INTELLIGENT ===
        clarification_result = await self._handle_clarification(
            request_data, question_text, user_id, conversation_id,
            processing_steps, ai_enhancements_used
        )
        
        if clarification_result:
            return clarification_result
        
        performance_breakdown["clarification_complete"] = int(time.time() * 1000)
        
        # === TRAITEMENT EXPERT AVEC RAG-FIRST + AM√âLIORATIONS ===
        expert_result = await self._process_expert_response_enhanced(
            question_text, request_data, request, current_user,
            conversation_id, processing_steps, ai_enhancements_used,
            debug_info, performance_breakdown, vagueness_result
        )
        
        # === ENREGISTREMENT R√âPONSE ===
        if self.integrations.intelligent_memory_available and expert_result["answer"]:
            try:
                self.integrations.add_message_to_conversation(
                    conversation_id=conversation_id,
                    user_id=user_id,
                    message=expert_result["answer"],
                    role="assistant",
                    language=request_data.language,
                    message_type="response"
                )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [Expert Service] Erreur enregistrement r√©ponse: {e}")
        
        processing_steps.append("response_storage")
        performance_breakdown["final"] = int(time.time() * 1000)
        
        # === CONSTRUCTION R√âPONSE FINALE AM√âLIOR√âE ===
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return self._build_final_enhanced_response(
            question_text, expert_result["answer"], conversation_id,
            user_email, request_data.language, response_time_ms,
            expert_result, validation_result, conversation_context,
            processing_steps, ai_enhancements_used, request_data,
            debug_info, performance_breakdown
        )
    
    # ===========================================================================================
    # ‚úÖ NOUVELLES FONCTIONS DE CLARIFICATION INTELLIGENTE
    # ===========================================================================================
    
    async def _handle_clarification(
        self, request_data, question_text, user_id, conversation_id, 
        processing_steps, ai_enhancements_used
    ):
        """
        ‚úÖ SYST√àME DE CLARIFICATION CORRIG√â
        D√©tecte les questions techniques n√©cessitant des pr√©cisions sp√©cifiques
        """
        
        # 1. Ignorer si c'est d√©j√† une r√©ponse de clarification
        if request_data.is_clarification_response:
            # ‚úÖ NOUVEAU : Traitement des r√©ponses de clarification
            return await self._process_clarification_response(
                request_data, question_text, conversation_id,
                processing_steps, ai_enhancements_used
            )
        
        # 2. ‚úÖ NOUVELLE LOGIQUE : D√©tection questions poids/performance
        clarification_needed = self._detect_performance_question_needing_clarification(
            question_text, request_data.language
        )
        
        if not clarification_needed:
            return None
        
        logger.info(f"üéØ [Expert Service] Clarification n√©cessaire: {clarification_needed['type']}")
        processing_steps.append("automatic_clarification_triggered")
        ai_enhancements_used.append("smart_performance_clarification")
        
        # 3. G√©n√©rer la demande de clarification
        clarification_response = self._generate_performance_clarification_response(
            question_text, clarification_needed, request_data.language, conversation_id
        )
        
        return clarification_response
    
    async def _process_clarification_response(
        self, request_data, question_text, conversation_id, 
        processing_steps, ai_enhancements_used
    ):
        """
        ‚úÖ NOUVELLE FONCTION : Traite les r√©ponses de clarification
        """
        
        if not request_data.original_question or not request_data.clarification_context:
            logger.warning("‚ö†Ô∏è [Expert Service] R√©ponse clarification sans contexte")
            return None
        
        # Extraire les infos manquantes du contexte de clarification
        missing_info = request_data.clarification_context.get("missing_information", [])
        
        # Valider que la clarification contient les infos demand√©es
        validation = validate_clarification_completeness(
            question_text, missing_info, request_data.language
        )
        
        if not validation["is_complete"]:
            logger.info(f"üîÑ [Expert Service] Clarification incompl√®te: {validation['still_missing']}")
            # Redemander les infos manquantes
            return self._generate_follow_up_clarification(
                question_text, validation, request_data.language, conversation_id
            )
        
        # Enrichir la question originale avec les infos extraites
        breed = validation["extracted_info"].get("breed")
        sex = validation["extracted_info"].get("sex")
        
        enriched_original_question = build_enriched_question_with_breed_sex(
            request_data.original_question, breed, sex, request_data.language
        )
        
        logger.info(f"‚úÖ [Expert Service] Question enrichie par clarification: {enriched_original_question}")
        
        # Remplacer la question actuelle par la version enrichie
        request_data.text = enriched_original_question
        request_data.is_clarification_response = False  # Traiter comme nouvelle question
        
        processing_steps.append("clarification_processed_successfully")
        ai_enhancements_used.append("breed_sex_extraction")
        ai_enhancements_used.append("question_enrichment_from_clarification")
        
        return None  # Continuer le traitement normal avec question enrichie
    
    def _detect_performance_question_needing_clarification(
        self, question: str, language: str = "fr"
    ) -> Optional[Dict[str, Any]]:
        """
        ‚úÖ NOUVELLE FONCTION : D√©tecte les questions techniques n√©cessitant race/sexe
        """
        
        question_lower = question.lower()
        
        # Patterns de questions sur poids/performance avec √¢ge mais sans race/sexe
        weight_age_patterns = {
            "fr": [
                r'(?:poids|p√®se)\s+.*?(\d+)\s*(?:jour|semaine)s?',
                r'(\d+)\s*(?:jour|semaine)s?.*?(?:poids|p√®se)',
                r'(?:quel|combien)\s+.*?(?:poids|p√®se).*?(\d+)',
                r'(?:croissance|d√©veloppement).*?(\d+)\s*(?:jour|semaine)',
                r'(\d+)\s*(?:jour|semaine).*?(?:normal|r√©f√©rence|standard)'
            ],
            "en": [
                r'(?:weight|weigh)\s+.*?(\d+)\s*(?:day|week)s?',
                r'(\d+)\s*(?:day|week)s?.*?(?:weight|weigh)',
                r'(?:what|how much)\s+.*?(?:weight|weigh).*?(\d+)',
                r'(?:growth|development).*?(\d+)\s*(?:day|week)',
                r'(\d+)\s*(?:day|week).*?(?:normal|reference|standard)'
            ],
            "es": [
                r'(?:peso|pesa)\s+.*?(\d+)\s*(?:d√≠a|semana)s?',
                r'(\d+)\s*(?:d√≠a|semana)s?.*?(?:peso|pesa)',
                r'(?:cu√°l|cu√°nto)\s+.*?(?:peso|pesa).*?(\d+)',
                r'(?:crecimiento|desarrollo).*?(\d+)\s*(?:d√≠a|semana)',
                r'(\d+)\s*(?:d√≠a|semana).*?(?:normal|referencia|est√°ndar)'
            ]
        }
        
        patterns = weight_age_patterns.get(language, weight_age_patterns["fr"])
        
        # V√©rifier si c'est une question poids+√¢ge
        age_detected = None
        for pattern in patterns:
            match = re.search(pattern, question_lower)
            if match:
                age_detected = match.group(1)
                break
        
        if not age_detected:
            return None
        
        # V√©rifier si race/sexe sont ABSENTS
        breed_patterns = [
            r'\b(ross\s*308|ross\s*708|cobb\s*500|cobb\s*700|hubbard|arbor\s*acres)\b',
            r'\b(broiler|poulet|chicken|pollo)\s+(ross|cobb|hubbard)',
            r'\brace\s*[:\-]?\s*(ross|cobb|hubbard)'
        ]
        
        sex_patterns = [
            r'\b(m√¢le|male|macho)s?\b',
            r'\b(femelle|female|hembra)s?\b',
            r'\b(coq|hen|poule|gallina)\b',
            r'\b(mixte|mixed|misto)\b'
        ]
        
        has_breed = any(re.search(pattern, question_lower, re.IGNORECASE) for pattern in breed_patterns)
        has_sex = any(re.search(pattern, question_lower, re.IGNORECASE) for pattern in sex_patterns)
        
        # ‚úÖ CLARIFICATION N√âCESSAIRE si poids+√¢ge MAIS pas de race NI sexe
        if not has_breed and not has_sex:
            return {
                "type": "performance_question_missing_breed_sex",
                "age_detected": age_detected,
                "question_type": "weight_performance",
                "missing_info": ["breed", "sex"],
                "confidence": 0.9
            }
        
        # Clarification partielle si seulement un des deux manque
        elif not has_breed or not has_sex:
            missing = []
            if not has_breed:
                missing.append("breed")
            if not has_sex:
                missing.append("sex")
            
            return {
                "type": "performance_question_partial_info",
                "age_detected": age_detected,
                "question_type": "weight_performance", 
                "missing_info": missing,
                "confidence": 0.7
            }
        
        return None

    def _generate_performance_clarification_response(
        self, question: str, clarification_info: Dict, language: str, conversation_id: str
    ) -> EnhancedExpertResponse:
        """
        ‚úÖ NOUVELLE FONCTION : G√©n√®re la demande de clarification optimis√©e
        """
        
        age = clarification_info.get("age_detected", "X")
        missing_info = clarification_info.get("missing_info", [])
        
        # Messages de clarification par langue
        clarification_messages = {
            "fr": {
                "both_missing": f"Pour vous donner le poids de r√©f√©rence exact d'un poulet de {age} jours, j'ai besoin de :\n\n‚Ä¢ **Race/souche** : Ross 308, Cobb 500, Hubbard, etc.\n‚Ä¢ **Sexe** : M√¢les, femelles, ou troupeau mixte\n\nPouvez-vous pr√©ciser ces informations ?",
                "breed_missing": f"Pour le poids exact √† {age} jours, quelle est la **race/souche** (Ross 308, Cobb 500, Hubbard, etc.) ?",
                "sex_missing": f"Pour le poids exact √† {age} jours, s'agit-il de **m√¢les, femelles, ou d'un troupeau mixte** ?"
            },
            "en": {
                "both_missing": f"To give you the exact reference weight for a {age}-day chicken, I need:\n\n‚Ä¢ **Breed/strain**: Ross 308, Cobb 500, Hubbard, etc.\n‚Ä¢ **Sex**: Males, females, or mixed flock\n\nCould you specify this information?",
                "breed_missing": f"For the exact weight at {age} days, what is the **breed/strain** (Ross 308, Cobb 500, Hubbard, etc.)?",
                "sex_missing": f"For the exact weight at {age} days, are these **males, females, or a mixed flock**?"
            },
            "es": {
                "both_missing": f"Para darle el peso de referencia exacto de un pollo de {age} d√≠as, necesito:\n\n‚Ä¢ **Raza/cepa**: Ross 308, Cobb 500, Hubbard, etc.\n‚Ä¢ **Sexo**: Machos, hembras, o lote mixto\n\n¬øPodr√≠a especificar esta informaci√≥n?",
                "breed_missing": f"Para el peso exacto a los {age} d√≠as, ¬øcu√°l es la **raza/cepa** (Ross 308, Cobb 500, Hubbard, etc.)?",
                "sex_missing": f"Para el peso exacto a los {age} d√≠as, ¬øson **machos, hembras, o un lote mixto**?"
            }
        }
        
        messages = clarification_messages.get(language, clarification_messages["fr"])
        
        # S√©lectionner le message appropri√©
        if len(missing_info) >= 2:
            response_text = messages["both_missing"]
        elif "breed" in missing_info:
            response_text = messages["breed_missing"]
        else:
            response_text = messages["sex_missing"]
        
        # Ajouter exemples de r√©ponse
        examples = {
            "fr": "\n\n**Exemples de r√©ponses :**\n‚Ä¢ \"Ross 308 m√¢les\"\n‚Ä¢ \"Cobb 500 femelles\"\n‚Ä¢ \"Hubbard troupeau mixte\"",
            "en": "\n\n**Example responses:**\n‚Ä¢ \"Ross 308 males\"\n‚Ä¢ \"Cobb 500 females\"\n‚Ä¢ \"Hubbard mixed flock\"",
            "es": "\n\n**Ejemplos de respuestas:**\n‚Ä¢ \"Ross 308 machos\"\n‚Ä¢ \"Cobb 500 hembras\"\n‚Ä¢ \"Hubbard lote mixto\""
        }
        
        response_text += examples.get(language, examples["fr"])
        
        return EnhancedExpertResponse(
            question=question,
            response=response_text,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=50,  # Clarification rapide
            mode="smart_performance_clarification",
            user=None,
            logged=True,
            validation_passed=True,
            clarification_result={
                "clarification_requested": True,
                "clarification_type": "performance_breed_sex",
                "missing_information": missing_info,
                "age_detected": age,
                "confidence": clarification_info.get("confidence", 0.9)
            },
            processing_steps=["smart_clarification_triggered"],
            ai_enhancements_used=["performance_question_detection", "targeted_clarification"]
        )
    
    def _generate_follow_up_clarification(
        self, question: str, validation: Dict, language: str, conversation_id: str
    ) -> EnhancedExpertResponse:
        """
        ‚úÖ NOUVELLE FONCTION : G√©n√®re une clarification de suivi si premi√®re r√©ponse incompl√®te
        """
        
        still_missing = validation["still_missing"]
        
        messages = {
            "fr": {
                "breed": "Il me manque encore la **race/souche**. Ross 308, Cobb 500, ou autre ?",
                "sex": "Il me manque encore le **sexe**. M√¢les, femelles, ou troupeau mixte ?",  
                "both": "Il me manque encore la **race et le sexe**. Exemple : \"Ross 308 m√¢les\""
            },
            "en": {
                "breed": "I still need the **breed/strain**. Ross 308, Cobb 500, or other?",
                "sex": "I still need the **sex**. Males, females, or mixed flock?",
                "both": "I still need the **breed and sex**. Example: \"Ross 308 males\""
            },
            "es": {
                "breed": "A√∫n necesito la **raza/cepa**. ¬øRoss 308, Cobb 500, u otra?",
                "sex": "A√∫n necesito el **sexo**. ¬øMachos, hembras, o lote mixto?",
                "both": "A√∫n necesito la **raza y sexo**. Ejemplo: \"Ross 308 machos\""
            }
        }
        
        lang_messages = messages.get(language, messages["fr"])
        
        if len(still_missing) >= 2:
            message = lang_messages["both"]
        elif "breed" in still_missing:
            message = lang_messages["breed"]
        else:
            message = lang_messages["sex"]
        
        return EnhancedExpertResponse(
            question=question,
            response=message,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=30,
            mode="follow_up_clarification",
            user=None,
            logged=True,
            validation_passed=True,
            clarification_result={
                "clarification_requested": True,
                "clarification_type": "follow_up_incomplete",
                "still_missing_information": still_missing,
                "confidence": validation["confidence"]
            },
            processing_steps=["follow_up_clarification_triggered"],
            ai_enhancements_used=["incomplete_clarification_handling"]
        )
    
    # === TRAITEMENT EXPERT AVEC RAG-FIRST + AM√âLIORATIONS ===
    
    async def _process_expert_response_enhanced(
        self, question_text: str, request_data: EnhancedQuestionRequest,
        request: Request, current_user: Optional[Dict], conversation_id: str,
        processing_steps: list, ai_enhancements_used: list,
        debug_info: Dict, performance_breakdown: Dict, vagueness_result = None
    ) -> Dict[str, Any]:
        """
        VERSION FINALE - RAG obligatoire avec toutes les am√©liorations int√©gr√©es
        ‚úÖ CORRIG√â: Param√®tre conversation_id supprim√© des appels RAG
        """
        
        # === 1. R√âCUP√âRER CONTEXTE CONVERSATIONNEL ===
        conversation_context_str = ""
        extracted_entities = {}
        
        if self.integrations.intelligent_memory_available:
            try:
                conversation_context_str = self.integrations.get_context_for_rag(conversation_id, max_chars=800)
                if conversation_context_str:
                    ai_enhancements_used.append("contextual_rag")
                    logger.info(f"üß† Contexte conversationnel r√©cup√©r√©: {conversation_context_str[:100]}...")
                
                # R√©cup√©rer les entit√©s extraites pour coh√©rence
                context_obj = self.integrations.get_conversation_context(conversation_id)
                if context_obj and hasattr(context_obj, 'consolidated_entities'):
                    extracted_entities = context_obj.consolidated_entities.to_dict()
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur r√©cup√©ration contexte: {e}")
        
        performance_breakdown["context_retrieved"] = int(time.time() * 1000)
        
        # === 2. AM√âLIORATION INTELLIGENTE DE LA QUESTION ===
        enriched_question, enhancement_info = self.rag_enhancer.enhance_question_for_rag(
            question=question_text,
            conversation_context=conversation_context_str,
            language=request_data.language
        )
        
        if enhancement_info["question_enriched"]:
            ai_enhancements_used.append("intelligent_question_enhancement")
            logger.info(f"‚ú® Question am√©lior√©e: {enriched_question[:150]}...")
        
        if enhancement_info["pronoun_detected"]:
            ai_enhancements_used.append("contextual_pronoun_resolution")
            logger.info(f"üéØ Pronoms contextuels r√©solus: {enhancement_info['context_entities_used']}")
        
        processing_steps.append("intelligent_question_enhancement")
        performance_breakdown["question_enhanced"] = int(time.time() * 1000)
        
        # === 3. V√âRIFICATION RAG DISPONIBLE ===
        app = request.app
        process_rag = getattr(app.state, 'process_question_with_rag', None)
        
        if not process_rag:
            logger.error("‚ùå Syst√®me RAG indisponible - Erreur critique")
            
            # ‚úÖ NOUVEAU: Fallback enrichi
            fallback_details = self.enhancement_service.create_enhanced_fallback(
                failure_point="rag_unavailable",
                last_entities=extracted_entities,
                confidence=0.0,
                error=RuntimeError("RAG system not available"),
                context={"processing_steps": processing_steps}
            )
            
            raise HTTPException(
                status_code=503, 
                detail={
                    "error": "Service RAG indisponible",
                    "message": "Le syst√®me expert n√©cessite l'acc√®s √† la base documentaire",
                    "fallback_details": fallback_details.dict(),
                    "technical_details": "process_question_with_rag not available in app.state"
                }
            )
        
        # === 4. APPEL RAG AVEC QUESTION AM√âLIOR√âE (‚úÖ CORRIG√â) ===
        try:
            logger.info("üîç Appel RAG avec question intelligemment am√©lior√©e...")
            
            if request_data.debug_mode:
                debug_info["original_question"] = question_text
                debug_info["enriched_question"] = enriched_question
                debug_info["enhancement_info"] = enhancement_info
            
            # ‚úÖ CORRECTION: Essayer d'abord avec le param√®tre context
            try:
                result = await process_rag(
                    question=enriched_question,
                    user=current_user,
                    language=request_data.language,
                    speed_mode=request_data.speed_mode,
                    # conversation_id=conversation_id,  # ‚Üê SUPPRIM√â !
                    context=conversation_context_str
                )
                logger.info("‚úÖ RAG appel√© avec param√®tre context")
            except TypeError as te:
                logger.info(f"‚ÑπÔ∏è Param√®tre context non support√©: {te}")
                # ‚úÖ CORRECTION: Fallback sans conversation_id ni context
                result = await process_rag(
                    question=enriched_question,
                    user=current_user,
                    language=request_data.language,
                    speed_mode=request_data.speed_mode
                    # conversation_id=conversation_id  # ‚Üê SUPPRIM√â AUSSI !
                )
                logger.info("‚úÖ RAG appel√© avec param√®tres basiques")
            
            performance_breakdown["rag_complete"] = int(time.time() * 1000)
            
            # === 5. TRAITEMENT R√âSULTAT RAG AVEC AM√âLIORATIONS ===
            answer = str(result.get("response", ""))
            rag_score = result.get("score", 0.0)
            original_mode = result.get("mode", "rag_processing")
            
            # ‚úÖ NOUVEAU: Document relevance d√©taill√©
            document_relevance = None
            if request_data.detailed_rag_scoring:
                document_relevance = self.enhancement_service.create_detailed_document_relevance(
                    rag_result=result,
                    question=enriched_question,
                    context=conversation_context_str
                )
                ai_enhancements_used.append("detailed_rag_scoring")
            
            # ‚úÖ NOUVEAU: V√©rification de coh√©rence contextuelle
            context_coherence = None
            if request_data.require_coherence_check and extracted_entities:
                context_coherence = self.enhancement_service.check_context_coherence(
                    rag_response=answer,
                    extracted_entities=extracted_entities,
                    rag_context=result,
                    original_question=question_text
                )
                ai_enhancements_used.append("context_coherence_check")
                
                if context_coherence.coherence_score < 0.5:
                    logger.warning(f"‚ö†Ô∏è Coh√©rence faible: {context_coherence.coherence_score}")
                    ai_enhancements_used.append("coherence_warning")
            
            performance_breakdown["enhancements_complete"] = int(time.time() * 1000)
            
            # ‚úÖ NOUVEAU: M√©triques de qualit√©
            quality_metrics = None
            if request_data.enable_quality_metrics and context_coherence and vagueness_result:
                quality_metrics = self.enhancement_service.calculate_quality_metrics(
                    question=question_text,
                    response=answer,
                    rag_score=rag_score,
                    coherence_result=context_coherence,
                    vagueness_result=vagueness_result
                )
                ai_enhancements_used.append("quality_metrics")
            
            # === 6. VALIDATION QUALIT√â R√âPONSE ===
            quality_check = self._validate_rag_response_quality(
                answer, enriched_question, enhancement_info
            )
            
            if not quality_check["valid"]:
                logger.warning(f"‚ö†Ô∏è Qualit√© RAG insuffisante: {quality_check['reason']}")
                ai_enhancements_used.append("quality_validation_failed")
            
            logger.info(f"‚úÖ RAG r√©ponse re√ßue: {len(answer)} caract√®res, score: {rag_score}")
            
            # Mode enrichi
            mode = f"enhanced_contextual_{original_mode}"
            
            processing_steps.append("mandatory_rag_with_enhancements")
            
            return {
                "answer": answer,
                "rag_used": True,
                "rag_score": rag_score,
                "mode": mode,
                "context_used": bool(conversation_context_str),
                "question_enriched": enhancement_info["question_enriched"],
                "enhancement_info": enhancement_info,
                "quality_check": quality_check,
                
                # ‚úÖ NOUVELLES DONN√âES AM√âLIOR√âES
                "document_relevance": document_relevance,
                "context_coherence": context_coherence,
                "quality_metrics": quality_metrics,
                "extracted_entities": extracted_entities
            }
            
        except Exception as rag_error:
            logger.error(f"‚ùå Erreur critique RAG: {rag_error}")
            processing_steps.append("rag_error")
            
            # ‚úÖ NOUVEAU: Fallback enrichi avec diagnostics
            fallback_details = self.enhancement_service.create_enhanced_fallback(
                failure_point="rag_execution",
                last_entities=extracted_entities,
                confidence=0.2,
                error=rag_error,
                context={
                    "processing_steps": processing_steps,
                    "enriched_question": enriched_question,
                    "original_question": question_text
                }
            )
            
            error_details = {
                "error": "Erreur RAG",
                "message": "Impossible d'interroger la base documentaire",
                "fallback_details": fallback_details.dict(),
                "question_original": question_text,
                "question_enriched": enriched_question,
                "context_available": bool(conversation_context_str)
            }
            
            raise HTTPException(status_code=503, detail=error_details)
    
    # === FONCTIONS DE SUPPORT ===
    
    def _create_vagueness_response(
        self, vagueness_result, question_text: str, conversation_id: str,
        language: str, start_time: float, processing_steps: list, ai_enhancements_used: list
    ) -> EnhancedExpertResponse:
        """Cr√©e une r√©ponse sp√©cialis√©e pour questions floues"""
        
        clarification_messages = {
            "fr": f"Votre question semble manquer de pr√©cision. {vagueness_result.suggested_clarification or 'Pouvez-vous √™tre plus sp√©cifique ?'}",
            "en": f"Your question seems to lack precision. {vagueness_result.suggested_clarification or 'Could you be more specific?'}",
            "es": f"Su pregunta parece carecer de precisi√≥n. {vagueness_result.suggested_clarification or '¬øPodr√≠a ser m√°s espec√≠fico?'}"
        }
        
        response_message = clarification_messages.get(language, clarification_messages["fr"])
        
        # Ajouter des exemples de questions
        if vagueness_result.question_clarity in ["very_unclear", "unclear"]:
            examples = {
                "fr": "\n\nExemples de questions pr√©cises:\n‚Ä¢ Quel est le poids normal d'un Ross 308 de 21 jours?\n‚Ä¢ Comment traiter la mortalit√© √©lev√©e chez des poulets de 3 semaines?\n‚Ä¢ Quelle temp√©rature maintenir pour des poussins de 7 jours?",
                "en": "\n\nExamples of precise questions:\n‚Ä¢ What is the normal weight of a 21-day Ross 308?\n‚Ä¢ How to treat high mortality in 3-week-old chickens?\n‚Ä¢ What temperature to maintain for 7-day chicks?",
                "es": "\n\nEjemplos de preguntas precisas:\n‚Ä¢ ¬øCu√°l es el peso normal de un Ross 308 de 21 d√≠as?\n‚Ä¢ ¬øC√≥mo tratar la alta mortalidad en pollos de 3 semanas?\n‚Ä¢ ¬øQu√© temperatura mantener para pollitos de 7 d√≠as?"
            }
            response_message += examples.get(language, examples["fr"])
        
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return EnhancedExpertResponse(
            question=question_text,
            response=response_message,
            conversation_id=conversation_id,
            rag_used=False,
            rag_score=None,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode="vagueness_clarification",
            user=None,
            logged=True,
            validation_passed=True,
            vagueness_detection=vagueness_result,
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used
        )
    
    def _build_final_enhanced_response(
        self, question_text: str, answer: str, conversation_id: str,
        user_email: Optional[str], language: str, response_time_ms: int,
        expert_result: Dict, validation_result: ValidationResult,
        conversation_context: Any, processing_steps: list,
        ai_enhancements_used: list, request_data: EnhancedQuestionRequest,
        debug_info: Dict, performance_breakdown: Dict
    ) -> EnhancedExpertResponse:
        """Construit la r√©ponse finale avec toutes les am√©liorations"""
        
        # M√©triques finales
        extracted_entities = expert_result.get("extracted_entities")
        confidence_overall = None
        conversation_state = None
        
        if conversation_context and hasattr(conversation_context, 'consolidated_entities'):
            if not extracted_entities:
                extracted_entities = conversation_context.consolidated_entities.to_dict()
            confidence_overall = conversation_context.consolidated_entities.confidence_overall
            conversation_state = conversation_context.conversation_urgency
        
        # Informations de debug si activ√©es
        final_debug_info = None
        final_performance = None
        
        if request_data.debug_mode:
            final_debug_info = {
                **debug_info,
                "total_processing_time_ms": response_time_ms,
                "ai_enhancements_count": len(ai_enhancements_used),
                "processing_steps_count": len(processing_steps)
            }
            
            final_performance = performance_breakdown
            
        return EnhancedExpertResponse(
            # Champs existants
            question=str(question_text),
            response=str(answer),
            conversation_id=conversation_id,
            rag_used=expert_result["rag_used"],
            rag_score=expert_result["rag_score"],
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode=expert_result["mode"],
            user=user_email,
            logged=True,
            validation_passed=True,
            validation_confidence=validation_result.confidence,
            reprocessed_after_clarification=request_data.is_clarification_response,
            conversation_state=conversation_state,
            extracted_entities=extracted_entities,
            confidence_overall=confidence_overall,
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used,
            
            # ‚úÖ NOUVELLES FONCTIONNALIT√âS
            document_relevance=expert_result.get("document_relevance"),
            context_coherence=expert_result.get("context_coherence"),
            vagueness_detection=None,  # D√©j√† trait√© si n√©cessaire
            fallback_details=None,  # Pas d'erreur si on arrive ici
            response_format_applied=request_data.expected_response_format.value,
            quality_metrics=expert_result.get("quality_metrics"),
            debug_info=final_debug_info,
            performance_breakdown=final_performance
        )
    
    # === M√âTHODES UTILITAIRES (identiques aux versions pr√©c√©dentes) ===
    
    def _extract_user_id(self, current_user: Optional[Dict], request_data: EnhancedQuestionRequest, request: Request) -> str:
        if current_user:
            return current_user.get("user_id") or request_data.user_id or "authenticated_user"
        return request_data.user_id or get_user_id_from_request(request)
    
    def _get_or_create_conversation_id(self, request_data: EnhancedQuestionRequest) -> str:
        if request_data.conversation_id and request_data.conversation_id.strip():
            conversation_id = request_data.conversation_id.strip()
            logger.info(f"üîÑ [Expert Service] CONTINUATION: {conversation_id}")
            return conversation_id
        else:
            conversation_id = str(uuid.uuid4())
            logger.info(f"üÜï [Expert Service] NOUVELLE: {conversation_id}")
            return conversation_id
    
    def _validate_rag_response_quality(
        self, answer: str, enriched_question: str, enhancement_info: Dict
    ) -> Dict[str, any]:
        """Valide la qualit√© de la r√©ponse RAG"""
        
        if not answer or len(answer.strip()) < 20:
            return {
                "valid": False,
                "reason": "R√©ponse trop courte",
                "answer_length": len(answer) if answer else 0
            }
        
        negative_responses = [
            "je ne sais pas", "i don't know", "no s√©",
            "pas d'information", "no information", "sin informaci√≥n"
        ]
        
        answer_lower = answer.lower()
        for negative in negative_responses:
            if negative in answer_lower:
                return {
                    "valid": False,
                    "reason": f"R√©ponse n√©gative d√©tect√©e: {negative}",
                    "answer_length": len(answer)
                }
        
        if any(word in enriched_question.lower() for word in ["poids", "weight", "peso"]):
            if not re.search(r'\d+', answer):
                return {
                    "valid": False,
                    "reason": "Question num√©rique mais pas de chiffres dans la r√©ponse",
                    "answer_length": len(answer)
                }
        
        return {
            "valid": True,
            "reason": "R√©ponse valide",
            "answer_length": len(answer)
        }
    
    # Autres m√©thodes h√©rit√©es des versions pr√©c√©dentes...
    async def _validate_agricultural_question(self, question: str, language: str, user_id: str, request_ip: str, conversation_id: str) -> ValidationResult:
        if not self.integrations.agricultural_validator_available:
            return ValidationResult(is_valid=False, rejection_message="Service temporairement indisponible")
        
        try:
            enriched_question = question
            if self.integrations.intelligent_memory_available:
                try:
                    rag_context = self.integrations.get_context_for_rag(conversation_id)
                    if rag_context:
                        enriched_question = f"{question}\n\nContexte: {rag_context}"
                except Exception:
                    pass
            
            validation_result = self.integrations.validate_agricultural_question(
                question=enriched_question, language=language, user_id=user_id, request_ip=request_ip
            )
            
            return ValidationResult(
                is_valid=validation_result.is_valid,
                rejection_message=validation_result.reason or "Question hors domaine agricole",
                confidence=validation_result.confidence
            )
            
        except Exception as e:
            logger.error(f"‚ùå [Expert Service] Erreur validateur: {e}")
            return ValidationResult(is_valid=False, rejection_message="Erreur de validation")
    
    def _create_rejection_response(self, question_text, validation_result, conversation_id, user_email, language, start_time, processing_steps, ai_enhancements_used, vagueness_result=None):
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return EnhancedExpertResponse(
            question=str(question_text),
            response=str(validation_result.rejection_message),
            conversation_id=conversation_id,
            rag_used=False,
            timestamp=datetime.now().isoformat(),
            language=language,
            response_time_ms=response_time_ms,
            mode="enhanced_agricultural_validation_rejected",
            user=user_email,
            logged=True,
            validation_passed=False,
            validation_confidence=validation_result.confidence,
            processing_steps=processing_steps,
            ai_enhancements_used=ai_enhancements_used,
            vagueness_detection=vagueness_result
        )
    
    # Autres m√©thodes (process_feedback, get_suggested_topics) identiques...
    async def process_feedback(self, feedback_data: FeedbackRequest) -> Dict[str, Any]:
        feedback_updated = False
        
        if feedback_data.conversation_id and self.integrations.logging_available:
            try:
                rating_numeric = {"positive": 1, "negative": -1, "neutral": 0}.get(feedback_data.rating, 0)
                feedback_updated = await self.integrations.update_feedback(feedback_data.conversation_id, rating_numeric)
            except Exception as e:
                logger.error(f"‚ùå [Expert Service] Erreur feedback: {e}")
        
        return {
            "success": True,
            "message": "Feedback enregistr√© avec succ√®s (Enhanced)",
            "rating": feedback_data.rating,
            "comment": feedback_data.comment,
            "conversation_id": feedback_data.conversation_id,
            "feedback_updated_in_db": feedback_updated,
            "enhanced_features_used": True,
            "timestamp": datetime.now().isoformat()
        }
    
    async def get_suggested_topics(self, language: str) -> Dict[str, Any]:
        lang = language.lower() if language else "fr"
        if lang not in ["fr", "en", "es"]:
            lang = "fr"
        
        topics_by_language = get_enhanced_topics_by_language()
        topics = topics_by_language.get(lang, topics_by_language["fr"])
        
        return {
            "topics": topics,
            "language": lang,
            "count": len(topics),
            "enhanced_features": {
                "vagueness_detection_available": True,
                "context_coherence_available": True,
                "detailed_rag_scoring_available": True,
                "quality_metrics_available": True,
                "smart_clarification_available": True
            },
            "system_status": {
                "validation_enabled": self.integrations.is_agricultural_validation_enabled(),
                "enhanced_clarification_enabled": self.integrations.is_enhanced_clarification_enabled(),
                "intelligent_memory_enabled": self.integrations.intelligent_memory_available,
                "api_enhancements_enabled": True
            }
        }

# =============================================================================
# CONFIGURATION FINALE
# =============================================================================

logger.info("‚úÖ [Expert Service] Services m√©tier finalis√©s avec TOUTES les am√©liorations + CLARIFICATION INTELLIGENTE")
logger.info("üöÄ [Expert Service] Fonctionnalit√©s disponibles:")
logger.info("   - üéØ D√©tection de questions floues avec clarification imm√©diate")
logger.info("   - üîç V√©rification de coh√©rence contextuelle avanc√©e")
logger.info("   - üìä Scoring RAG d√©taill√© avec m√©tadonn√©es compl√®tes")
logger.info("   - üîß Fallback enrichi avec diagnostics d'erreur")
logger.info("   - üìà M√©triques de qualit√© pr√©dictives")
logger.info("   - üêõ Mode debug complet pour d√©veloppeurs")
logger.info("   - ‚ö° Breakdown de performance d√©taill√©")
logger.info("   - üß† Syst√®me de clarification intelligent race/sexe")
logger.info("   - üé™ Traitement des r√©ponses de clarification")
logger.info("   - üîÑ Suivi automatique des clarifications incompl√®tes")