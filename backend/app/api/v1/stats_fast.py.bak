# app/api/v1/stats_fast.py
# -*- coding: utf-8 -*-
"""
üöÄ ENDPOINTS ULTRA-RAPIDES - VERSION MEMORY-SAFE CORRIG√âE
üõ°Ô∏è SAFE: Imports conditionnels + Gestion d'erreurs robuste 
‚ö° MEMORY-OPTIMIZED: Suppression des parties probl√©matiques de m√©moire
üîß WORKING: Invitations avec vraies donn√©es
üìã FIXED: Questions endpoint avec structure correcte (SAFE - NO HEAVY IMPORTS)
üõ°Ô∏è MEMORY-SAFE: Lazy loading, cache local, limites strictes
üîß CORRECTIFS APPLIQU√âS:
   - Import logging forc√© (LOG=True garanti)
   - TTL cache optimis√©s
   - Gestion d'erreur robuste
   - Conservation int√©grale du code original
"""

import logging
import os
import gc
import sys
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
from fastapi import APIRouter, Depends, Query, HTTPException

# üõ°Ô∏è CONFIGURATION MEMORY-SAFE POUR ENDPOINTS (CONSERV√âE)
FAST_CONFIG = {
    "ENABLE_HEAVY_IMPORTS": os.getenv("ENABLE_HEAVY_IMPORTS", "true").lower() == "true",  # ‚Üê CORRIG√â: true par d√©faut
    "MAX_RESPONSE_SIZE_KB": 50,             # Max 50KB par r√©ponse
    "CACHE_LOCAL_RESPONSES": True,          # Cache local temporaire
    "ENABLE_IMPORT_MONITORING": True,       # Monitor imports lourds
    "LAZY_LOAD_MODULES": True,              # Lazy loading des modules
    "MAX_FALLBACK_RETRIES": 2,              # Limite tentatives fallback
    "REDUCE_LOG_VERBOSITY": True,           # Logs moins verbeux
    "FORCE_GC_AFTER_ENDPOINT": False       # GC apr√®s chaque endpoint (debug)
}

logger = logging.getLogger(__name__)

# üõ°Ô∏è CACHE LOCAL TEMPORAIRE (en m√©moire, limit√©) - CONSERV√â INT√âGRALEMENT
_local_cache = {}
_cache_timestamps = {}
_cache_max_entries = 20

def get_memory_usage_mb():
    """Estime l'usage m√©moire du processus actuel - CONSERV√â"""
    try:
        import psutil
        return psutil.Process().memory_info().rss / 1024 / 1024
    except ImportError:
        return 0

def should_use_local_cache(key: str) -> bool:
    """D√©termine si on peut utiliser le cache local - CONSERV√â"""
    if not FAST_CONFIG["CACHE_LOCAL_RESPONSES"]:
        return False
    
    # Limiter le nombre d'entr√©es en cache local
    if len(_local_cache) >= _cache_max_entries:
        # Supprimer la plus ancienne entr√©e
        oldest_key = min(_cache_timestamps.keys(), key=lambda k: _cache_timestamps[k])
        _local_cache.pop(oldest_key, None)
        _cache_timestamps.pop(oldest_key, None)
    
    return True

def set_local_cache(key: str, data: Any, ttl_minutes: int = 5):
    """Stocke dans le cache local avec TTL - CONSERV√â"""
    if should_use_local_cache(key):
        _local_cache[key] = data
        _cache_timestamps[key] = datetime.now()
        
        # Nettoyer les entr√©es expir√©es
        cleanup_expired_local_cache()

def get_local_cache(key: str) -> Optional[Any]:
    """R√©cup√®re du cache local si valide - CONSERV√â"""
    if key not in _local_cache or key not in _cache_timestamps:
        return None
    
    # V√©rifier TTL (5 minutes par d√©faut)
    if datetime.now() - _cache_timestamps[key] > timedelta(minutes=5):
        _local_cache.pop(key, None)
        _cache_timestamps.pop(key, None)
        return None
    
    return _local_cache[key]

def cleanup_expired_local_cache():
    """Nettoie le cache local expir√© - CONSERV√â"""
    now = datetime.now()
    expired_keys = [
        key for key, timestamp in _cache_timestamps.items()
        if now - timestamp > timedelta(minutes=5)
    ]
    
    for key in expired_keys:
        _local_cache.pop(key, None)
        _cache_timestamps.pop(key, None)

# ==================== IMPORTS CONDITIONNELS MEMORY-SAFE - CORRIG√âS ====================

# üõ°Ô∏è Variables de disponibilit√© des modules
AUTH_AVAILABLE = False
STATS_CACHE_AVAILABLE = False
PERMISSIONS_AVAILABLE = False
LOGGING_AVAILABLE = False

# Cache des modules import√©s pour √©viter les re-imports
_imported_modules = {}

def safe_import_module(module_name: str, lazy: bool = True):
    """üõ°Ô∏è Import s√©curis√© avec monitoring m√©moire - CORRIG√â"""
    if module_name in _imported_modules:
        return _imported_modules[module_name]
    
    # üîß CORRECTIF: Logique simplifi√©e pour √©viter les blocages
    if lazy and not FAST_CONFIG["ENABLE_HEAVY_IMPORTS"]:
        logger.info(f"üõ°Ô∏è Import {module_name} skipp√© (ENABLE_HEAVY_IMPORTS=false)")
        return None
    
    try:
        if FAST_CONFIG["ENABLE_IMPORT_MONITORING"]:
            memory_before = get_memory_usage_mb()
        
        # Import dynamique - CONSERV√â avec corrections
        if module_name == "auth":
            from app.api.v1.auth import get_current_user
            module = get_current_user
        elif module_name == "permissions":
            from app.api.v1.logging import has_permission, Permission
            module = {"has_permission": has_permission, "Permission": Permission}
        elif module_name == "stats_cache":
            from app.api.v1.stats_cache import get_stats_cache
            module = get_stats_cache
        elif module_name == "logging":
            # üîß CORRECTIF MAJEUR: Import direct depuis logging_endpoints
            try:
                from app.api.v1.logging_endpoints import questions_final
                module = {"questions_final": questions_final}
                logger.info("‚úÖ Import questions_final depuis logging_endpoints r√©ussi")
            except ImportError:
                # Fallback vers le module principal
                try:
                    from app.api.v1.logging import questions_final
                    module = {"questions_final": questions_final}
                    logger.info("‚úÖ Import questions_final depuis logging r√©ussi (fallback)")
                except ImportError as e:
                    logger.warning(f"‚ö†Ô∏è Import questions_final √©chou√© compl√®tement: {e}")
                    # Fallback vide pour √©viter les crashes
                    module = {"questions_final": None}
        else:
            return None
        
        _imported_modules[module_name] = module
        
        if FAST_CONFIG["ENABLE_IMPORT_MONITORING"]:
            memory_after = get_memory_usage_mb()
            memory_delta = memory_after - memory_before
            if memory_delta > 5:  # Alert si > 5MB
                logger.warning(f"‚ö†Ô∏è Import {module_name} co√ªteux: +{memory_delta:.1f}MB RAM")
            else:
                logger.debug(f"‚úÖ Import {module_name}: +{memory_delta:.1f}MB RAM")
        
        return module
        
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Import {module_name} √©chou√©: {e}")
        _imported_modules[module_name] = None
        return None
    except Exception as e:
        logger.error(f"‚ùå Erreur import {module_name}: {e}")
        _imported_modules[module_name] = None
        return None

# üõ°Ô∏è INITIALISATION LAZY DES IMPORTS - CORRIG√âE
def initialize_imports():
    """Initialise les imports selon la configuration - CORRIG√â"""
    global AUTH_AVAILABLE, STATS_CACHE_AVAILABLE, PERMISSIONS_AVAILABLE, LOGGING_AVAILABLE
    
    logger.info("üîÑ Initialisation imports MEMORY-SAFE...")
    
    # Auth (prioritaire) - CONSERV√â
    auth_module = safe_import_module("auth", lazy=False)  # Toujours essayer auth
    AUTH_AVAILABLE = auth_module is not None
    
    # Stats cache (conditionnel) - CONSERV√â
    if os.getenv("DISABLE_STATS_CACHE") != "true":
        stats_cache_module = safe_import_module("stats_cache")
        STATS_CACHE_AVAILABLE = stats_cache_module is not None
    else:
        logger.info("üõ°Ô∏è Stats cache d√©sactiv√© via DISABLE_STATS_CACHE")
        STATS_CACHE_AVAILABLE = False
    
    # Permissions (conditionnel) - CONSERV√â
    permissions_module = safe_import_module("permissions")
    PERMISSIONS_AVAILABLE = permissions_module is not None
    
    # üîß CORRECTIF MAJEUR: Logging toujours activ√© si ENABLE_HEAVY_IMPORTS=true
    if FAST_CONFIG["ENABLE_HEAVY_IMPORTS"]:
        logging_module = safe_import_module("logging", lazy=False)  # ‚Üê FORC√â: lazy=False
        LOGGING_AVAILABLE = logging_module is not None and logging_module.get("questions_final") is not None
        
        if LOGGING_AVAILABLE:
            logger.info("‚úÖ Module logging correctement import√© avec questions_final")
        else:
            logger.warning("‚ö†Ô∏è Module logging import√© mais questions_final manquante")
    else:
        LOGGING_AVAILABLE = False
        logger.info("üõ°Ô∏è Logging import skipp√© (ENABLE_HEAVY_IMPORTS=false)")
    
    # üîß CORRECTIF: Log de confirmation garanti
    logger.info(f"‚úÖ Imports initialis√©s: AUTH={AUTH_AVAILABLE}, CACHE={STATS_CACHE_AVAILABLE}, PERMS={PERMISSIONS_AVAILABLE}, LOG={LOGGING_AVAILABLE}")

# Initialiser au chargement du module
initialize_imports()

router = APIRouter(tags=["statistics-fast"])

# ==================== UTILITAIRES OPTIMIS√âS - CONSERV√âS INT√âGRALEMENT ====================

def calculate_performance_gain(dashboard_snapshot: Dict[str, Any]) -> float:
    """üõ°Ô∏è CONSERV√â: Calcul intelligent du gain de performance (optimis√©)"""
    try:
        # Version all√©g√©e du calcul original
        cache_hit_rate = min(dashboard_snapshot.get("cache_hit_rate", 85.2), 100)
        avg_response_time = float(dashboard_snapshot.get("avg_response_time", 0.250))
        error_rate = float(dashboard_snapshot.get("error_rate", 0))
        
        # Calcul simplifi√© pour √©conomiser CPU
        cache_gain = cache_hit_rate * 0.6
        time_gain = max(0, (1.0 - avg_response_time) * 20) if avg_response_time > 0 else 20
        reliability_gain = max(0, 15 - (error_rate * 3))
        
        total_gain = min(cache_gain + time_gain + reliability_gain, 100.0)
        return round(total_gain, 1)
        
    except Exception as e:
        if not FAST_CONFIG["REDUCE_LOG_VERBOSITY"]:
            logger.warning(f"Erreur calcul performance_gain: {e}")
        return 75.0

def calculate_cache_age_minutes(generated_at: str = None) -> int:
    """üõ°Ô∏è CONSERV√â: Calcule l'√¢ge du cache en minutes (optimis√©)"""
    if not generated_at:
        return 0
    
    try:
        # Version simplifi√©e pour √©conomiser CPU
        cache_time = datetime.fromisoformat(generated_at.replace('Z', ''))
        age_delta = datetime.now() - cache_time
        return max(0, int(age_delta.total_seconds() / 60))
    except Exception as e:
        if not FAST_CONFIG["REDUCE_LOG_VERBOSITY"]:
            logger.warning(f"Erreur calcul cache age: {e}")
        return 0

def safe_get_cache():
    """üõ°Ô∏è CONSERV√â: R√©cup√©ration s√©curis√©e du cache (optimis√©)"""
    if not STATS_CACHE_AVAILABLE:
        return None
    
    try:
        # Utiliser le module d√©j√† import√©
        get_stats_cache = _imported_modules.get("stats_cache")
        if get_stats_cache:
            return get_stats_cache()
        return None
    except Exception as e:
        logger.error(f"‚ùå Erreur acc√®s cache: {e}")
        return None

def safe_has_permission(user: Dict[str, Any], permission_name: str) -> bool:
    """üõ°Ô∏è CONSERV√â: V√©rification s√©curis√©e des permissions (optimis√©)"""
    if not PERMISSIONS_AVAILABLE or not user:
        return user.get("user_type") == "super_admin" if user else False
    
    try:
        permissions_module = _imported_modules.get("permissions")
        if permissions_module and "has_permission" in permissions_module:
            permission_enum = getattr(permissions_module["Permission"], permission_name, None)
            if permission_enum:
                return permissions_module["has_permission"](user, permission_enum)
        
        # Fallback
        return user.get("user_type") == "super_admin"
    except Exception as e:
        if not FAST_CONFIG["REDUCE_LOG_VERBOSITY"]:
            logger.warning(f"‚ö†Ô∏è Erreur v√©rification permission: {e}")
        return user.get("user_type") == "super_admin"

def get_current_user_safe():
    """üõ°Ô∏è R√©cup√®re get_current_user de mani√®re s√©curis√©e - CONSERV√â"""
    if not AUTH_AVAILABLE:
        return None
    return _imported_modules.get("auth")

def limit_response_size(data: Dict[str, Any]) -> Dict[str, Any]:
    """üõ°Ô∏è NOUVEAU: Limite la taille des r√©ponses pour √©conomiser m√©moire - CONSERV√â"""
    try:
        import json
        data_str = json.dumps(data, default=str, separators=(',', ':'))
        size_kb = len(data_str.encode('utf-8')) / 1024
        
        if size_kb > FAST_CONFIG["MAX_RESPONSE_SIZE_KB"]:
            logger.warning(f"‚ö†Ô∏è R√©ponse trop large ({size_kb:.1f}KB), truncation")
            
            # Tronquer les listes longues
            for key, value in data.items():
                if isinstance(value, list) and len(value) > 10:
                    data[key] = value[:10] + [f"...[{len(value)-10} more items truncated]"]
                elif isinstance(value, dict):
                    for subkey, subvalue in value.items():
                        if isinstance(subvalue, list) and len(subvalue) > 10:
                            value[subkey] = subvalue[:10] + [f"...[{len(subvalue)-10} more items]"]
            
            # Ajouter metadata sur truncation
            data["_truncated"] = {
                "original_size_kb": round(size_kb, 1),
                "max_allowed_kb": FAST_CONFIG["MAX_RESPONSE_SIZE_KB"],
                "reason": "memory_optimization"
            }
        
        return data
        
    except Exception as e:
        logger.error(f"‚ùå Erreur limitation taille r√©ponse: {e}")
        return data

# ==================== ENDPOINTS PRINCIPAUX - CONSERV√âS INT√âGRALEMENT ====================

@router.get("/dashboard")
async def get_dashboard_fast(
    current_user: dict = Depends(get_current_user_safe()) if AUTH_AVAILABLE else None
) -> Dict[str, Any]:
    """üõ°Ô∏è CONSERV√â + OPTIMIS√â: DASHBOARD ULTRA-RAPIDE MEMORY-SAFE"""
    
    # V√©rifier cache local d'abord
    cache_key = f"dashboard:{current_user.get('email') if current_user else 'anonymous'}"
    local_cached = get_local_cache(cache_key)
    if local_cached:
        logger.info("üì¶ Dashboard cache local HIT")
        return local_cached
    
    if current_user and not safe_has_permission(current_user, "ADMIN_DASHBOARD"):
        raise HTTPException(
            status_code=403, 
            detail=f"Admin dashboard access required. Your role: {current_user.get('user_type', 'user') if current_user else 'unknown'}"
        )
    
    try:
        cache = safe_get_cache()
        dashboard_snapshot = None
        cache_available = False
        
        if cache:
            try:
                dashboard_snapshot = cache.get_dashboard_snapshot()
                cache_available = True
                if not FAST_CONFIG["REDUCE_LOG_VERBOSITY"]:
                    logger.info("üìä Dashboard snapshot r√©cup√©r√© du cache")
            except Exception as cache_error:
                logger.warning(f"‚ö†Ô∏è Erreur r√©cup√©ration snapshot: {cache_error}")
            
            if not dashboard_snapshot:
                try:
                    cached_data = cache.get_cache("dashboard:main")
                    if cached_data:
                        dashboard_snapshot = cached_data["data"]
                        cache_available = True
                except Exception as fallback_error:
                    logger.warning(f"‚ö†Ô∏è Erreur cache fallback: {fallback_error}")
        
        if not dashboard_snapshot:
            cache_available = False
            dashboard_snapshot = {
                "total_users": 0,
                "total_questions": 0,
                "questions_today": 0,
                "questions_this_month": 0,
                "total_revenue": 0.0,
                "avg_response_time": 0.0,
                "source_distribution": {"safe_mode": 1},
                "system_health": "safe_mode",
                "error_rate": 0.0,
                "top_users": [],
                "note": "Mode s√©curis√© - cache non disponible"
            }
        
        performance_gain = calculate_performance_gain(dashboard_snapshot)
        cache_age_minutes = calculate_cache_age_minutes(dashboard_snapshot.get("generated_at"))
        
        # Structure de r√©ponse optimis√©e (version all√©g√©e) - CONSERV√âE
        formatted_response = {
            "cache_info": {
                "is_available": cache_available,
                "last_update": dashboard_snapshot.get("generated_at", datetime.now().isoformat()),
                "cache_age_minutes": cache_age_minutes,
                "performance_gain": f"{performance_gain}%",
                "next_update": (datetime.now() + timedelta(hours=1)).isoformat(),
                "stats_cache_available": STATS_CACHE_AVAILABLE,
                "local_cache_used": False
            },
            
            "systemStats": {
                "system_health": {
                    "uptime_hours": min(dashboard_snapshot.get("total_questions", 0) * 0.01, 168),  # Estimation
                    "total_requests": min(dashboard_snapshot.get("total_questions", 0), 99999),  # Limite
                    "error_rate": float(dashboard_snapshot.get("error_rate", 0)),
                    "rag_status": {"global": True, "broiler": True, "layer": True}
                },
                "features_enabled": {
                    "analytics": STATS_CACHE_AVAILABLE,
                    "billing": True,
                    "authentication": AUTH_AVAILABLE,
                    "openai_fallback": True,
                    "memory_optimization": True
                }
            },
            
            "usageStats": {
                "unique_users": min(dashboard_snapshot.get("total_users", 0), 50000),
                "total_questions": min(dashboard_snapshot.get("total_questions", 0), 100000),
                "questions_today": dashboard_snapshot.get("questions_today", 0),
                "questions_this_month": dashboard_snapshot.get("questions_this_month", 0),
                "source_distribution": dashboard_snapshot.get("source_distribution", {})
            },
            
            "billingStats": {
                "plans": dashboard_snapshot.get("plan_distribution", {"free": dashboard_snapshot.get("total_users", 0)}),
                "total_revenue": round(float(dashboard_snapshot.get("total_revenue", 0)), 2),
                "top_users": dashboard_snapshot.get("top_users", [])[:5]  # Limit√© √† 5
            },
            
            "performanceStats": {
                "avg_response_time": round(float(dashboard_snapshot.get("avg_response_time", 0)), 3),
                "median_response_time": round(float(dashboard_snapshot.get("median_response_time", 0)), 3),
                "openai_costs": round(float(dashboard_snapshot.get("openai_costs", 0)), 2),
                "cache_hit_rate": 85.2,
                "performance_gain": performance_gain,
                "memory_optimized": True
            },
            
            "meta": {
                "cached": cache_available,
                "cache_age": dashboard_snapshot.get("generated_at", datetime.now().isoformat()),
                "response_time_ms": "< 50ms",
                "data_source": "statistics_cache_memory_safe" if cache_available else "fallback_safe",
                "memory_optimization": "enabled"
            }
        }
        
        # Limiter la taille de la r√©ponse
        formatted_response = limit_response_size(formatted_response)
        
        # Stocker en cache local
        set_local_cache(cache_key, formatted_response, ttl_minutes=3)
        
        if not FAST_CONFIG["REDUCE_LOG_VERBOSITY"]:
            logger.info(f"üìä Dashboard fast response SAFE: {current_user.get('email') if current_user else 'anonymous'}")
        
        # Force GC si configur√© (debug)
        if FAST_CONFIG["FORCE_GC_AFTER_ENDPOINT"]:
            gc.collect()
        
        return formatted_response
        
    except Exception as e:
        logger.error(f"‚ùå Erreur dashboard fast SAFE: {e}")
        return {
            "cache_info": {"is_available": False, "error": "Dashboard en mode s√©curis√©", "memory_safe": True},
            "systemStats": {"system_health": {"uptime_hours": 0, "total_requests": 0, "error_rate": 0.0}},
            "usageStats": {"unique_users": 0, "total_questions": 0, "questions_today": 0, "questions_this_month": 0, "source_distribution": {}},
            "billingStats": {"plans": {}, "total_revenue": 0.0, "top_users": []},
            "performanceStats": {"avg_response_time": 0.0, "median_response_time": 0.0, "openai_costs": 0.0, "cache_hit_rate": 0.0},
            "meta": {"fallback_activated": True, "error": str(e)[:100], "memory_safe": True}
        }

# ==================== TOUS LES AUTRES ENDPOINTS CONSERV√âS INT√âGRALEMENT ====================

@router.get("/performance")
async def get_performance_fast(
    hours: int = Query(24, ge=1, le=168),
    current_user: dict = Depends(get_current_user_safe()) if AUTH_AVAILABLE else None
) -> Dict[str, Any]:
    """üõ°Ô∏è CONSERV√â + OPTIMIS√â: Performance serveur ultra-rapide MEMORY-SAFE"""
    
    # Cache local
    cache_key = f"performance:{hours}:{current_user.get('email') if current_user else 'anon'}"
    local_cached = get_local_cache(cache_key)
    if local_cached:
        return local_cached
    
    if current_user and not safe_has_permission(current_user, "VIEW_SERVER_PERFORMANCE"):
        raise HTTPException(status_code=403, detail="Server performance access required")
    
    try:
        cache = safe_get_cache()
        performance_data = None
        
        if cache:
            try:
                performance_data = cache.get_cache("server:performance:24h")
            except Exception as cache_error:
                logger.warning(f"‚ö†Ô∏è Erreur cache performance: {cache_error}")
        
        if not performance_data:
            performance_data = {
                "data": {
                    "period_hours": hours,
                    "current_status": {
                        "overall_health": "safe_mode",
                        "avg_response_time_ms": 250,
                        "error_rate_percent": 0
                    },
                    "global_stats": {"total_requests": 0, "memory_optimized": True},
                    "note": "Cache performance non disponible - mode s√©curis√©"
                }
            }
        
        result = performance_data["data"]
        result["requested_by_role"] = current_user.get("user_type") if current_user else "anonymous"
        result["memory_optimization"] = "enabled"
        
        # Limiter la taille
        result = limit_response_size(result)
        
        # Cache local
        set_local_cache(cache_key, result, ttl_minutes=5)
        
        if not FAST_CONFIG["REDUCE_LOG_VERBOSITY"]:
            logger.info(f"‚ö° Performance fast response SAFE: {current_user.get('email') if current_user else 'anonymous'}")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erreur performance fast SAFE: {e}")
        return {
            "period_hours": hours,
            "current_status": {"overall_health": "error", "avg_response_time_ms": 0, "error_rate_percent": 0},
            "global_stats": {"memory_optimized": True},
            "error": "Performance endpoint en mode s√©curis√©",
            "memory_safe": True
        }

@router.get("/openai-costs/current")
async def get_openai_costs_fast(
    current_user: dict = Depends(get_current_user_safe()) if AUTH_AVAILABLE else None
) -> Dict[str, Any]:
    """üõ°Ô∏è CONSERV√â + OPTIMIS√â: Co√ªts OpenAI ultra-rapides MEMORY-SAFE"""
    
    # Cache local
    cache_key = f"openai_costs:{current_user.get('email') if current_user else 'anon'}"
    local_cached = get_local_cache(cache_key)
    if local_cached:
        return local_cached
    
    if current_user and not safe_has_permission(current_user, "VIEW_OPENAI_COSTS"):
        raise HTTPException(status_code=403, detail="View OpenAI costs permission required")
    
    try:
        cache = safe_get_cache()
        costs_data = None
        
        if cache:
            try:
                costs_data = cache.get_cache("openai:costs:current")
                if not costs_data:
                    costs_data = cache.get_cache("openai:costs:fallback")
            except Exception as cache_error:
                logger.warning(f"‚ö†Ô∏è Erreur cache OpenAI costs: {cache_error}")
        
        if not costs_data:
            costs_data = {
                "data": {
                    "total_cost": 6.30,
                    "total_tokens": 450000,
                    "api_calls": 250,
                    "models_usage": {"gpt-4": {"cost": 4.20}, "gpt-3.5-turbo": {"cost": 2.10}},
                    "note": "Cache co√ªts OpenAI non disponible - valeurs estim√©es",
                    "memory_safe": True
                }
            }
        
        result = costs_data["data"]
        result["user_role"] = current_user.get("user_type") if current_user else "anonymous"
        result["memory_optimization"] = "enabled"
        
        # Limiter la taille
        result = limit_response_size(result)
        
        # Cache local
        set_local_cache(cache_key, result, ttl_minutes=10)  # Plus long pour les co√ªts
        
        if not FAST_CONFIG["REDUCE_LOG_VERBOSITY"]:
            logger.info(f"üí∞ OpenAI costs fast response SAFE: {current_user.get('email') if current_user else 'anonymous'}")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erreur OpenAI costs fast SAFE: {e}")
        return {
            "total_cost": 0.0,
            "total_tokens": 0,
            "api_calls": 0,
            "models_usage": {},
            "error": "OpenAI costs endpoint en mode s√©curis√©",
            "memory_safe": True
        }

@router.get("/questions")
async def get_questions_fast(
    page: int = Query(1, ge=1),
    limit: int = Query(20, ge=1, le=100),
    search: str = Query("", description="Recherche dans questions/r√©ponses"),
    source: str = Query("all", description="Filtrer par source"),
    confidence: str = Query("all", description="Filtrer par confiance"),
    feedback: str = Query("all", description="Filtrer par feedback"),
    user: str = Query("all", description="Filtrer par utilisateur"),
    current_user: dict = Depends(get_current_user_safe()) if AUTH_AVAILABLE else None
) -> Dict[str, Any]:
    """üõ°Ô∏è CONSERV√â + OPTIMIS√â: Questions ultra-rapides MEMORY-SAFE avec fallback corrig√©"""
    
    # Cache local avec param√®tres
    cache_key = f"questions:{page}:{limit}:{hash(f'{search}{source}{confidence}{feedback}{user}')}"
    local_cached = get_local_cache(cache_key)
    if local_cached:
        return local_cached
    
    if current_user and not safe_has_permission(current_user, "VIEW_ALL_ANALYTICS"):
        raise HTTPException(status_code=403, detail="View all analytics permission required")
    
    try:
        cache = safe_get_cache()
        cached_questions = None
        
        if cache:
            try:
                filters = {
                    "search": search.lower()[:50] if search else "",  # Limiter search
                    "source": source,
                    "confidence": confidence,
                    "feedback": feedback,
                    "user": user
                }
                cache_key_full = f"questions:page:{page}:limit:{min(limit, 50)}:filters:{hash(str(sorted(filters.items())))}"
                cached_questions = cache.get_cache(cache_key_full)
            except Exception as cache_error:
                logger.warning(f"‚ö†Ô∏è Erreur cache questions: {cache_error}")
        
        if cached_questions:
            result = cached_questions["data"]
            result["meta"]["cache_hit"] = True
            result["cache_info"] = {
                "is_available": True,
                "last_update": datetime.now().isoformat(),
                "cache_age_minutes": 0,
                "performance_gain": "95%",
                "next_update": None,
                "memory_safe": True
            }
            
            # Limiter la taille + cache local
            result = limit_response_size(result)
            set_local_cache(cache_key, result, ttl_minutes=2)
            
            if not FAST_CONFIG["REDUCE_LOG_VERBOSITY"]:
                logger.info(f"üìã Questions cache HIT SAFE: page {page}")
            return result
        
        # üîß CORRECTIF: Fallback logging endpoint am√©lior√©
        if LOGGING_AVAILABLE:
            try:
                logging_module = _imported_modules.get("logging")
                questions_final_func = logging_module.get("questions_final") if logging_module else None
                
                if questions_final_func and callable(questions_final_func):
                    logger.info("üìã Utilisation fallback logging endpoint CORRIG√â")
                    
                    old_response = await questions_final_func(
                        page=page,
                        limit=min(limit, 20),  # Limiter encore plus
                        current_user=current_user or {"user_type": "anonymous"}
                    )
                    
                    # V√âRIFIER et CORRIGER la structure de la r√©ponse
                    if isinstance(old_response, dict) and "questions" in old_response:
                        questions_list = old_response.get("questions", [])
                        
                        # S'assurer que questions est une liste et limiter
                        if not isinstance(questions_list, list):
                            questions_list = []
                        else:
                            questions_list = questions_list[:20]  # Max 20 pour m√©moire
                        
                        fallback_response = {
                            "cache_info": {
                                "is_available": True,
                                "last_update": datetime.now().isoformat(),
                                "cache_age_minutes": 0,
                                "performance_gain": "60%",
                                "next_update": None,
                                "fallback_used": "logging_endpoint_safe_corrected",
                                "memory_safe": True
                            },
                            "questions": questions_list,
                            "pagination": {
                                "page": page,
                                "limit": limit,
                                "total": len(questions_list),
                                "pages": max(1, (len(questions_list) + limit - 1) // limit),
                                "has_next": False,
                                "has_prev": page > 1
                            },
                            "meta": {
                                "retrieved": len(questions_list),
                                "user_role": current_user.get("user_type") if current_user else "anonymous",
                                "timestamp": datetime.now().isoformat(),
                                "cache_hit": False,
                                "source": "logging_endpoint_fallback_corrected",
                                "fallback_successful": True,
                                "memory_optimization": "enabled"
                            }
                        }
                        
                        # Limiter taille + cache
                        fallback_response = limit_response_size(fallback_response)
                        set_local_cache(cache_key, fallback_response, ttl_minutes=2)
                        
                        logger.info(f"üìã Questions logging fallback CORRIG√â SUCCESS: {len(questions_list)} r√©sultats")
                        return fallback_response
                else:
                    logger.warning("‚ö†Ô∏è questions_final non callable ou manquante")
                        
            except Exception as fallback_error:
                logger.warning(f"‚ö†Ô∏è Fallback logging endpoint √©chou√© CORRIG√â: {fallback_error}")
        
        # Fallback ultime - STRUCTURE CORRECTE GARANTIE
        fallback_response = {
            "cache_info": {
                "is_available": False,
                "last_update": datetime.now().isoformat(),
                "cache_age_minutes": 0,
                "performance_gain": "0%",
                "next_update": None,
                "fallback_reason": "Cache et logging endpoint indisponibles",
                "memory_safe": True
            },
            "questions": [],  # ‚úÖ LISTE VIDE - STRUCTURE CORRECTE
            "pagination": {
                "page": page,
                "limit": limit,
                "total": 0,
                "pages": 0,
                "has_next": False,
                "has_prev": False
            },
            "meta": {
                "retrieved": 0,
                "user_role": current_user.get("user_type") if current_user else "anonymous",
                "timestamp": datetime.now().isoformat(),
                "cache_hit": False,
                "source": "safe_fallback_empty_list_corrected",
                "note": "Questions endpoint en mode s√©curis√© - structure correcte garantie",
                "memory_optimization": "enabled"
            }
        }
        
        # Cache m√™me le fallback
        set_local_cache(cache_key, fallback_response, ttl_minutes=1)
        
        logger.info(f"üìã Questions fallback ultime CORRIG√â: page {page} - structure correcte")
        return fallback_response
        
    except Exception as e:
        logger.error(f"‚ùå Erreur questions fast CORRIG√â: {e}")
        # M√äME EN CAS D'ERREUR - STRUCTURE CORRECTE
        return {
            "cache_info": {"is_available": False, "error": "Erreur g√©n√©rale endpoint questions", "memory_safe": True},
            "questions": [],  # ‚úÖ LISTE VIDE - STRUCTURE CORRECTE
            "pagination": {"page": page, "limit": limit, "total": 0, "pages": 0, "has_next": False, "has_prev": False},
            "meta": {"retrieved": 0, "error": "Questions endpoint en mode s√©curis√©", "source": "error_fallback_safe_corrected"}
        }

# ==================== TOUS LES AUTRES ENDPOINTS CONSERV√âS SANS MODIFICATION ====================

@router.get("/invitations")
async def get_invitations_fast(
    current_user: dict = Depends(get_current_user_safe()) if AUTH_AVAILABLE else None
) -> Dict[str, Any]:
    """üõ°Ô∏è CONSERV√â: Invitations - Redirect vers stats"""
    logger.info(f"üìß Invitations endpoint appel√© SAFE: {current_user.get('email') if current_user else 'anonymous'}")
    return await get_invitations_stats_fast(current_user)

@router.get("/invitations/stats")
async def get_invitations_stats_fast(
    current_user: dict = Depends(get_current_user_safe()) if AUTH_AVAILABLE else None
) -> Dict[str, Any]:
    """üõ°Ô∏è CONSERV√â + OPTIMIS√â: Statistiques invitations - VRAIES DONN√âES MEMORY-SAFE"""
    
    # Cache local
    cache_key = f"invitations_stats:{current_user.get('email') if current_user else 'anon'}"
    local_cached = get_local_cache(cache_key)
    if local_cached:
        return local_cached
    
    if current_user and not safe_has_permission(current_user, "VIEW_ALL_ANALYTICS"):
        logger.warning(f"üìß Permission refus√©e SAFE: {current_user.get('email')}")
        raise HTTPException(status_code=403, detail="View analytics permission required")
    
    logger.info(f"üìß Invitations stats SAFE: {current_user.get('email') if current_user else 'anonymous'}")
    
    try:
        # Essayer le cache d'abord
        cache = safe_get_cache()
        if cache:
            try:
                cached_stats = cache.get_cache("invitations:real_stats")
                if cached_stats:
                    result = cached_stats["data"]
                    result["memory_optimization"] = "enabled"
                    set_local_cache(cache_key, result, ttl_minutes=10)
                    logger.info("üìß Cache HIT pour invitations stats SAFE")
                    return result
            except Exception as cache_error:
                logger.warning(f"‚ö†Ô∏è Erreur cache invitations SAFE: {cache_error}")
        
        # üõ°Ô∏è Import conditionnel pour √©viter surcharge m√©moire
        try:
            import psycopg2
            from psycopg2.extras import RealDictCursor
        except ImportError:
            logger.error("‚ùå psycopg2 non disponible SAFE")
            raise HTTPException(status_code=500, detail="Database driver not available")
        
        dsn = os.getenv("DATABASE_URL")
        if not dsn:
            logger.error("‚ùå DATABASE_URL manquante SAFE")
            raise HTTPException(status_code=500, detail="Database not configured")
        
        with psycopg2.connect(dsn) as conn:
            with conn.cursor(cursor_factory=RealDictCursor) as cur:
                
                # V√©rifier si la table invitations existe
                cur.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = 'invitations'
                    )
                """)
                
                table_exists = cur.fetchone()['exists']
                
                if not table_exists:
                    logger.error("‚ùå Table invitations n'existe pas SAFE")
                    raise HTTPException(status_code=500, detail="Invitations table does not exist")
                
                # üõ°Ô∏è Requ√™tes avec LIMIT pour √©conomie m√©moire
                cur.execute("""
                    SELECT 
                        COUNT(*) as total_sent,
                        COUNT(*) FILTER (WHERE status = 'accepted') as total_accepted,
                        COUNT(DISTINCT inviter_email) as unique_inviters
                    FROM (
                        SELECT * FROM invitations 
                        ORDER BY created_at DESC 
                        LIMIT 5000
                    ) recent_invitations
                """)
                
                totals = dict(cur.fetchone() or {})
                total_sent = totals.get('total_sent', 0)
                total_accepted = totals.get('total_accepted', 0)
                unique_inviters = totals.get('unique_inviters', 0)
                
                acceptance_rate = (total_accepted / total_sent * 100) if total_sent > 0 else 0
                
                # Top inviters (limit√© √† 5 pour √©conomie m√©moire)
                cur.execute("""
                    SELECT 
                        inviter_email,
                        inviter_name,
                        COUNT(*) as invitations_sent,
                        COUNT(*) FILTER (WHERE status = 'accepted') as invitations_accepted,
                        CASE 
                            WHEN COUNT(*) > 0 THEN 
                                (COUNT(*) FILTER (WHERE status = 'accepted')::float / COUNT(*) * 100)
                            ELSE 0 
                        END as acceptance_rate
                    FROM (
                        SELECT * FROM invitations 
                        ORDER BY created_at DESC 
                        LIMIT 1000
                    ) recent_inv
                    GROUP BY inviter_email, inviter_name
                    ORDER BY invitations_sent DESC
                    LIMIT 5
                """)
                
                top_inviters = []
                for row in cur.fetchall():
                    top_inviters.append({
                        "inviter_email": (row['inviter_email'] or '')[:50],  # Tronquer
                        "inviter_name": (row['inviter_name'] or row['inviter_email'] or 'Unknown')[:50],
                        "invitations_sent": int(row['invitations_sent']),
                        "invitations_accepted": int(row['invitations_accepted']),
                        "acceptance_rate": round(float(row['acceptance_rate']), 1)
                    })
                
                # Version simplifi√©e pour top_accepted (m√™me donn√©es)
                top_accepted = [inv for inv in top_inviters if inv['invitations_accepted'] > 0][:3]
                
                result = {
                    "cache_info": {
                        "is_available": True,
                        "last_update": datetime.now().isoformat(),
                        "cache_age_minutes": 0,
                        "performance_gain": "95%",
                        "next_update": (datetime.now() + timedelta(hours=2)).isoformat(),
                        "data_source": "database_direct_safe",
                        "memory_optimization": "enabled"
                    },
                    "invitation_stats": {
                        "total_invitations_sent": total_sent,
                        "total_invitations_accepted": total_accepted,
                        "acceptance_rate": round(acceptance_rate, 1),
                        "unique_inviters": unique_inviters,
                        "top_inviters": top_inviters,
                        "top_accepted": top_accepted
                    }
                }
                
                # Sauvegarder dans le cache
                if cache:
                    try:
                        cache.set_cache("invitations:real_stats", result, ttl_hours=2, source="invitations_stats_real_safe")
                        logger.info("‚úÖ Statistiques invitations sauv√©es dans le cache SAFE")
                    except Exception as cache_save_error:
                        logger.warning(f"‚ö†Ô∏è Erreur sauvegarde cache SAFE: {cache_save_error}")
                
                # Cache local
                set_local_cache(cache_key, result, ttl_minutes=15)
                
                logger.info(f"‚úÖ Statistiques invitations calcul√©es SAFE: {total_sent} sent, {total_accepted} accepted")
                return result
                
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur g√©n√©rale invitations stats SAFE: {e}")
        raise HTTPException(status_code=500, detail="Internal server error retrieving invitation stats")

@router.get("/my-analytics")
async def get_my_analytics_fast(
    days: int = Query(30, ge=1, le=365),
    current_user: dict = Depends(get_current_user_safe()) if AUTH_AVAILABLE else None
) -> Dict[str, Any]:
    """üõ°Ô∏è CONSERV√â + OPTIMIS√â: Analytics personnelles MEMORY-SAFE"""
    
    if current_user and not safe_has_permission(current_user, "VIEW_OWN_ANALYTICS"):
        raise HTTPException(status_code=403, detail="View own analytics permission required")
    
    try:
        user_email = current_user.get("email") if current_user else "anonymous"
        if current_user and not user_email:
            raise HTTPException(status_code=400, detail="User email not found")
        
        # Cache local
        cache_key = f"my_analytics:{user_email}:{days}"
        local_cached = get_local_cache(cache_key)
        if local_cached:
            return local_cached
        
        cache = safe_get_cache()
        user_analytics = None
        
        if cache and user_email != "anonymous":
            try:
                cache_key_full = f"analytics:user:{user_email}:days:{days}"
                user_analytics = cache.get_cache(cache_key_full)
            except Exception as cache_error:
                logger.warning(f"‚ö†Ô∏è Erreur cache user analytics SAFE: {cache_error}")
        
        if not user_analytics:
            user_analytics = {
                "data": {
                    "user_email": user_email[:50],  # Tronquer
                    "period_days": min(days, 90),  # Limiter
                    "questions": {
                        "total_questions": 0,
                        "successful_questions": 0,
                        "failed_questions": 0,
                        "avg_processing_time": 0,
                        "avg_confidence": 0
                    },
                    "openai_costs": {
                        "total_calls": 0,
                        "total_tokens": 0,
                        "total_cost_usd": 0,
                        "avg_cost_per_call": 0
                    },
                    "cost_by_purpose": [],
                    "note": "Cache utilisateur non disponible - mode s√©curis√©",
                    "memory_optimization": "enabled"
                }
            }
        
        result = user_analytics["data"]
        result["user_role"] = current_user.get("user_type") if current_user else "anonymous"
        result["memory_optimization"] = "enabled"
        
        # Limiter taille + cache local
        result = limit_response_size(result)
        set_local_cache(cache_key, result, ttl_minutes=10)
        
        if not FAST_CONFIG["REDUCE_LOG_VERBOSITY"]:
            logger.info(f"üìà User analytics fast response SAFE: {user_email}")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erreur user analytics fast SAFE: {e}")
        return {
            "user_email": current_user.get("email") if current_user else "anonymous",
            "period_days": days,
            "error": "User analytics en mode s√©curis√©",
            "memory_safe": True
        }

# ==================== ENDPOINTS DE MONITORING OPTIMIS√âS - CONSERV√âS ====================

@router.get("/health")
async def cache_health() -> Dict[str, Any]:
    """üõ°Ô∏è CONSERV√â + OPTIMIS√â: Health check MEMORY-SAFE"""
    try:
        # Cache local pour health check
        local_cached = get_local_cache("health_check")
        if local_cached:
            local_cached["cached_response"] = True
            return local_cached
        
        cache = safe_get_cache()
        
        health_status = {
            "status": "healthy",
            "cache_available": cache is not None,
            "stats_cache_available": STATS_CACHE_AVAILABLE,
            "auth_available": AUTH_AVAILABLE,
            "permissions_available": PERMISSIONS_AVAILABLE,
            "logging_available": LOGGING_AVAILABLE,
            "memory_optimization": "enabled",
            "fast_config": {
                "heavy_imports_enabled": FAST_CONFIG["ENABLE_HEAVY_IMPORTS"],
                "local_cache_enabled": FAST_CONFIG["CACHE_LOCAL_RESPONSES"],
                "max_response_size_kb": FAST_CONFIG["MAX_RESPONSE_SIZE_KB"]
            },
            "timestamp": datetime.now().isoformat()
        }
        
        # Test cache minimal (pour √©viter surcharge)
        if cache:
            try:
                test_key = "health:test:minimal"
                test_data = {"ts": int(datetime.now().timestamp())}
                
                write_success = cache.set_cache(test_key, test_data, ttl_hours=0.1, source="health_check_safe")  # TTL court
                read_result = cache.get_cache(test_key)
                read_success = read_result is not None
                
                cache.invalidate_cache(key=test_key)
                
                health_status.update({
                    "cache_test_results": {
                        "write": write_success,
                        "read": read_success,
                        "test_minimal": True
                    }
                })
                
            except Exception as test_error:
                logger.warning(f"‚ö†Ô∏è Test cache health √©chou√© SAFE: {test_error}")
                health_status["cache_test_error"] = str(test_error)[:100]
        
        # Cache local du health check
        set_local_cache("health_check", health_status, ttl_minutes=2)
        
        logger.info("ü•º Cache health check completed SAFE")
        return health_status
        
    except Exception as e:
        logger.error(f"‚ùå Erreur cache health SAFE: {e}")
        return {
            "status": "error",
            "error": str(e)[:100],
            "timestamp": datetime.now().isoformat(),
            "memory_safe": True
        }

@router.get("/system-info")
async def get_system_info() -> Dict[str, Any]:
    """üõ°Ô∏è CONSERV√â + OPTIMIS√â: Informations syst√®me MEMORY-SAFE"""
    
    # Cache local
    local_cached = get_local_cache("system_info")
    if local_cached:
        return local_cached
    
    system_info = {
        "status": "active",
        "components": {
            "stats_cache": STATS_CACHE_AVAILABLE,
            "auth": AUTH_AVAILABLE,
            "permissions": PERMISSIONS_AVAILABLE,
            "logging": LOGGING_AVAILABLE
        },
        "memory_optimization": {
            "enabled": True,
            "version": "memory_safe_v2_corrected",
            "local_cache_entries": len(_local_cache),
            "max_response_size_kb": FAST_CONFIG["MAX_RESPONSE_SIZE_KB"],
            "heavy_imports_disabled": not FAST_CONFIG["ENABLE_HEAVY_IMPORTS"]
        },
        "performance": {
            "lazy_loading": FAST_CONFIG["LAZY_LOAD_MODULES"],
            "import_monitoring": FAST_CONFIG["ENABLE_IMPORT_MONITORING"],
            "reduced_logging": FAST_CONFIG["REDUCE_LOG_VERBOSITY"]
        },
        "timestamp": datetime.now().isoformat()
    }
    
    # Ajouter info m√©moire si disponible
    try:
        memory_mb = get_memory_usage_mb()
        if memory_mb > 0:
            system_info["memory_usage_mb"] = round(memory_mb, 1)
    except:
        pass
    
    # Cache local
    set_local_cache("system_info", system_info, ttl_minutes=5)
    
    return system_info

# ==================== ENDPOINTS DE COMPATIBILIT√â (CONSERV√âS) ====================

@router.get("/compatibility/logging-dashboard") 
async def compatibility_logging_dashboard(
    current_user: dict = Depends(get_current_user_safe()) if AUTH_AVAILABLE else None
) -> Dict[str, Any]:
    """üõ°Ô∏è CONSERV√â: Compatibilit√© avec /logging/analytics/dashboard"""
    return await get_dashboard_fast(current_user)

@router.get("/compatibility/logging-performance")
async def compatibility_logging_performance(
    hours: int = Query(24, ge=1, le=168),
    current_user: dict = Depends(get_current_user_safe()) if AUTH_AVAILABLE else None
) -> Dict[str, Any]:
    """üõ°Ô∏è CONSERV√â: Compatibilit√© avec /logging/analytics/performance"""
    return await get_performance_fast(hours, current_user)

# ==================== UTILITAIRES CONSERV√âS ====================

def format_timestamp(timestamp: Optional[str]) -> str:
    """üõ°Ô∏è CONSERV√â: Formate un timestamp pour l'affichage"""
    if not timestamp:
        return "N/A"
    
    try:
        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
        return dt.strftime("%Y-%m-%d %H:%M:%S UTC")
    except Exception as e:
        if not FAST_CONFIG["REDUCE_LOG_VERBOSITY"]:
            logger.warning(f"Erreur format timestamp: {e}")
        return str(timestamp)[:19]  # Tronquer si erreur

# ==================== NOUVELLES FONCTIONS MEMORY-SAFE - CONSERV√âES ====================

@router.get("/memory-stats")
async def get_memory_stats() -> Dict[str, Any]:
    """üÜï CONSERV√â: Statistiques m√©moire en temps r√©el"""
    try:
        memory_mb = get_memory_usage_mb()
        
        stats = {
            "memory_usage_mb": round(memory_mb, 1),
            "local_cache": {
                "entries": len(_local_cache),
                "max_entries": _cache_max_entries,
                "keys": list(_local_cache.keys())[:10]  # √âchantillon
            },
            "imported_modules": list(_imported_modules.keys()),
            "fast_config": FAST_CONFIG.copy(),
            "recommendations": []
        }
        
        # Recommandations bas√©es sur l'usage
        if memory_mb > 1500:
            stats["recommendations"].append("Consid√©rer ENABLE_HEAVY_IMPORTS=false")
        if len(_local_cache) > 15:
            stats["recommendations"].append("Cache local proche de la limite")
        if not FAST_CONFIG["REDUCE_LOG_VERBOSITY"]:
            stats["recommendations"].append("Activer REDUCE_LOG_VERBOSITY=true")
        
        return stats
        
    except Exception as e:
        return {"error": str(e), "memory_safe": True}

@router.post("/optimize-memory")
async def optimize_memory() -> Dict[str, Any]:
    """üÜï CONSERV√â: Optimisation m√©moire manuelle"""
    try:
        # Nettoyer cache local
        cleanup_expired_local_cache()
        old_cache_size = len(_local_cache)
        _local_cache.clear()
        _cache_timestamps.clear()
        
        # Force garbage collection
        gc.collect()
        
        # R√©initialiser imports si n√©cessaire
        heavy_imports_cleared = 0
        if not FAST_CONFIG["ENABLE_HEAVY_IMPORTS"]:
            for module_name in ["logging"]:
                if module_name in _imported_modules and _imported_modules[module_name]:
                    _imported_modules[module_name] = None
                    heavy_imports_cleared += 1
        
        return {
            "status": "optimized",
            "actions_taken": {
                "local_cache_cleared": old_cache_size,
                "garbage_collection": "forced",
                "heavy_imports_cleared": heavy_imports_cleared
            },
            "current_memory_mb": round(get_memory_usage_mb(), 1),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return {"status": "error", "error": str(e)}

# Force garbage collection au chargement si configur√©
if FAST_CONFIG["FORCE_GC_AFTER_ENDPOINT"]:
    logger.info("üóëÔ∏è Garbage collection forc√© activ√© (debug mode)")
    gc.collect()

logger.info("‚úÖ stats_fast.py MEMORY-SAFE CORRIG√â charg√© avec succ√®s")