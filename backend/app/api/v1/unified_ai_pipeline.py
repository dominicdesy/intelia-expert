"""
unified_ai_pipeline.py - PIPELINE UNIFI√â IA

üéØ ORCHESTRATEUR CENTRAL: Coordonne tous les services IA
üöÄ CAPACIT√âS:
- ‚úÖ Pipeline complet end-to-end avec IA
- ‚úÖ Orchestration intelligente des services IA
- ‚úÖ Gestion des contextes conversationnels
- ‚úÖ Optimisation des performances avec cache
- ‚úÖ Fallbacks automatiques robustes
- ‚úÖ Monitoring et m√©triques d√©taill√©es

Architecture:
- Orchestration s√©quentielle optimis√©e
- Cache intelligent multi-niveau
- Gestion d'erreurs avec fallbacks
- Interface unifi√©e pour expert_services.py
- Monitoring complet du pipeline
"""

import json
import logging
import asyncio
from typing import Dict, Any, Optional, List, Union, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum

# Imports des services IA
from .ai_service_manager import get_ai_service_manager
from .ai_entity_extractor import get_ai_entity_extractor, ExtractedEntities
from .ai_context_enhancer import get_ai_context_enhancer, EnhancedContext  
from .ai_response_generator import get_ai_response_generator, ResponseData
from .ai_validation_service import get_ai_validation_service, ClassificationResult, ValidationResult

logger = logging.getLogger(__name__)

class PipelineStage(Enum):
    """√âtapes du pipeline"""
    ENTITY_EXTRACTION = "entity_extraction"
    CONTEXT_ENHANCEMENT = "context_enhancement"
    INTENT_CLASSIFICATION = "intent_classification"
    RESPONSE_GENERATION = "response_generation"
    VALIDATION = "validation"

@dataclass
class PipelineResult:
    """R√©sultat complet du pipeline IA"""
    # R√©sultat final
    final_response: str
    response_type: str
    confidence: float
    
    # Donn√©es interm√©diaires
    extracted_entities: ExtractedEntities
    enhanced_context: Optional[EnhancedContext] = None
    classification_result: Optional[ClassificationResult] = None
    weight_data: Dict[str, Any] = None
    
    # M√©triques pipeline
    total_processing_time_ms: int = 0
    stages_completed: List[str] = None
    ai_calls_made: int = 0
    cache_hits: int = 0
    fallback_used: bool = False
    
    # M√©tadonn√©es
    conversation_id: Optional[str] = None
    language: str = "fr"
    pipeline_version: str = "1.0.0"
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.stages_completed is None:
            self.stages_completed = []
        if self.timestamp is None:
            self.timestamp = datetime.now()
        if self.weight_data is None:
            self.weight_data = {}

class UnifiedAIPipeline:
    """Pipeline unifi√© orchestrant tous les services IA"""
    
    def __init__(self):
        # Services IA
        self.ai_manager = get_ai_service_manager()
        self.entity_extractor = get_ai_entity_extractor()
        self.context_enhancer = get_ai_context_enhancer()
        self.response_generator = get_ai_response_generator()
        self.validation_service = get_ai_validation_service()
        
        # Configuration du pipeline
        self.pipeline_config = {
            "max_processing_time_ms": 30000,  # 30 secondes max
            "enable_parallel_processing": True,
            "cache_intermediate_results": True,
            "fallback_on_ai_failure": True,
            "min_confidence_threshold": 0.3
        }
        
        # M√©triques du pipeline
        self.pipeline_metrics = {
            "total_runs": 0,
            "successful_runs": 0,
            "failed_runs": 0,
            "average_processing_time": 0.0,
            "stage_success_rates": {stage.value: 0 for stage in PipelineStage},
            "fallback_usage": 0
        }
        
        logger.info("ü§ñ [Unified AI Pipeline] Initialis√© avec orchestration compl√®te")
    
    async def process_complete_pipeline(self,
                                      question: str,
                                      conversation_id: str = None,
                                      language: str = "fr",
                                      user_context: Dict[str, Any] = None) -> PipelineResult:
        """
        Point d'entr√©e principal - Pipeline complet end-to-end
        
        Args:
            question: Question de l'utilisateur
            conversation_id: ID de conversation pour contexte
            language: Langue d√©tect√©e
            user_context: Contexte utilisateur additionnel
            
        Returns:
            PipelineResult avec r√©ponse compl√®te et m√©triques
        """
        start_time = datetime.now()
        self.pipeline_metrics["total_runs"] += 1
        
        try:
            logger.info(f"üöÄ [Unified AI Pipeline] D√©but pipeline: '{question[:50]}...' (conv: {conversation_id})")
            
            # Initialiser le r√©sultat
            pipeline_result = PipelineResult(
                final_response="",
                response_type="processing",
                confidence=0.0,
                extracted_entities=ExtractedEntities(),
                conversation_id=conversation_id,
                language=language
            )
            
            # √âTAPE 1: Extraction d'entit√©s avec IA
            extracted_entities = await self._execute_entity_extraction(question, language, pipeline_result)
            
            # √âTAPE 2: Enhancement contextuel (si contexte disponible)
            enhanced_context = await self._execute_context_enhancement(
                question, extracted_entities, conversation_id, pipeline_result
            )
            
            # √âTAPE 3: Classification d'intention et type de r√©ponse
            classification_result = await self._execute_intent_classification(
                question, extracted_entities, enhanced_context, pipeline_result
            )
            
            # √âTAPE 4: Calcul des donn√©es de poids si applicable
            weight_data = await self._execute_weight_calculation(
                extracted_entities, enhanced_context, classification_result, pipeline_result
            )
            
            # √âTAPE 5: G√©n√©ration de r√©ponse finale
            final_response = await self._execute_response_generation(
                question, extracted_entities, enhanced_context, classification_result, weight_data, pipeline_result
            )
            
            # Finaliser le r√©sultat
            processing_time = int((datetime.now() - start_time).total_seconds() * 1000)
            
            pipeline_result.final_response = final_response.content
            pipeline_result.response_type = final_response.response_type
            pipeline_result.confidence = final_response.confidence
            pipeline_result.total_processing_time_ms = processing_time
            pipeline_result.weight_data = weight_data
            
            # M√©triques de succ√®s
            self.pipeline_metrics["successful_runs"] += 1
            self._update_processing_time_metrics(processing_time)
            
            logger.info(f"‚úÖ [Unified AI Pipeline] Pipeline termin√© avec succ√®s: {processing_time}ms, conf: {pipeline_result.confidence}")
            
            return pipeline_result
            
        except Exception as e:
            # M√©triques d'√©chec
            self.pipeline_metrics["failed_runs"] += 1
            processing_time = int((datetime.now() - start_time).total_seconds() * 1000)
            
            logger.error(f"‚ùå [Unified AI Pipeline] Erreur pipeline: {e}")
            
            # Fallback d'urgence
            return await self._execute_emergency_fallback(question, conversation_id, language, str(e), processing_time)
    
    async def _execute_entity_extraction(self, 
                                       question: str, 
                                       language: str, 
                                       pipeline_result: PipelineResult) -> ExtractedEntities:
        """√âtape 1: Extraction d'entit√©s avec IA"""
        
        try:
            logger.info("üîç [Pipeline] √âtape 1: Extraction entit√©s IA")
            
            extracted_entities = await self.entity_extractor.extract_entities(question, language)
            
            pipeline_result.extracted_entities = extracted_entities
            pipeline_result.stages_completed.append(PipelineStage.ENTITY_EXTRACTION.value)
            pipeline_result.ai_calls_made += 1
            
            self.pipeline_metrics["stage_success_rates"][PipelineStage.ENTITY_EXTRACTION.value] += 1
            
            logger.info(f"‚úÖ [Pipeline] Entit√©s extraites: {extracted_entities.breed_specific or 'N/A'}, {extracted_entities.age_days or 'N/A'}j, {extracted_entities.sex or 'N/A'}")
            
            return extracted_entities
            
        except Exception as e:
            logger.error(f"‚ùå [Pipeline] Erreur extraction entit√©s: {e}")
            # Retourner entit√©s vides plut√¥t que faire √©chouer
            pipeline_result.fallback_used = True
            return ExtractedEntities()
    
    async def _execute_context_enhancement(self,
                                         question: str,
                                         extracted_entities: ExtractedEntities,
                                         conversation_id: str,
                                         pipeline_result: PipelineResult) -> Optional[EnhancedContext]:
        """√âtape 2: Enhancement contextuel"""
        
        try:
            if not conversation_id:
                logger.info("üîó [Pipeline] √âtape 2: Pas de contexte conversationnel")
                return None
            
            logger.info("üîó [Pipeline] √âtape 2: Enhancement contextuel IA")
            
            # R√©cup√©ration du contexte conversationnel (simplified)
            conversation_context = await self._get_conversation_context(conversation_id)
            
            # Conversion des entit√©s pour compatibilit√©
            entities_dict = self._convert_entities_to_dict(extracted_entities)
            
            enhanced_context = await self.context_enhancer.enhance_question_for_rag(
                original_question=question,
                conversation_context=conversation_context,
                current_entities=entities_dict,
                language=pipeline_result.language
            )
            
            pipeline_result.enhanced_context = enhanced_context
            pipeline_result.stages_completed.append(PipelineStage.CONTEXT_ENHANCEMENT.value)
            pipeline_result.ai_calls_made += 2  # G√©n√©ralement 2 appels IA pour enhancement
            
            self.pipeline_metrics["stage_success_rates"][PipelineStage.CONTEXT_ENHANCEMENT.value] += 1
            
            logger.info(f"‚úÖ [Pipeline] Contexte enrichi: '{enhanced_context.enhanced_question}'")
            
            return enhanced_context
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [Pipeline] Erreur enhancement contextuel: {e}")
            return None
    
    async def _execute_intent_classification(self,
                                           question: str,
                                           extracted_entities: ExtractedEntities,
                                           enhanced_context: Optional[EnhancedContext],
                                           pipeline_result: PipelineResult) -> ClassificationResult:
        """√âtape 3: Classification d'intention avec IA"""
        
        try:
            logger.info("üß† [Pipeline] √âtape 3: Classification intention IA")
            
            # Utiliser le contexte enrichi si disponible
            entities_dict = enhanced_context.merged_entities if enhanced_context else self._convert_entities_to_dict(extracted_entities)
            
            # Contexte conversationnel pour classification
            conversation_context = enhanced_context.context_summary if enhanced_context else ""
            
            classification_result = await self.validation_service.classify_intent_and_response_type(
                question=question,
                entities=entities_dict,
                conversation_context=conversation_context,
                language=pipeline_result.language
            )
            
            pipeline_result.classification_result = classification_result
            pipeline_result.stages_completed.append(PipelineStage.INTENT_CLASSIFICATION.value)
            pipeline_result.ai_calls_made += 1
            
            self.pipeline_metrics["stage_success_rates"][PipelineStage.INTENT_CLASSIFICATION.value] += 1
            
            logger.info(f"‚úÖ [Pipeline] Classification: {classification_result.intent_type.value} ‚Üí {classification_result.response_type.value}")
            
            return classification_result
            
        except Exception as e:
            logger.error(f"‚ùå [Pipeline] Erreur classification: {e}")
            # Fallback classification simple
            pipeline_result.fallback_used = True
            return self._fallback_classification(question, extracted_entities)
    
    async def _execute_weight_calculation(self,
                                        extracted_entities: ExtractedEntities,
                                        enhanced_context: Optional[EnhancedContext],
                                        classification_result: ClassificationResult,
                                        pipeline_result: PipelineResult) -> Dict[str, Any]:
        """√âtape 4: Calcul des donn√©es de poids si applicable"""
        
        try:
            # V√©rifier si calcul de poids recommand√©
            if not classification_result.suggested_weight_calculation:
                logger.info("üìä [Pipeline] √âtape 4: Calcul poids non n√©cessaire")
                return {}
            
            logger.info("üìä [Pipeline] √âtape 4: Calcul donn√©es poids")
            
            # Utiliser entit√©s fusionn√©es si disponibles
            entities_dict = enhanced_context.merged_entities if enhanced_context else self._convert_entities_to_dict(extracted_entities)
            
            # Calcul avec la logique existante (intelligent_system_config)
            weight_data = await self._calculate_weight_data_integrated(entities_dict)
            
            pipeline_result.weight_data = weight_data
            
            if weight_data:
                weight_range = weight_data.get("weight_range", (0, 0))
                logger.info(f"‚úÖ [Pipeline] Donn√©es poids calcul√©es: {weight_range[0]}-{weight_range[1]}g")
            
            return weight_data
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [Pipeline] Erreur calcul poids: {e}")
            return {}
    
    async def _execute_response_generation(self,
                                         question: str,
                                         extracted_entities: ExtractedEntities,
                                         enhanced_context: Optional[EnhancedContext],
                                         classification_result: ClassificationResult,
                                         weight_data: Dict[str, Any],
                                         pipeline_result: PipelineResult) -> ResponseData:
        """√âtape 5: G√©n√©ration de r√©ponse finale"""
        
        try:
            logger.info("‚ú® [Pipeline] √âtape 5: G√©n√©ration r√©ponse IA")
            
            # Choisir la m√©thode de g√©n√©ration selon classification
            response_type = classification_result.response_type.value
            
            if response_type == "contextual_answer":
                # R√©ponse contextuelle avec donn√©es pr√©cises
                entities_dict = enhanced_context.merged_entities if enhanced_context else self._convert_entities_to_dict(extracted_entities)
                
                response_data = await self.response_generator.generate_contextual_response(
                    question=enhanced_context.enhanced_question if enhanced_context else question,
                    merged_entities=entities_dict,
                    weight_data=weight_data,
                    conversation_context=enhanced_context.context_summary if enhanced_context else "",
                    language=pipeline_result.language
                )
                
            elif response_type == "general_answer":
                # R√©ponse g√©n√©rale informative
                entities_dict = self._convert_entities_to_dict(extracted_entities)
                
                response_data = await self.response_generator.generate_general_response(
                    question=question,
                    entities=entities_dict,
                    missing_entities=classification_result.missing_entities,
                    language=pipeline_result.language
                )
                
            elif response_type == "needs_clarification":
                # Demande de clarification
                entities_dict = self._convert_entities_to_dict(extracted_entities)
                
                response_data = await self.response_generator.generate_clarification_request(
                    question=question,
                    missing_entities=classification_result.missing_entities,
                    available_entities=entities_dict,
                    language=pipeline_result.language
                )
                
            elif classification_result.intent_type.value == "sant√©":
                # R√©ponse sp√©cialis√©e sant√©
                response_data = await self.response_generator.generate_health_response(
                    question=question,
                    symptoms=extracted_entities.symptoms,
                    entities=self._convert_entities_to_dict(extracted_entities),
                    language=pipeline_result.language
                )
                
            else:
                # Fallback vers r√©ponse g√©n√©rale
                entities_dict = self._convert_entities_to_dict(extracted_entities)
                response_data = await self.response_generator.generate_general_response(
                    question=question,
                    entities=entities_dict,
                    language=pipeline_result.language
                )
            
            pipeline_result.stages_completed.append(PipelineStage.RESPONSE_GENERATION.value)
            pipeline_result.ai_calls_made += 1
            
            self.pipeline_metrics["stage_success_rates"][PipelineStage.RESPONSE_GENERATION.value] += 1
            
            logger.info(f"‚úÖ [Pipeline] R√©ponse g√©n√©r√©e: {len(response_data.content)} caract√®res, conf: {response_data.confidence}")
            
            return response_data
            
        except Exception as e:
            logger.error(f"‚ùå [Pipeline] Erreur g√©n√©ration r√©ponse: {e}")
            # Fallback response simple
            pipeline_result.fallback_used = True
            return self._generate_fallback_response(question, classification_result, pipeline_result.language)
    
    async def _get_conversation_context(self, conversation_id: str) -> str:
        """R√©cup√®re le contexte conversationnel (simplified)"""
        
        try:
            # Ici on int√©grerait avec le ContextManager existant
            # Pour l'instant, retour simplifi√©
            return f"Contexte conversation {conversation_id}"
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [Pipeline] Erreur r√©cup√©ration contexte: {e}")
            return ""
    
    def _convert_entities_to_dict(self, entities: ExtractedEntities) -> Dict[str, Any]:
        """Convertit ExtractedEntities en dictionnaire"""
        
        return {
            "age_days": entities.age_days,
            "age_weeks": entities.age_weeks,
            "breed_specific": entities.breed_specific,
            "breed_generic": entities.breed_generic,
            "sex": entities.sex,
            "weight_mentioned": entities.weight_mentioned,
            "weight_grams": entities.weight_grams,
            "symptoms": entities.symptoms,
            "context_type": entities.context_type,
            "housing_conditions": entities.housing_conditions,
            "feeding_context": entities.feeding_context,
            "extraction_confidence": entities.extraction_confidence,
            "normalized_by_ai": entities.normalized_by_ai
        }
    
    async def _calculate_weight_data_integrated(self, entities: Dict[str, Any]) -> Dict[str, Any]:
        """Calcul int√©gr√© des donn√©es de poids"""
        
        try:
            # Import de la fonction existante
            from .intelligent_system_config import get_weight_range
            
            breed = entities.get("breed_specific", "").lower().replace(" ", "_")
            age_days = entities.get("age_days")
            sex = entities.get("sex", "mixed")
            
            if not breed or not age_days:
                return {}
            
            # Normalisation du sexe
            sex_mapping = {"male": "male", "female": "female", "mixed": "mixed"}
            sex = sex_mapping.get(sex, "mixed")
            
            # Calcul avec fonction existante
            weight_range = get_weight_range(breed, age_days, sex)
            min_weight, max_weight = weight_range
            
            target_weight = (min_weight + max_weight) // 2
            alert_low = int(min_weight * 0.85)
            alert_high = int(max_weight * 1.15)
            
            return {
                "breed": breed.replace("_", " ").title(),
                "age_days": age_days,
                "sex": sex,
                "weight_range": weight_range,
                "target_weight": target_weight,
                "alert_thresholds": {
                    "low": alert_low,
                    "high": alert_high
                },
                "confidence": 0.95,
                "calculation_method": "unified_ai_pipeline"
            }
            
        except Exception as e:
            logger.error(f"‚ùå [Pipeline] Erreur calcul poids: {e}")
            return {}
    
    def _fallback_classification(self, question: str, entities: ExtractedEntities) -> ClassificationResult:
        """Classification de fallback simple"""
        
        from .ai_validation_service import ClassificationResult, IntentType, ResponseType
        
        # Logique simple bas√©e sur mots-cl√©s
        question_lower = question.lower()
        
        if any(word in question_lower for word in ["poids", "croissance", "weight"]):
            intent = IntentType.PERFORMANCE_QUERY
        elif any(word in question_lower for word in ["malade", "sympt√¥me", "sick"]):
            intent = IntentType.HEALTH_CONCERN
        else:
            intent = IntentType.GENERAL_INFO
        
        # Type de r√©ponse simple
        if entities.breed_specific and entities.age_days:
            response_type = ResponseType.PRECISE_ANSWER
        elif entities.breed_specific or entities.age_days:
            response_type = ResponseType.GENERAL_ANSWER
        else:
            response_type = ResponseType.NEEDS_CLARIFICATION
        
        return ClassificationResult(
            intent_type=intent,
            response_type=response_type,
            confidence=0.5,
            reasoning="Classification fallback simple"
        )
    
    def _generate_fallback_response(self, question: str, classification: ClassificationResult, language: str) -> ResponseData:
        """G√©n√®re une r√©ponse de fallback"""
        
        fallback_content = {
            "fr": "Je comprends votre question sur l'√©levage avicole. Pour vous donner la r√©ponse la plus pr√©cise, pr√©cisez la race, l'√¢ge et le sexe de vos animaux si pertinent.",
            "en": "I understand your poultry farming question. To give you the most accurate answer, please specify the breed, age and sex of your animals if relevant.",
            "es": "Entiendo su pregunta sobre avicultura. Para darle la respuesta m√°s precisa, especifique la raza, edad y sexo de sus animales si es relevante."
        }
        
        content = fallback_content.get(language, fallback_content["fr"])
        
        return ResponseData(
            content=content,
            response_type=classification.response_type.value,
            confidence=0.3,
            reasoning="R√©ponse fallback du pipeline",
            language=language,
            generation_method="pipeline_fallback"
        )
    
    async def _execute_emergency_fallback(self, question: str, conversation_id: str, language: str, error: str, processing_time: int) -> PipelineResult:
        """Fallback d'urgence complet"""
        
        logger.error(f"üÜò [Pipeline] Fallback d'urgence activ√©: {error}")
        
        self.pipeline_metrics["fallback_usage"] += 1
        
        emergency_response = {
            "fr": f"Je rencontre une difficult√© technique pour traiter votre question. Veuillez reformuler ou contacter le support.",
            "en": f"I'm experiencing a technical difficulty processing your question. Please rephrase or contact support.",
            "es": f"Tengo una dificultad t√©cnica para procesar su pregunta. Por favor reformule o contacte soporte."
        }
        
        return PipelineResult(
            final_response=emergency_response.get(language, emergency_response["fr"]),
            response_type="emergency_fallback",
            confidence=0.1,
            extracted_entities=ExtractedEntities(),
            total_processing_time_ms=processing_time,
            stages_completed=["emergency_fallback"],
            fallback_used=True,
            conversation_id=conversation_id,
            language=language
        )
    
    def _update_processing_time_metrics(self, processing_time: int):
        """Met √† jour les m√©triques de temps de traitement"""
        
        current_avg = self.pipeline_metrics["average_processing_time"]
        total_runs = self.pipeline_metrics["successful_runs"]
        
        # Moyenne pond√©r√©e
        self.pipeline_metrics["average_processing_time"] = (
            (current_avg * (total_runs - 1) + processing_time) / total_runs
        )
    
    def get_pipeline_health(self) -> Dict[str, Any]:
        """Retourne l'√©tat de sant√© du pipeline"""
        
        total_runs = self.pipeline_metrics["total_runs"]
        success_rate = (self.pipeline_metrics["successful_runs"] / total_runs * 100) if total_runs > 0 else 0
        
        return {
            "pipeline_name": "Unified AI Pipeline",
            "version": "1.0.0",
            "total_runs": total_runs,
            "success_rate": round(success_rate, 2),
            "average_processing_time_ms": round(self.pipeline_metrics["average_processing_time"], 2),
            "fallback_usage_rate": round((self.pipeline_metrics["fallback_usage"] / total_runs * 100) if total_runs > 0 else 0, 2),
            "stage_success_rates": {
                stage: round((count / total_runs * 100) if total_runs > 0 else 0, 2)
                for stage, count in self.pipeline_metrics["stage_success_rates"].items()
            },
            "ai_service_health": self.ai_manager.get_service_health(),
            "configuration": self.pipeline_config
        }
    
    def reset_metrics(self):
        """Remet √† z√©ro les m√©triques du pipeline"""
        
        self.pipeline_metrics = {
            "total_runs": 0,
            "successful_runs": 0,
            "failed_runs": 0,
            "average_processing_time": 0.0,
            "stage_success_rates": {stage.value: 0 for stage in PipelineStage},
            "fallback_usage": 0
        }
        
        logger.info("üîÑ [Unified AI Pipeline] M√©triques remises √† z√©ro")

# Instance globale pour utilisation facile
_unified_ai_pipeline = None

def get_unified_ai_pipeline() -> UnifiedAIPipeline:
    """R√©cup√®re l'instance singleton du pipeline IA unifi√©"""
    global _unified_ai_pipeline
    if _unified_ai_pipeline is None:
        _unified_ai_pipeline = UnifiedAIPipeline()
    return _unified_ai_pipeline