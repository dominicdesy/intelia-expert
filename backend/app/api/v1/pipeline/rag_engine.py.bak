"""
RAGEngine - Version 2 (FAISS local + filtrage esp√®ce)
- Remplace l'usage direct de VectorStoreClient par le FastRAGEmbedder local
- Passe l'esp√®ce (broiler/layer) d√©duite du contexte/la question √† l'embedder pour filtrer les docs
- Fallback propre vers OpenAI si aucun document pertinent
- Sortie standardis√©e: dict avec 'response' (str), 'source', 'documents_used', 'warning', 'citations'
"""

from __future__ import annotations

import os
import logging
from typing import Any, Dict, List, Optional

from app.api.v1.utils.openai_utils import safe_chat_completion

# ‚ö†Ô∏è Importe l'embedder local (FAISS) ‚Äî chemin 'rag/embedder.py'
try:
    # Chemin absolu vs relatif selon ta structure de projet
    from rag.embedder import FastRAGEmbedder, create_optimized_embedder
except Exception as e:
    FastRAGEmbedder = None  # type: ignore
    create_optimized_embedder = None  # type: ignore

logger = logging.getLogger(__name__)


class RAGEngine:
    """
    Retrieval-Augmented Generation avec FAISS local et filtrage par esp√®ce.
    - Charge l'index FAISS pr√©sent dans RAG_INDEX_PATH (def: 'rag_index')
    - D√©duit l'esp√®ce (broiler/layer) √† partir du contexte + question
    - Appelle embedder.search(query, k, species=...) pour r√©cup√©rer des chunks filtr√©s
    - Construit un prompt RAG et appelle OpenAI
    - Fallback OpenAI si aucun document pertinent
    """

    def __init__(self, index_path: Optional[str] = None, k: int = 4) -> None:
        self.index_path = index_path or os.getenv("RAG_INDEX_PATH", "rag_index")
        self.k = int(os.getenv("RAG_TOP_K", str(k)))
        self.embedder: Optional[FastRAGEmbedder] = None
        self.rag_available = False

        # Init embedder + index
        self._init_local_rag()

    # --------------------------------------------------------------------- #
    # Initialisation FAISS local
    # --------------------------------------------------------------------- #
    def _init_local_rag(self) -> None:
        if FastRAGEmbedder is None or create_optimized_embedder is None:
            logger.error("‚ùå RAGEngine: FastRAGEmbedder indisponible (import).")
            self.rag_available = False
            return

        try:
            # Param√®tres par d√©faut align√©s avec notre embedder corrig√©
            self.embedder = create_optimized_embedder(
                model_name=os.getenv("RAG_EMBED_MODEL", "all-MiniLM-L6-v2"),
                similarity_threshold=float(os.getenv("RAG_SIM_THRESHOLD", "0.20")),
                normalize_queries=True,
                debug=True,
            )

            loaded = self.embedder.load_index(self.index_path)
            self.rag_available = bool(loaded)

            if self.rag_available:
                stats = self.embedder.get_stats()
                logger.info("‚úÖ RAGEngine: Index FAISS charg√© (%s)", self.index_path)
                logger.info("üìä RAG stats: %s", stats)
            else:
                logger.warning("‚ö†Ô∏è RAGEngine: Impossible de charger l'index FAISS (%s)", self.index_path)

        except Exception as e:
            logger.error("‚ùå RAGEngine: Erreur init embedder/index: %s", e)
            self.rag_available = False

    # --------------------------------------------------------------------- #
    # Inf√©rence de l'esp√®ce (contexte + question)
    # --------------------------------------------------------------------- #
    def _infer_species_from_context(self, question: str, context: Optional[Dict[str, Any]]) -> Optional[str]:
        """
        Essaie de d√©terminer 'broiler' ou 'layer' √† partir du contexte ET de la question.
        L'embedder poss√®de d√©j√† une inf√©rence interne, mais passer l'info ici verrouille le filtre.
        """
        # 1) Contexte structur√©
        species_ctx = (context or {}).get("species") or (context or {}).get("espece")
        if isinstance(species_ctx, str) and species_ctx.strip():
            s = species_ctx.lower()
            if "broiler" in s or "chair" in s:
                return "broiler"
            if "layer" in s or "pondeuse" in s:
                return "layer"

        breed = ((context or {}).get("breed") or (context or {}).get("race") or "").lower()
        if any(x in breed for x in ["ross", "cobb", "broiler"]):
            return "broiler"
        if any(x in breed for x in ["lohmann", "hy-line", "w36", "w-36", "w80", "w-80", "layer"]):
            return "layer"

        # 2) Question (s√©curit√©)
        q = (question or "").lower()
        if any(x in q for x in ["pondeuse", "layer", "lohmann", "hy-line", "w36", "w-36", "w80", "w-80", "ponte"]):
            return "layer"
        if any(x in q for x in ["broiler", "poulet de chair", "ross 308", "cobb 500"]):
            return "broiler"

        return None  # on laissera l‚Äôembedder deviner

    # --------------------------------------------------------------------- #
    # API publique: g√©n√©ration de r√©ponse
    # --------------------------------------------------------------------- #
    def generate_answer(self, question: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Retourne:
        {
            "response": str,
            "source": "rag_enhanced" | "openai_fallback" | "rag_error" | "error_fallback",
            "documents_used": int,
            "warning": Optional[str],
            "citations": List[Dict[str, str]]   # [{"source": ".../path/file.pdf", "snippet": "..."}]
        }
        """
        result: Dict[str, Any] = {
            "response": "",
            "source": "",
            "documents_used": 0,
            "warning": None,
            "citations": [],
        }

        # 1) Recherche documentaire locale (FAISS)
        docs: List[Dict[str, Any]] = []
        species_hint = self._infer_species_from_context(question, context)

        if self.rag_available and self.embedder is not None:
            try:
                logger.info("üîé RAGEngine: recherche locale (k=%d, species=%s)", self.k, species_hint or "auto")
                search_hits = self.embedder.search(question, k=self.k, species=species_hint)

                # search_hits est une liste de dicts: {"text", "metadata", "score", ...}
                # On la traduit en liste minimale de docs homog√®nes pour le prompt
                for hit in search_hits:
                    if not isinstance(hit, dict):
                        continue
                    text = hit.get("text") or ""
                    md = hit.get("metadata") or {}
                    source = (
                        md.get("file_path")
                        or md.get("source")
                        or md.get("path")
                        or md.get("filename")
                        or "unknown_source"
                    )
                    docs.append({"content": text, "source": str(source)})
                logger.info("‚úÖ RAGEngine: %d documents retenus", len(docs))

            except Exception as e:
                logger.error("‚ùå RAGEngine: erreur pendant la recherche locale: %s", e)
                docs = []

        else:
            logger.warning("‚ö†Ô∏è RAGEngine: FAISS local indisponible (index non charg√©).")

        # 2) G√©n√©ration: avec RAG ou fallback
        if not docs:
            # Fallback OpenAI (connaissances g√©n√©rales)
            prompt = self._build_fallback_prompt(question, context)
            try:
                resp = safe_chat_completion(
                    model=os.getenv("OPENAI_MODEL", "gpt-4o"),
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.0,
                    max_tokens=700,
                )
                result.update(
                    {
                        "response": (resp.choices[0].message.content or "").strip(),
                        "source": "openai_fallback",
                        "documents_used": 0,
                        "warning": "R√©ponse g√©n√©rale : aucun document sp√©cialis√© pertinent n'a √©t√© trouv√©.",
                        "citations": [],
                    }
                )
            except Exception as e:
                logger.error("‚ùå RAGEngine: erreur fallback OpenAI: %s", e)
                result.update(
                    {
                        "response": "Je rencontre un probl√®me technique pour r√©pondre. Veuillez r√©essayer.",
                        "source": "error_fallback",
                        "documents_used": 0,
                        "warning": f"Erreur technique: {e}",
                        "citations": [],
                    }
                )
            return result

        # RAG prompt avec docs
        prompt = self._build_rag_prompt(question, context, docs)
        try:
            resp = safe_chat_completion(
                model=os.getenv("OPENAI_MODEL", "gpt-4o"),
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=900,
            )

            result.update(
                {
                    "response": (resp.choices[0].message.content or "").strip(),
                    "source": "rag_enhanced",
                    "documents_used": len(docs),
                    "warning": None,
                    "citations": self._build_citations(docs),
                }
            )
        except Exception as e:
            logger.error("‚ùå RAGEngine: erreur OpenAI sur RAG: %s", e)
            result.update(
                {
                    "response": f"Documents trouv√©s ({len(docs)}) mais erreur de g√©n√©ration. R√©essayez plus tard.",
                    "source": "rag_error",
                    "documents_used": len(docs),
                    "warning": f"Erreur traitement RAG: {e}",
                    "citations": self._build_citations(docs),
                }
            )

        return result

    # --------------------------------------------------------------------- #
    # Prompts
    # --------------------------------------------------------------------- #
    def _build_rag_prompt(self, question: str, context: Optional[Dict[str, Any]], docs: List[Dict[str, str]]) -> str:
        """
        Construit un prompt RAG propre et orient√© 'utiliser d'abord les documents'.
        """
        doc_lines: List[str] = []
        for i, d in enumerate(docs, 1):
            content = (d.get("content") or "").strip()
            if len(content) > 600:
                content = content[:600] + "..."
            source = d.get("source") or "unknown_source"
            doc_lines.append(f"[Doc {i} | {source}]\n{content}")
        docs_block = "\n\n".join(doc_lines)

        missing_info = self._identify_missing_context(context)

        prompt = f"""Tu es un expert v√©t√©rinaire sp√©cialis√© en aviculture (poulets de chair et pondeuses).

QUESTION
{question}

CONTEXTE DISPONIBLE
{context if context else "‚Äî"}

DOCUMENTS SP√âCIALIS√âS (√† utiliser en priorit√©)
{docs_block}

CONSIGNE
1) Utilise d'abord les documents ci-dessus (ne les ignore pas).
2) Si une info manque, compl√®te prudemment par les bonnes pratiques reconnues.
3) Sois clair, pratique et pr√©cis. Mentionne les hypoth√®ses si besoin.
4) Distingue les recommandations g√©n√©rales de celles bas√©es sur les documents.

{missing_info}

R√©ponds en fran√ßais, de fa√ßon professionnelle et op√©rationnelle.
"""
        return prompt

    def _build_fallback_prompt(self, question: str, context: Optional[Dict[str, Any]]) -> str:
        missing_info = self._identify_missing_context(context)
        prompt = f"""Tu es un expert v√©t√©rinaire sp√©cialis√© en aviculture.

QUESTION
{question}

CONTEXTE DISPONIBLE
{context if context else "‚Äî"}

SITUATION
Aucun document sp√©cialis√© n'a √©t√© trouv√©/retourn√© par le moteur RAG.

CONSIGNE
1) Donne une r√©ponse g√©n√©rale, utile et prudente (bonnes pratiques standard).
2) Explique les √©ventuelles variations selon la lign√©e, le sexe, l'√¢ge, etc.
3) Propose 2-3 questions de clarification si pertinent.

{missing_info}

R√©ponds en fran√ßais et indique clairement qu'il s'agit d'une r√©ponse g√©n√©rale (sans source documentaire)."""
        return prompt

    # --------------------------------------------------------------------- #
    # Utilitaires
    # --------------------------------------------------------------------- #
    def _identify_missing_context(self, context: Optional[Dict[str, Any]]) -> str:
        ctx = context or {}
        missing = []

        if not (ctx.get("race") or ctx.get("breed")):
            missing.append("la lign√©e (Ross, Cobb, Hubbard...)")
        if not (ctx.get("sexe") or ctx.get("sex_category")):
            missing.append("le sexe (m√¢le, femelle, mixte)")
        if not (ctx.get("age_jours") or ctx.get("age_phase")):
            missing.append("l'√¢ge pr√©cis (en jours/semaine)")

        if missing:
            return (
                "INFORMATIONS MANQUANTES POUR PLUS DE PR√âCISION\n"
                f"- Donn√©es manquantes: {', '.join(missing)}.\n"
                "- Donne quand m√™me une r√©ponse utile.\n"
                "- Explique pourquoi ces infos sont importantes (ex: diff√©rences selon lign√©e/√¢ge/sexe)."
            )
        return "CONTEXTE: suffisant pour une r√©ponse pertinente."

    def _build_citations(self, docs: List[Dict[str, str]]) -> List[Dict[str, str]]:
        cites: List[Dict[str, str]] = []
        for d in docs:
            source = d.get("source") or "unknown_source"
            snippet = (d.get("content") or "").strip()[:120].replace("\n", " ")
            cites.append({"source": source, "snippet": snippet})
        return cites

    # Diagnostic/status
    def get_status(self) -> Dict[str, Any]:
        return {
            "faiss_loaded": bool(self.rag_available),
            "index_path": self.index_path,
            "top_k": self.k,
            "embedder_ok": bool(self.embedder is not None),
        }
