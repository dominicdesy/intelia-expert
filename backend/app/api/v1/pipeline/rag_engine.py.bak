"""
RAGEngine - Version 2 (FAISS local + filtrage espÃ¨ce)
- Remplace l'usage direct de VectorStoreClient par le FastRAGEmbedder local
- Passe l'espÃ¨ce (broiler/layer) dÃ©duite du contexte/la question Ã  l'embedder pour filtrer les docs
- Fallback propre vers OpenAI si aucun document pertinent
- Sortie standardisÃ©e: dict avec 'response' (str), 'source', 'documents_used', 'warning', 'citations'
"""

from __future__ import annotations

import os
import logging
from typing import Any, Dict, List, Optional

from app.api.v1.utils.openai_utils import safe_chat_completion

# âš ï¸ Importe l'embedder local (FAISS) â€” chemin 'rag/embedder.py'
try:
    # Chemin absolu vs relatif selon ta structure de projet
    from rag.embedder import FastRAGEmbedder, create_optimized_embedder
except Exception as e:
    FastRAGEmbedder = None  # type: ignore
    create_optimized_embedder = None  # type: ignore

logger = logging.getLogger(__name__)


class RAGEngine:
    """
    Retrieval-Augmented Generation avec FAISS local et filtrage par espÃ¨ce.
    - Charge l'index FAISS prÃ©sent dans RAG_INDEX_PATH (def: 'rag_index')
    - DÃ©duit l'espÃ¨ce (broiler/layer) Ã  partir du contexte + question
    - Appelle embedder.search(query, k, species=...) pour rÃ©cupÃ©rer des chunks filtrÃ©s
    - Construit un prompt RAG et appelle OpenAI
    - Fallback OpenAI si aucun document pertinent
    """

    def __init__(self, index_path: Optional[str] = None, k: int = 4) -> None:
        self.index_path = index_path or os.getenv("RAG_INDEX_PATH", "rag_index")
        self.k = int(os.getenv("RAG_TOP_K", str(k)))
        self.embedder: Optional[FastRAGEmbedder] = None
        self.rag_available = False

        # Init embedder + index
        self._init_local_rag()

    # --------------------------------------------------------------------- #
    # Initialisation FAISS local
    # --------------------------------------------------------------------- #
    def _init_local_rag(self) -> None:
        if FastRAGEmbedder is None or create_optimized_embedder is None:
            logger.error("âŒ RAGEngine: FastRAGEmbedder indisponible (import).")
            self.rag_available = False
            return

        try:
            # ParamÃ¨tres par dÃ©faut alignÃ©s avec notre embedder corrigÃ©
            self.embedder = create_optimized_embedder(
                model_name=os.getenv("RAG_EMBED_MODEL", "all-MiniLM-L6-v2"),
                similarity_threshold=float(os.getenv("RAG_SIM_THRESHOLD", "0.20")),
                normalize_queries=True,
                debug=True,
            )

            loaded = self.embedder.load_index(self.index_path)
            self.rag_available = bool(loaded)

            if self.rag_available:
                stats = self.embedder.get_stats()
                logger.info("âœ… RAGEngine: Index FAISS chargÃ© (%s)", self.index_path)
                logger.info("ðŸ“Š RAG stats: %s", stats)
            else:
                logger.warning("âš ï¸ RAGEngine: Impossible de charger l'index FAISS (%s)", self.index_path)

        except Exception as e:
            logger.error("âŒ RAGEngine: Erreur init embedder/index: %s", e)
            self.rag_available = False

    # --------------------------------------------------------------------- #
    # InfÃ©rence de l'espÃ¨ce (contexte + question)
    # --------------------------------------------------------------------- #
    def _infer_species_from_context(self, question: str, context: Optional[Dict[str, Any]]) -> Optional[str]:
        """
        Essaie de dÃ©terminer 'broiler' ou 'layer' Ã  partir du contexte ET de la question.
        L'embedder possÃ¨de dÃ©jÃ  une infÃ©rence interne, mais passer l'info ici verrouille le filtre.
        """
        # 1) Contexte structurÃ©
        species_ctx = (context or {}).get("species") or (context or {}).get("espece")
        if isinstance(species_ctx, str) and species_ctx.strip():
            s = species_ctx.lower()
            if "broiler" in s or "chair" in s:
                return "broiler"
            if "layer" in s or "pondeuse" in s:
                return "layer"

        breed = ((context or {}).get("breed") or (context or {}).get("race") or "").lower()
        if any(x in breed for x in ["ross", "cobb", "broiler"]):
            return "broiler"
        if any(x in breed for x in ["lohmann", "hy-line", "w36", "w-36", "w80", "w-80", "layer"]):
            return "layer"

        # 2) Question (sÃ©curitÃ©)
        q = (question or "").lower()
        if any(x in q for x in ["pondeuse", "layer", "lohmann", "hy-line", "w36", "w-36", "w80", "w-80", "ponte"]):
            return "layer"
        if any(x in q for x in ["broiler", "poulet de chair", "ross 308", "cobb 500"]):
            return "broiler"

        return None  # on laissera lâ€™embedder deviner

    # --------------------------------------------------------------------- #
    # API publique: gÃ©nÃ©ration de rÃ©ponse
    # --------------------------------------------------------------------- #
    def generate_answer(self, question: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Retourne:
        {
            "response": str,
            "source": "rag_enhanced" | "openai_fallback" | "rag_error" | "error_fallback",
            "documents_used": int,
            "warning": Optional[str],
            "citations": List[Dict[str, str]]   # [{"source": ".../path/file.pdf", "snippet": "..."}]
        }
        """
        result: Dict[str, Any] = {
            "response": "",
            "source": "",
            "documents_used": 0,
            "warning": None,
            "citations": [],
        }

        # 1) Recherche documentaire locale (FAISS)
        docs: List[Dict[str, Any]] = []
        species_hint = self._infer_species_from_context(question, context)

        if self.rag_available and self.embedder is not None:
            try:
                logger.info("ðŸ”Ž RAGEngine: recherche locale (k=%d, species=%s)", self.k, species_hint or "auto")
                search_hits = self.embedder.search(question, k=self.k, species=species_hint)

                # search_hits est une liste de dicts: {"text", "metadata", "score", ...}
                # On la traduit en liste minimale de docs homogÃ¨nes pour le prompt
                for hit in search_hits:
                    if not isinstance(hit, dict):
                        continue
                    text = hit.get("text") or ""
                    md = hit.get("metadata") or {}
                    source = (
                        md.get("file_path")
                        or md.get("source")
                        or md.get("path")
                        or md.get("filename")
                        or "unknown_source"
                    )
                    docs.append({"content": text, "source": str(source)})
                logger.info("âœ… RAGEngine: %d documents retenus", len(docs))

            except Exception as e:
                logger.error("âŒ RAGEngine: erreur pendant la recherche locale: %s", e)
                docs = []

        else:
            logger.warning("âš ï¸ RAGEngine: FAISS local indisponible (index non chargÃ©).")

        # 2) GÃ©nÃ©ration: avec RAG ou fallback
        if not docs:
            # Fallback OpenAI (connaissances gÃ©nÃ©rales)
            prompt = self._build_fallback_prompt(question, context)
            try:
                resp = safe_chat_completion(
                    model=os.getenv("OPENAI_MODEL", "gpt-4o"),
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.0,
                    max_tokens=700,
                )
                result.update(
                    {
                        "response": (resp.choices[0].message.content or "").strip(),
                        "source": "openai_fallback",
                        "documents_used": 0,
                        "warning": "RÃ©ponse gÃ©nÃ©rale : aucun document spÃ©cialisÃ© pertinent n'a Ã©tÃ© trouvÃ©.",
                        "citations": [],
                    }
                )
            except Exception as e:
                logger.error("âŒ RAGEngine: erreur fallback OpenAI: %s", e)
                result.update(
                    {
                        "response": "Je rencontre un problÃ¨me technique pour rÃ©pondre. Veuillez rÃ©essayer.",
                        "source": "error_fallback",
                        "documents_used": 0,
                        "warning": f"Erreur technique: {e}",
                        "citations": [],
                    }
                )
            return result

        # RAG prompt avec docs
        prompt = self._build_rag_prompt(question, context, docs)
        try:
            resp = safe_chat_completion(
                model=os.getenv("OPENAI_MODEL", "gpt-4o"),
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=900,
            )

            result.update(
                {
                    "response": (resp.choices[0].message.content or "").strip(),
                    "source": "rag_enhanced",
                    "documents_used": len(docs),
                    "warning": None,
                    "citations": self._build_citations(docs),
                }
            )
        except Exception as e:
            logger.error("âŒ RAGEngine: erreur OpenAI sur RAG: %s", e)
            result.update(
                {
                    "response": f"Documents trouvÃ©s ({len(docs)}) mais erreur de gÃ©nÃ©ration. RÃ©essayez plus tard.",
                    "source": "rag_error",
                    "documents_used": len(docs),
                    "warning": f"Erreur traitement RAG: {e}",
                    "citations": self._build_citations(docs),
                }
            )

        return result

    # --------------------------------------------------------------------- #
    # Prompts
    # --------------------------------------------------------------------- #
    def _build_rag_prompt(self, question: str, context: Optional[Dict[str, Any]], docs: List[Dict[str, str]]) -> str:
        """
        Construit un prompt RAG propre et orientÃ© 'utiliser d'abord les documents'.
        """
        doc_lines: List[str] = []
        for i, d in enumerate(docs, 1):
            content = (d.get("content") or "").strip()
            if len(content) > 600:
                content = content[:600] + "..."
            source = d.get("source") or "unknown_source"
            doc_lines.append(f"[Doc {i} | {source}]\n{content}")
        docs_block = "\n\n".join(doc_lines)

        missing_info = self._identify_missing_context(context)

        prompt = f"""Tu es un expert vÃ©tÃ©rinaire spÃ©cialisÃ© en aviculture (poulets de chair et pondeuses).

QUESTION
{question}

CONTEXTE DISPONIBLE
{context if context else "â€”"}

DOCUMENTS SPÃ‰CIALISÃ‰S (Ã  utiliser en prioritÃ©)
{docs_block}

CONSIGNE
1) Utilise d'abord les documents ci-dessus (ne les ignore pas).
2) Si une info manque, complÃ¨te prudemment par les bonnes pratiques reconnues.
3) Sois clair, pratique et prÃ©cis. Mentionne les hypothÃ¨ses si besoin.
4) Distingue les recommandations gÃ©nÃ©rales de celles basÃ©es sur les documents.

{missing_info}

RÃ©ponds en franÃ§ais, de faÃ§on professionnelle et opÃ©rationnelle.
"""
        return prompt

    def _build_fallback_prompt(self, question: str, context: Optional[Dict[str, Any]]) -> str:
        missing_info = self._identify_missing_context(context)
        prompt = f"""Tu es un expert vÃ©tÃ©rinaire spÃ©cialisÃ© en aviculture.

QUESTION
{question}

CONTEXTE DISPONIBLE
{context if context else "â€”"}

SITUATION
Aucun document spÃ©cialisÃ© n'a Ã©tÃ© trouvÃ©/retournÃ© par le moteur RAG.

CONSIGNE
1) Donne une rÃ©ponse gÃ©nÃ©rale, utile et prudente (bonnes pratiques standard).
2) Explique les Ã©ventuelles variations selon la lignÃ©e, le sexe, l'Ã¢ge, etc.
3) Propose 2-3 questions de clarification si pertinent.

{missing_info}

RÃ©ponds en franÃ§ais et indique clairement qu'il s'agit d'une rÃ©ponse gÃ©nÃ©rale (sans source documentaire)."""
        return prompt

    # --------------------------------------------------------------------- #
    # Utilitaires
    # --------------------------------------------------------------------- #
    def _identify_missing_context(self, context: Optional[Dict[str, Any]]) -> str:
        ctx = context or {}
        missing = []

        if not (ctx.get("race") or ctx.get("breed")):
            missing.append("la lignÃ©e (Ross, Cobb, Hubbard...)")
        if not (ctx.get("sexe") or ctx.get("sex_category")):
            missing.append("le sexe (mÃ¢le, femelle, mixte)")
        if not (ctx.get("age_jours") or ctx.get("age_phase")):
            missing.append("l'Ã¢ge prÃ©cis (en jours/semaine)")

        if missing:
            return (
                "INFORMATIONS MANQUANTES POUR PLUS DE PRÃ‰CISION\n"
                f"- DonnÃ©es manquantes: {', '.join(missing)}.\n"
                "- Donne quand mÃªme une rÃ©ponse utile.\n"
                "- Explique pourquoi ces infos sont importantes (ex: diffÃ©rences selon lignÃ©e/Ã¢ge/sexe)."
            )
        return "CONTEXTE: suffisant pour une rÃ©ponse pertinente."

    def _build_citations(self, docs: List[Dict[str, str]]) -> List[Dict[str, str]]:
        cites: List[Dict[str, str]] = []
        for d in docs:
            source = d.get("source") or "unknown_source"
            snippet = (d.get("content") or "").strip()[:120].replace("\n", " ")
            cites.append({"source": source, "snippet": snippet})
        return cites

    # Diagnostic/status
    def get_status(self) -> Dict[str, Any]:
        return {
            "faiss_loaded": bool(self.rag_available),
            "index_path": self.index_path,
            "top_k": self.k,
            "embedder_ok": bool(self.embedder is not None),
        }
