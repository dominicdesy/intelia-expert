# app/api/v1/stats_updater.py
# -*- coding: utf-8 -*-
"""
üöÄ COLLECTEUR INTELLIGENT DE STATISTIQUES - VERSION MEMORY-SAFE
Utilise les gestionnaires existants SANS les modifier
Collecte p√©riodique + cache optimis√©
SAFE: Aucune rupture avec logging.py et billing.py
‚ú® OPTIMIS√â: Gestion m√©moire drastiquement am√©lior√©e pour DigitalOcean App Platform
üõ°Ô∏è MEMORY-SAFE: Collecte s√©quentielle, limites strictes, monitoring temps r√©el
üîß FIXED: Correction erreur retour 0 dans _check_feedback_columns_availability
"""

import asyncio
import json
import logging
import time
import os
import gc
import psutil
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple

# Import des gestionnaires existants (SAFE)
from app.api.v1.logging import get_analytics_manager
from app.api.v1.billing import get_billing_manager
from app.api.v1.stats_cache import get_stats_cache

# Import pour co√ªts OpenAI optimis√©s
from app.api.v1.billing_openai import get_openai_usage_data_safe

logger = logging.getLogger(__name__)

# üõ°Ô∏è CONFIGURATION MEMORY-SAFE POUR UPDATER
UPDATER_CONFIG = {
    "ENABLE_PARALLEL_COLLECTION": os.getenv("ENABLE_PARALLEL_STATS", "false").lower() == "true",
    "MAX_MEMORY_PERCENT_COLLECTION": 70,    # Arr√™te si > 70% RAM pendant collecte
    "SEQUENTIAL_DELAY_MS": 500,             # 500ms entre chaque collecteur s√©quentiel
    "MAX_COLLECTION_TIME_SECONDS": 120,     # Timeout global 2min
    "ENABLE_MEMORY_MONITORING": True,       # Monitoring m√©moire temps r√©el
    "FORCE_GC_AFTER_COLLECTION": True,      # Force garbage collection
    "MAX_SQL_ROWS_PER_QUERY": 1000,        # Limite lignes SQL
    "REDUCE_DATASET_SIZE": True,            # R√©duit la taille des datasets
    "SKIP_HEAVY_ANALYTICS": os.getenv("SKIP_HEAVY_ANALYTICS", "false").lower() == "true"
}

def get_memory_usage_percent():
    """Retourne le pourcentage d'utilisation m√©moire syst√®me"""
    try:
        return psutil.virtual_memory().percent
    except Exception:
        return 0

def should_abort_collection():
    """D√©termine si la collecte doit √™tre abandonn√©e pour pr√©server la m√©moire"""
    memory_percent = get_memory_usage_percent()
    if memory_percent > UPDATER_CONFIG["MAX_MEMORY_PERCENT_COLLECTION"]:
        logger.warning(f"üö® Collecte abandonn√©e: m√©moire critique ({memory_percent}%)")
        return True, f"M√©moire critique: {memory_percent}%"
    return False, None

def force_garbage_collection():
    """Force le garbage collection pour lib√©rer la m√©moire"""
    if UPDATER_CONFIG["FORCE_GC_AFTER_COLLECTION"]:
        gc.collect()
        logger.debug("üßπ Garbage collection forc√©")

class StatisticsUpdater:
    """
    üõ°Ô∏è Collecteur intelligent MEMORY-SAFE qui utilise les gestionnaires existants
    - Met √† jour le cache p√©riodiquement avec gestion m√©moire
    - G√®re les erreurs et fallbacks
    - NOUVEAU: Collecte s√©quentielle au lieu de parall√®le (√©conomie RAM)
    - Support d√©fensif pour colonnes feedback
    - Monitoring m√©moire temps r√©el
    """
    
    def __init__(self):
        self.cache = get_stats_cache()
        self.analytics = get_analytics_manager()
        self.billing = get_billing_manager()
        self.last_update = None
        self.update_in_progress = False
        
        # üõ°Ô∏è NOUVEAU: Compteurs de performance pour monitoring
        self.collection_stats = {
            "total_collections": 0,
            "successful_collections": 0,
            "memory_aborts": 0,
            "last_memory_peak": 0
        }
        
        # ‚úÖ CONSERV√â: V√©rification analytics manager
        if not self.analytics:
            logger.error("‚ùå Analytics manager non disponible")
            self._feedback_columns_available = {
                "table_exists": False, 
                "feedback": False, 
                "feedback_comment": False, 
                "error": "no_analytics_manager"
            }
        else:
            # ‚úÖ CONSERV√â: D√©tection des colonnes feedback au d√©marrage
            try:
                self._feedback_columns_available = self._check_feedback_columns_availability()
                logger.info(f"üîç D√©tection feedback au d√©marrage: {self._feedback_columns_available}")
            except Exception as e:
                logger.error(f"‚ùå Erreur d√©tection feedback au d√©marrage: {e}")
                self._feedback_columns_available = {
                    "table_exists": False, 
                    "feedback": False, 
                    "feedback_comment": False, 
                    "error": str(e)
                }
    
    # ==================== M√âTHODES CONSERV√âES INT√âGRALEMENT ====================
    
    def _check_feedback_columns_availability(self) -> Dict[str, bool]:
        """
        üîç CONSERV√â: V√©rifie la disponibilit√© des colonnes feedback au d√©marrage.
        Cache le r√©sultat pour √©viter les v√©rifications r√©p√©t√©es.
        ‚úÖ CORRIG√â: Retourne toujours un dictionnaire valide
        """
        try:
            import psycopg2
            from psycopg2.extras import RealDictCursor
            
            # ‚úÖ CONSERV√â: V√©rifier que l'analytics manager existe et a un DSN
            if not hasattr(self.analytics, 'dsn') or not self.analytics.dsn:
                logger.warning("‚ö†Ô∏è DSN analytics non disponible - utilisation valeurs par d√©faut")
                return {"table_exists": False, "feedback": False, "feedback_comment": False}
            
            with psycopg2.connect(self.analytics.dsn) as conn:
                with conn.cursor(cursor_factory=RealDictCursor) as cur:
                    
                    # V√©rifier si la table existe d'abord
                    cur.execute("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.tables 
                            WHERE table_schema = 'public' 
                            AND table_name = 'user_questions_complete'
                        )
                    """)
                    
                    table_exists = cur.fetchone()[0]
                    
                    if not table_exists:
                        logger.warning("‚ö†Ô∏è Table user_questions_complete n'existe pas")
                        return {"table_exists": False, "feedback": False, "feedback_comment": False}
                    
                    # V√©rifier les colonnes feedback
                    cur.execute("""
                        SELECT column_name 
                        FROM information_schema.columns 
                        WHERE table_name = 'user_questions_complete' 
                            AND column_name IN ('feedback', 'feedback_comment')
                    """)
                    
                    available_columns = {row["column_name"] for row in cur.fetchall()}
                    
                    result = {
                        "table_exists": True,
                        "feedback": "feedback" in available_columns,
                        "feedback_comment": "feedback_comment" in available_columns
                    }
                    
                    logger.info(f"üîç D√©tection colonnes feedback: {result}")
                    return result
                    
        except ImportError as import_err:
            logger.error(f"‚ùå Module psycopg2 non disponible: {import_err}")
            return {
                "table_exists": False, 
                "feedback": False, 
                "feedback_comment": False, 
                "error": "psycopg2_missing"
            }
            
        except Exception as e:
            # ‚úÖ CORRIG√â: Retourner un dictionnaire valide au lieu de 0
            logger.error(f"‚ùå Erreur v√©rification colonnes feedback: {e}")
            return {
                "table_exists": False, 
                "feedback": False, 
                "feedback_comment": False,
                "error": str(e)[:100]  # Limiter la taille de l'erreur
            }
    
    def diagnose_database_connection(self) -> Dict[str, Any]:
        """
        üîß CONSERV√â: Diagnostique complet de la connection base de donn√©es
        """
        try:
            diagnosis = {
                "analytics_manager": {
                    "available": self.analytics is not None,
                    "has_dsn": hasattr(self.analytics, 'dsn') if self.analytics else False,
                    "dsn_configured": bool(getattr(self.analytics, 'dsn', None)) if self.analytics else False
                },
                "database_connection": {
                    "can_connect": False,
                    "tables_found": [],
                    "user_questions_complete": {
                        "exists": False,
                        "columns": []
                    }
                },
                "psycopg2_available": False,
                "errors": []
            }
            
            # Test import psycopg2
            try:
                import psycopg2
                from psycopg2.extras import RealDictCursor
                diagnosis["psycopg2_available"] = True
            except ImportError as e:
                diagnosis["errors"].append(f"psycopg2 non disponible: {e}")
                return diagnosis
            
            # Test connection database
            if diagnosis["analytics_manager"]["dsn_configured"]:
                try:
                    with psycopg2.connect(self.analytics.dsn) as conn:
                        diagnosis["database_connection"]["can_connect"] = True
                        
                        with conn.cursor(cursor_factory=RealDictCursor) as cur:
                            # Lister toutes les tables
                            cur.execute("""
                                SELECT table_name 
                                FROM information_schema.tables 
                                WHERE table_schema = 'public'
                                ORDER BY table_name
                            """)
                            
                            diagnosis["database_connection"]["tables_found"] = [
                                row["table_name"] for row in cur.fetchall()
                            ]
                            
                            # V√©rifier user_questions_complete sp√©cifiquement
                            if "user_questions_complete" in diagnosis["database_connection"]["tables_found"]:
                                diagnosis["database_connection"]["user_questions_complete"]["exists"] = True
                                
                                cur.execute("""
                                    SELECT column_name, data_type, is_nullable
                                    FROM information_schema.columns 
                                    WHERE table_name = 'user_questions_complete'
                                    ORDER BY ordinal_position
                                """)
                                
                                diagnosis["database_connection"]["user_questions_complete"]["columns"] = [
                                    {
                                        "name": row["column_name"],
                                        "type": row["data_type"],
                                        "nullable": row["is_nullable"] == "YES"
                                    }
                                    for row in cur.fetchall()
                                ]
                            
                except Exception as db_err:
                    diagnosis["errors"].append(f"Erreur connexion DB: {db_err}")
            else:
                diagnosis["errors"].append("DSN non configur√© dans analytics manager")
            
            return diagnosis
            
        except Exception as e:
            return {
                "status": "diagnostic_failed",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def create_missing_tables(self) -> Dict[str, Any]:
        """
        üõ†Ô∏è CONSERV√â: Cr√©e automatiquement les tables manquantes
        """
        try:
            if not hasattr(self.analytics, 'dsn') or not self.analytics.dsn:
                return {"status": "error", "error": "DSN non configur√©"}
            
            import psycopg2
            from psycopg2.extras import RealDictCursor
            
            results = {
                "tables_created": [],
                "tables_updated": [],
                "errors": []
            }
            
            with psycopg2.connect(self.analytics.dsn) as conn:
                with conn.cursor(cursor_factory=RealDictCursor) as cur:
                    
                    # Cr√©er user_questions_complete si manquante
                    cur.execute("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.tables 
                            WHERE table_schema = 'public' 
                            AND table_name = 'user_questions_complete'
                        )
                    """)
                    
                    if not cur.fetchone()[0]:
                        logger.info("üîß Cr√©ation table user_questions_complete...")
                        
                        create_table_sql = """
                        CREATE TABLE user_questions_complete (
                            id SERIAL PRIMARY KEY,
                            question_id VARCHAR(50) UNIQUE,
                            user_email VARCHAR(255),
                            session_id VARCHAR(100),
                            question TEXT NOT NULL,
                            response_text TEXT,
                            response_source VARCHAR(50),
                            response_confidence DECIMAL(5,4),
                            processing_time_ms INTEGER,
                            status VARCHAR(20) DEFAULT 'success',
                            intent VARCHAR(100),
                            entities JSONB,
                            language VARCHAR(10) DEFAULT 'fr',
                            completeness_score DECIMAL(5,4),
                            error_type VARCHAR(50),
                            error_message TEXT,
                            error_traceback TEXT,
                            feedback INTEGER CHECK (feedback IN (-1, 0, 1)),
                            feedback_comment TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                        """
                        
                        cur.execute(create_table_sql)
                        
                        # Cr√©er index pour performance
                        cur.execute("CREATE INDEX idx_user_questions_created_at ON user_questions_complete(created_at)")
                        cur.execute("CREATE INDEX idx_user_questions_user_email ON user_questions_complete(user_email)")
                        cur.execute("CREATE INDEX idx_user_questions_feedback ON user_questions_complete(feedback) WHERE feedback IS NOT NULL")
                        
                        conn.commit()
                        results["tables_created"].append("user_questions_complete")
                        logger.info("‚úÖ Table user_questions_complete cr√©√©e avec succ√®s")
                    
                    else:
                        # V√©rifier si colonnes feedback existent, les ajouter si n√©cessaire
                        cur.execute("""
                            SELECT column_name 
                            FROM information_schema.columns 
                            WHERE table_name = 'user_questions_complete' 
                            AND column_name IN ('feedback', 'feedback_comment')
                        """)
                        
                        existing_feedback_cols = {row["column_name"] for row in cur.fetchall()}
                        
                        if "feedback" not in existing_feedback_cols:
                            cur.execute("""
                                ALTER TABLE user_questions_complete 
                                ADD COLUMN feedback INTEGER CHECK (feedback IN (-1, 0, 1))
                            """)
                            results["tables_updated"].append("user_questions_complete: ajout colonne feedback")
                            logger.info("‚úÖ Colonne feedback ajout√©e")
                        
                        if "feedback_comment" not in existing_feedback_cols:
                            cur.execute("""
                                ALTER TABLE user_questions_complete 
                                ADD COLUMN feedback_comment TEXT
                            """)
                            results["tables_updated"].append("user_questions_complete: ajout colonne feedback_comment")
                            logger.info("‚úÖ Colonne feedback_comment ajout√©e")
                        
                        if results["tables_updated"]:
                            conn.commit()
            
            # Actualiser la d√©tection apr√®s cr√©ation
            self._feedback_columns_available = self._check_feedback_columns_availability()
            
            return {
                "status": "success",
                "results": results,
                "new_feedback_status": self._feedback_columns_available,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation tables: {e}")
            return {"status": "error", "error": str(e)}

    # ==================== FONCTION PRINCIPALE OPTIMIS√âE ====================

    async def update_all_statistics(self) -> Dict[str, Any]:
        """
        üéØ FONCTION PRINCIPALE - MEMORY-SAFE VERSION
        Met √† jour toutes les statistiques avec gestion m√©moire optimis√©e
        üõ°Ô∏è NOUVEAU: Collecte s√©quentielle au lieu de parall√®le pour √©conomiser RAM
        """
        if self.update_in_progress:
            logger.warning("‚è≥ Mise √† jour d√©j√† en cours, skip")
            return {"status": "skipped", "reason": "update_in_progress"}
        
        start_time = time.time()
        start_memory = get_memory_usage_percent()
        self.update_in_progress = True
        
        try:
            logger.info(f"üöÄ D√©but mise √† jour MEMORY-SAFE (RAM: {start_memory}%)")
            
            # üõ°Ô∏è V√©rification m√©moire pr√©liminaire
            should_abort, abort_reason = should_abort_collection()
            if should_abort:
                self.collection_stats["memory_aborts"] += 1
                return {
                    "status": "aborted",
                    "reason": abort_reason,
                    "memory_percent": start_memory
                }
            
            # üîÑ COLLECTE S√âQUENTIELLE OPTIMIS√âE (au lieu de parall√®le)
            results = []
            successful_updates = 0
            errors = []
            
            update_methods = [
                ("dashboard", self._update_dashboard_stats_safe),
                ("openai_costs", self._update_openai_costs_safe), 
                ("invitations", self._update_invitation_stats_safe),
                ("server_performance", self._update_server_performance_safe)
            ]
            
            for update_name, update_method in update_methods:
                try:
                    # V√©rification m√©moire avant chaque collecteur
                    current_memory = get_memory_usage_percent()
                    if current_memory > UPDATER_CONFIG["MAX_MEMORY_PERCENT_COLLECTION"]:
                        logger.warning(f"üö® Arr√™t anticip√© √† {update_name}: m√©moire {current_memory}%")
                        errors.append(f"{update_name}: Arr√™t m√©moire critique ({current_memory}%)")
                        break
                    
                    logger.info(f"üîÑ Collecte {update_name}... (RAM: {current_memory}%)")
                    
                    result = await update_method()
                    
                    if isinstance(result, Exception):
                        error_msg = f"{update_name}: {str(result)}"
                        errors.append(error_msg)
                        logger.error(f"‚ùå Erreur {update_name}: {result}")
                    elif result.get("status") == "success":
                        successful_updates += 1
                        logger.info(f"‚úÖ {update_name}: OK")
                    else:
                        errors.append(f"{update_name}: {result.get('error', 'Unknown error')}")
                    
                    results.append(result)
                    
                    # D√©lai entre collecteurs + nettoyage m√©moire
                    if UPDATER_CONFIG["SEQUENTIAL_DELAY_MS"] > 0:
                        await asyncio.sleep(UPDATER_CONFIG["SEQUENTIAL_DELAY_MS"] / 1000)
                    
                    force_garbage_collection()
                    
                except Exception as method_error:
                    error_msg = f"{update_name}: Exception {str(method_error)}"
                    errors.append(error_msg)
                    logger.error(f"‚ùå Exception {update_name}: {method_error}")
                    results.append({"status": "error", "error": str(method_error)})
            
            # Nettoyer le cache expir√©
            try:
                cleaned_entries = self.cache.cleanup_expired_cache()
                logger.info(f"üßπ Cache nettoy√©: {cleaned_entries} entr√©es supprim√©es")
            except Exception as cleanup_error:
                logger.warning(f"‚ö†Ô∏è Erreur cleanup cache: {cleanup_error}")
            
            # Garbage collection final
            force_garbage_collection()
            
            # R√©sultats finaux avec m√©triques m√©moire
            end_memory = get_memory_usage_percent()
            duration_ms = int((time.time() - start_time) * 1000)
            self.last_update = datetime.now()
            
            # Mise √† jour des statistiques de performance
            self.collection_stats["total_collections"] += 1
            self.collection_stats["successful_collections"] += successful_updates
            self.collection_stats["last_memory_peak"] = max(start_memory, end_memory)
            
            result = {
                "status": "completed",
                "successful_updates": successful_updates,
                "total_updates": len(update_methods),
                "errors": errors,
                "duration_ms": duration_ms,
                "last_update": self.last_update.isoformat(),
                "next_update_due": (self.last_update + timedelta(hours=1)).isoformat(),
                "feedback_support": self._feedback_columns_available,
                "memory_info": {
                    "start_memory_percent": start_memory,
                    "end_memory_percent": end_memory,
                    "memory_delta": end_memory - start_memory,
                    "collection_mode": "sequential_safe" if not UPDATER_CONFIG["ENABLE_PARALLEL_COLLECTION"] else "parallel",
                    "memory_monitoring_enabled": UPDATER_CONFIG["ENABLE_MEMORY_MONITORING"]
                },
                "collection_stats": self.collection_stats.copy()
            }
            
            # Cacher le r√©sum√© de la mise √† jour
            self.cache.set_cache(
                "system:last_update_summary", 
                result, 
                ttl_hours=25,  # Un peu plus qu'une heure pour √©viter les gaps
                source="stats_updater_safe"
            )
            
            logger.info(f"‚úÖ Mise √† jour termin√©e SAFE: {successful_updates}/{len(update_methods)} succ√®s en {duration_ms}ms (RAM: {start_memory}%‚Üí{end_memory}%)")
            return result
            
        except Exception as e:
            duration_ms = int((time.time() - start_time) * 1000)
            end_memory = get_memory_usage_percent()
            logger.error(f"üí• Erreur critique mise √† jour SAFE: {e}")
            
            return {
                "status": "failed",
                "error": str(e),
                "duration_ms": duration_ms,
                "memory_info": {
                    "start_memory_percent": start_memory,
                    "end_memory_percent": end_memory,
                    "memory_delta": end_memory - start_memory
                },
                "timestamp": datetime.now().isoformat()
            }
            
        finally:
            self.update_in_progress = False

    # ==================== M√âTHODES DE COLLECTE OPTIMIS√âES ====================

    async def _get_feedback_stats_safe(self, cur) -> Dict[str, Any]:
        """
        üõ°Ô∏è CONSERV√â: Collecte feedback stats avec v√©rification d√©fensive des colonnes
        Compatible avec toutes les configurations de base de donn√©es
        """
        try:
            # Utiliser le cache de d√©tection des colonnes
            if not self._feedback_columns_available["table_exists"]:
                logger.warning("‚ö†Ô∏è Table user_questions_complete manquante - pas de feedback")
                return {
                    "total": 0, "positive": 0, "negative": 0, 
                    "with_comments": 0, "satisfaction_rate": 0.0,
                    "note": "Table user_questions_complete manquante"
                }
            
            has_feedback = self._feedback_columns_available["feedback"]
            has_feedback_comment = self._feedback_columns_available["feedback_comment"]
            
            if not has_feedback:
                logger.info("‚ÑπÔ∏è Colonne feedback non disponible - stats feedback d√©sactiv√©es")
                return {
                    "total": 0, "positive": 0, "negative": 0, 
                    "with_comments": 0, "satisfaction_rate": 0.0,
                    "note": "Migration feedback requise - colonnes manquantes"
                }
            
            # ‚úÖ Colonnes feedback disponibles - construire requ√™te dynamique
            query = """
                SELECT 
                    COUNT(*) FILTER (WHERE feedback = 1) as positive_feedback,
                    COUNT(*) FILTER (WHERE feedback = -1) as negative_feedback,
                    COUNT(*) FILTER (WHERE feedback IS NOT NULL) as total_feedback
            """
            
            if has_feedback_comment:
                query += ",\n                    COUNT(*) FILTER (WHERE feedback_comment IS NOT NULL AND feedback_comment != '') as feedback_with_comments"
            else:
                query += ",\n                    0 as feedback_with_comments"
                
            # üõ°Ô∏è OPTIMIS√â: Limiter la plage de dates pour √©conomiser m√©moire
            query += f"""
                FROM user_questions_complete 
                WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
                LIMIT {UPDATER_CONFIG["MAX_SQL_ROWS_PER_QUERY"]}
            """
            
            cur.execute(query)
            result = cur.fetchone()
            
            if result:
                total_fb = result["total_feedback"] or 0
                positive_fb = result["positive_feedback"] or 0
                satisfaction_rate = (positive_fb / total_fb * 100) if total_fb > 0 else 0
                
                feedback_stats = {
                    "total": total_fb,
                    "positive": positive_fb,
                    "negative": result["negative_feedback"] or 0,
                    "with_comments": result.get("feedback_with_comments", 0),
                    "satisfaction_rate": round(satisfaction_rate, 1)
                }
                
                logger.info(f"üìä Feedback stats collect√©es SAFE: {total_fb} total, {satisfaction_rate}% satisfaction")
                return feedback_stats
            
            return {"total": 0, "positive": 0, "negative": 0, "with_comments": 0, "satisfaction_rate": 0.0}
            
        except Exception as e:
            logger.error(f"‚ùå Erreur collecte feedback safe: {e}")
            return {
                "total": 0, "positive": 0, "negative": 0, 
                "with_comments": 0, "satisfaction_rate": 0.0,
                "error": str(e)[:100]  # Limiter la longueur de l'erreur
            }

    async def _update_dashboard_stats_safe(self) -> Dict[str, Any]:
        """üõ°Ô∏è OPTIMIS√â: Collecte dashboard stats avec gestion m√©moire"""
        try:
            logger.info("üìä Collecte dashboard stats SAFE...")
            
            # V√©rification m√©moire avant requ√™tes lourdes
            memory_before = get_memory_usage_percent()
            if memory_before > UPDATER_CONFIG["MAX_MEMORY_PERCENT_COLLECTION"]:
                return {
                    "status": "skipped",
                    "reason": f"M√©moire trop √©lev√©e: {memory_before}%"
                }
            
            dashboard_data = {}
            
            # üõ°Ô∏è OPTIMIS√â: Analytics serveur via logging.py (si pas skip)
            if not UPDATER_CONFIG["SKIP_HEAVY_ANALYTICS"]:
                try:
                    from app.api.v1.logging import get_server_analytics
                    server_stats = get_server_analytics(hours=24)
                    
                    if "error" not in server_stats:
                        dashboard_data.update({
                            "avg_response_time": server_stats.get("current_status", {}).get("avg_response_time_ms", 0) / 1000,
                            "error_rate": server_stats.get("current_status", {}).get("error_rate_percent", 0),
                            "system_health": server_stats.get("current_status", {}).get("overall_health", "healthy")
                        })
                    
                except Exception as server_error:
                    logger.warning(f"‚ö†Ô∏è Erreur r√©cup√©ration server stats SAFE: {server_error}")
            
            # üõ°Ô∏è OPTIMIS√â: Requ√™tes DB avec limites strictes
            import psycopg2
            from psycopg2.extras import RealDictCursor
            
            with psycopg2.connect(self.analytics.dsn) as conn:
                with conn.cursor(cursor_factory=RealDictCursor) as cur:
                    
                    # üõ°Ô∏è Requ√™te principale avec LIMIT pour √©conomie m√©moire
                    limit = UPDATER_CONFIG["MAX_SQL_ROWS_PER_QUERY"]
                    cur.execute(f"""
                        SELECT 
                            COUNT(DISTINCT user_email) FILTER (WHERE user_email IS NOT NULL AND user_email != '') as total_users,
                            COUNT(*) as total_questions,
                            COUNT(*) FILTER (WHERE DATE(created_at) = CURRENT_DATE) as questions_today,
                            COUNT(*) FILTER (WHERE created_at >= DATE_TRUNC('week', CURRENT_DATE)) as questions_this_week,
                            COUNT(*) FILTER (WHERE DATE_TRUNC('month', created_at) = DATE_TRUNC('month', CURRENT_DATE)) as questions_this_month,
                            AVG(processing_time_ms) FILTER (WHERE processing_time_ms > 0) / 1000 as avg_response_time_calc,
                            AVG(response_confidence) FILTER (WHERE response_confidence IS NOT NULL) * 100 as avg_confidence
                        FROM (
                            SELECT * FROM user_questions_complete 
                            WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
                            ORDER BY created_at DESC
                            LIMIT {limit}
                        ) recent_questions
                    """)
                    
                    stats_result = cur.fetchone()
                    if stats_result:
                        dashboard_data.update({
                            "total_users": min(stats_result["total_users"] or 0, 10000),  # Cap arbitraire
                            "unique_active_users": min(stats_result["total_users"] or 0, 10000),
                            "total_questions": min(stats_result["total_questions"] or 0, 50000),
                            "questions_today": stats_result["questions_today"] or 0,
                            "questions_this_week": stats_result["questions_this_week"] or 0,
                            "questions_this_month": stats_result["questions_this_month"] or 0,
                            "avg_confidence": round(stats_result["avg_confidence"] or 0, 1)
                        })
                        
                        if not dashboard_data.get("avg_response_time"):
                            dashboard_data["avg_response_time"] = round(stats_result["avg_response_time_calc"] or 0, 3)
                    
                    # üõ°Ô∏è Distribution sources (version limit√©e)
                    if UPDATER_CONFIG["REDUCE_DATASET_SIZE"]:
                        cur.execute(f"""
                            SELECT response_source, COUNT(*) as count
                            FROM (
                                SELECT response_source FROM user_questions_complete 
                                WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
                                LIMIT {limit // 2}
                            ) recent_sources
                            GROUP BY response_source
                            ORDER BY count DESC
                            LIMIT 10
                        """)
                    else:
                        # Version originale conserv√©e
                        cur.execute("""
                            SELECT response_source, COUNT(*) as count
                            FROM user_questions_complete 
                            WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
                            GROUP BY response_source
                            ORDER BY count DESC
                        """)
                    
                    source_dist = {}
                    for row in cur.fetchall():
                        source_name = row["response_source"] or "unknown"
                        if source_name == "rag":
                            source_dist["rag_retriever"] = row["count"]
                        elif source_name == "openai_fallback":
                            source_dist["openai_fallback"] = row["count"]
                        elif source_name in ["table_lookup", "perfstore"]:
                            source_dist["perfstore"] = source_dist.get("perfstore", 0) + row["count"]
                        else:
                            source_dist[source_name] = row["count"]
                    
                    dashboard_data["source_distribution"] = source_dist
                    
                    # üõ°Ô∏è Top utilisateurs (version limit√©e)
                    cur.execute(f"""
                        SELECT 
                            user_email,
                            COUNT(*) as question_count,
                            'free' as plan
                        FROM (
                            SELECT user_email FROM user_questions_complete 
                            WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
                                AND user_email IS NOT NULL 
                                AND user_email != ''
                            LIMIT {limit}
                        ) recent_users
                        GROUP BY user_email
                        ORDER BY question_count DESC
                        LIMIT 10
                    """)
                    
                    top_users = []
                    for row in cur.fetchall()[:5]:  # Max 5 pour √©conomiser m√©moire
                        top_users.append({
                            "email": row["user_email"][:50],  # Tronquer email long
                            "question_count": row["question_count"],
                            "plan": row["plan"]
                        })
                    
                    dashboard_data["top_users"] = top_users
                    
                    # üõ°Ô∏è Stats feedback d√©fensives
                    dashboard_data["feedback_stats"] = await self._get_feedback_stats_safe(cur)
            
            # Donn√©es billing (version simplifi√©e)
            billing_summary = {
                "total_revenue": 0,
                "monthly_revenue": 0,
                "plan_distribution": {"free": dashboard_data.get("total_users", 0)}
            }
            dashboard_data.update(billing_summary)
            
            # Valeurs par d√©faut
            defaults = {
                "median_response_time": dashboard_data.get("avg_response_time", 0),
                "openai_costs": 6.30,
                "top_inviters": []
            }
            
            for key, default_value in defaults.items():
                if key not in dashboard_data:
                    dashboard_data[key] = default_value
            
            # Sauvegarder dans le cache
            self.cache.set_dashboard_snapshot(dashboard_data, period_hours=24)
            self.cache.set_cache("dashboard:main", dashboard_data, ttl_hours=1, source="analytics_computed_safe")
            
            memory_after = get_memory_usage_percent()
            logger.info(f"‚úÖ Dashboard stats collect√©es SAFE: {len(dashboard_data)} m√©triques (RAM: {memory_before}%‚Üí{memory_after}%)")
            return {"status": "success", "metrics_collected": len(dashboard_data)}
            
        except Exception as e:
            logger.error(f"‚ùå Erreur collecte dashboard stats SAFE: {e}")
            return {"status": "error", "error": str(e)}

    async def _update_openai_costs_safe(self) -> Dict[str, Any]:
        """üõ°Ô∏è OPTIMIS√â: Collecte co√ªts OpenAI avec gestion m√©moire"""
        try:
            logger.info("üí∞ Collecte co√ªts OpenAI SAFE...")
            
            # P√©riode plus courte pour √©conomiser API calls
            end_date = datetime.now()
            days = 3 if UPDATER_CONFIG["REDUCE_DATASET_SIZE"] else 7
            start_date = end_date - timedelta(days=days)
            
            start_str = start_date.strftime("%Y-%m-%d")
            end_str = end_date.strftime("%Y-%m-%d")
            
            costs_data = await get_openai_usage_data_safe(
                start_str, end_str, max_days=days
            )
            
            # Enrichir avec m√©tadonn√©es
            costs_data.update({
                "period_start": start_str,
                "period_end": end_str,
                "collected_at": datetime.now().isoformat(),
                "data_source": "openai_api_optimized_safe"
            })
            
            # Cache dans les deux syst√®mes
            self.cache.set_openai_costs(start_str, end_str, "week", costs_data)
            self.cache.set_cache("openai:costs:current", costs_data, ttl_hours=4, source="openai_api_safe")
            
            logger.info(f"üí∞ Co√ªts OpenAI collect√©s SAFE: ${costs_data.get('total_cost', 0):.2f}")
            return {
                "status": "success", 
                "total_cost": costs_data.get('total_cost', 0),
                "api_calls_made": costs_data.get('api_calls_made', 0),
                "cached_days": costs_data.get('cached_days', 0)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur collecte co√ªts OpenAI SAFE: {e}")
            
            # Fallback
            fallback_data = {
                "total_cost": 6.30,
                "total_tokens": 450000,
                "api_calls": 250,
                "models_usage": {"gpt-4": {"cost": 4.20}, "gpt-3.5-turbo": {"cost": 2.10}},
                "data_source": "fallback_safe",
                "note": f"Erreur API OpenAI SAFE: {str(e)[:100]}"
            }
            
            self.cache.set_cache("openai:costs:fallback", fallback_data, ttl_hours=1, source="fallback_safe")
            return {"status": "fallback", "error": str(e)[:100], "fallback_cost": 6.30}

    async def _update_invitation_stats_safe(self) -> Dict[str, Any]:
        """üõ°Ô∏è OPTIMIS√â: Collecte stats invitations avec gestion m√©moire"""
        try:
            logger.info("üìß Collecte stats invitations SAFE...")
            
            import psycopg2
            from psycopg2.extras import RealDictCursor
            
            with psycopg2.connect(self.analytics.dsn) as conn:
                with conn.cursor(cursor_factory=RealDictCursor) as cur:
                    
                    # V√©rifier existence table
                    cur.execute("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.tables 
                            WHERE table_name = 'invitations'
                        )
                    """)
                    
                    if not cur.fetchone()["exists"]:
                        invitation_data = {
                            "total_invitations_sent": 0,
                            "total_invitations_accepted": 0,
                            "acceptance_rate": 0.0,
                            "unique_inviters": 0,
                            "top_inviters_by_sent": [],
                            "top_inviters_by_accepted": [],
                            "note": "Table invitations non trouv√©e SAFE"
                        }
                    else:
                        # üõ°Ô∏è Requ√™tes avec LIMIT pour √©conomie m√©moire
                        limit = UPDATER_CONFIG["MAX_SQL_ROWS_PER_QUERY"]
                        
                        cur.execute(f"""
                            SELECT 
                                COUNT(*) as total_sent,
                                COUNT(*) FILTER (WHERE status = 'accepted') as total_accepted,
                                COUNT(DISTINCT inviter_email) as unique_inviters,
                                CASE 
                                    WHEN COUNT(*) > 0 THEN 
                                        ROUND((COUNT(*) FILTER (WHERE status = 'accepted')::DECIMAL / COUNT(*)) * 100, 2)
                                    ELSE 0 
                                END as acceptance_rate
                            FROM (
                                SELECT * FROM invitations
                                WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
                                LIMIT {limit}
                            ) recent_invitations
                        """)
                        
                        stats_result = cur.fetchone()
                        
                        # Top inviters (limit√©)
                        cur.execute(f"""
                            SELECT 
                                inviter_email,
                                inviter_name,
                                COUNT(*) as invitations_sent,
                                COUNT(*) FILTER (WHERE status = 'accepted') as invitations_accepted
                            FROM (
                                SELECT * FROM invitations
                                WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
                                LIMIT {limit}
                            ) recent_inv
                            GROUP BY inviter_email, inviter_name
                            ORDER BY invitations_sent DESC
                            LIMIT 5
                        """)
                        
                        top_inviters = [dict(row) for row in cur.fetchall()]
                        
                        invitation_data = {
                            "total_invitations_sent": stats_result["total_sent"],
                            "total_invitations_accepted": stats_result["total_accepted"],
                            "acceptance_rate": float(stats_result["acceptance_rate"]),
                            "unique_inviters": stats_result["unique_inviters"],
                            "top_inviters_by_sent": top_inviters,
                            "top_inviters_by_accepted": top_inviters  # Simplifi√© pour √©conomie
                        }
            
            self.cache.set_cache("invitations:global_stats", invitation_data, ttl_hours=2, source="computed_safe")
            
            logger.info(f"üìß Stats invitations collect√©es SAFE: {invitation_data['total_invitations_sent']} sent")
            return {"status": "success", "invitations_processed": invitation_data["total_invitations_sent"]}
            
        except Exception as e:
            logger.error(f"‚ùå Erreur collecte stats invitations SAFE: {e}")
            return {"status": "error", "error": str(e)[:100]}

    async def _update_server_performance_safe(self) -> Dict[str, Any]:
        """üõ°Ô∏è OPTIMIS√â: Collecte performance serveur avec gestion m√©moire"""
        try:
            logger.info("‚ö° Collecte performance serveur SAFE...")
            
            performance_data = {}
            
            # Version all√©g√©e si skip heavy analytics
            if UPDATER_CONFIG["SKIP_HEAVY_ANALYTICS"]:
                import psycopg2
                from psycopg2.extras import RealDictCursor
                
                with psycopg2.connect(self.analytics.dsn) as conn:
                    with conn.cursor(cursor_factory=RealDictCursor) as cur:
                        limit = UPDATER_CONFIG["MAX_SQL_ROWS_PER_QUERY"]
                        
                        cur.execute(f"""
                            SELECT 
                                COUNT(*) as total_requests,
                                COUNT(*) FILTER (WHERE status = 'success') as successful_requests,
                                COUNT(*) FILTER (WHERE status != 'success') as failed_requests,
                                AVG(processing_time_ms) FILTER (WHERE processing_time_ms > 0) as avg_response_time_ms
                            FROM (
                                SELECT * FROM user_questions_complete 
                                WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '24 hours'
                                LIMIT {limit}
                            ) recent_perf
                        """)
                        
                        result = cur.fetchone()
                        total_req = result["total_requests"] or 0
                        failed_req = result["failed_requests"] or 0
                        error_rate = (failed_req / total_req * 100) if total_req > 0 else 0
                        
                        performance_data = {
                            "period_hours": 24,
                            "current_status": {
                                "overall_health": "healthy" if error_rate < 5 else "degraded",
                                "avg_response_time_ms": int(result["avg_response_time_ms"] or 0),
                                "error_rate_percent": round(error_rate, 2),
                                "total_errors": failed_req
                            },
                            "global_stats": {
                                "total_requests": total_req,
                                "total_successes": result["successful_requests"] or 0,
                                "total_failures": failed_req
                            },
                            "collected_at": datetime.now().isoformat(),
                            "source": "questions_computed_safe"
                        }
            else:
                # Version compl√®te conserv√©e
                try:
                    from app.api.v1.logging import get_server_analytics
                    server_metrics = get_server_analytics(hours=24)
                    
                    if "error" not in server_metrics:
                        performance_data = {
                            "period_hours": 24,
                            "current_status": server_metrics.get("current_status", {}),
                            "global_stats": server_metrics.get("global_stats", {}),
                            "collected_at": datetime.now().isoformat()
                        }
                    else:
                        raise Exception(server_metrics["error"])
                        
                except Exception:
                    # Fallback vers version all√©g√©e
                    return await self._update_server_performance_safe()
            
            self.cache.set_cache("server:performance:24h", performance_data, ttl_hours=1, source="computed_safe")
            
            logger.info(f"‚ö° Performance serveur collect√©e SAFE: {performance_data['current_status']['overall_health']}")
            return {
                "status": "success", 
                "health": performance_data["current_status"]["overall_health"],
                "total_requests": performance_data["global_stats"].get("total_requests", 0)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur collecte performance SAFE: {e}")
            return {"status": "error", "error": str(e)[:100]}

    # ==================== M√âTHODES CONSERV√âES INT√âGRALEMENT ====================
    
    def get_update_status(self) -> Dict[str, Any]:
        """CONSERV√â: Retourne le statut de la derni√®re mise √† jour"""
        try:
            # R√©cup√©rer depuis le cache
            cached_summary = self.cache.get_cache("system:last_update_summary")
            
            if cached_summary:
                return cached_summary["data"]
            else:
                return {
                    "status": "never_updated",
                    "message": "Aucune mise √† jour effectu√©e",
                    "update_in_progress": self.update_in_progress,
                    "last_update": self.last_update.isoformat() if self.last_update else None,
                    "feedback_support": self._feedback_columns_available,
                    "memory_optimization": "enabled",
                    "collection_mode": "sequential_safe" if not UPDATER_CONFIG["ENABLE_PARALLEL_COLLECTION"] else "parallel"
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration statut: {e}")
            return {"status": "error", "error": str(e)}

    async def force_update_specific(self, component: str) -> Dict[str, Any]:
        """CONSERV√â: Force la mise √† jour d'un composant sp√©cifique"""
        try:
            logger.info(f"üîÑ Force update SAFE: {component}")
            
            if component == "dashboard":
                result = await self._update_dashboard_stats_safe()
            elif component == "openai":
                result = await self._update_openai_costs_safe()
            elif component == "invitations":
                result = await self._update_invitation_stats_safe()
            elif component == "performance":
                result = await self._update_server_performance_safe()
            else:
                return {"status": "error", "error": f"Composant '{component}' inconnu"}
            
            logger.info(f"‚úÖ Force update SAFE {component}: {result['status']}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur force update SAFE {component}: {e}")
            return {"status": "error", "error": str(e)}

    def refresh_feedback_detection(self) -> Dict[str, Any]:
        """
        CONSERV√â: Actualise la d√©tection des colonnes feedback
        Utile apr√®s une migration ou modification de sch√©ma
        """
        try:
            logger.info("üîÑ Actualisation d√©tection colonnes feedback...")
            old_status = self._feedback_columns_available.copy()
            self._feedback_columns_available = self._check_feedback_columns_availability()
            
            result = {
                "status": "success",
                "old_detection": old_status,
                "new_detection": self._feedback_columns_available,
                "changes_detected": old_status != self._feedback_columns_available,
                "timestamp": datetime.now().isoformat()
            }
            
            if result["changes_detected"]:
                logger.info(f"üîÑ Changements d√©tect√©s dans les colonnes feedback: {result['new_detection']}")
            else:
                logger.info("‚ÑπÔ∏è Aucun changement d√©tect√© dans les colonnes feedback")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur refresh feedback detection: {e}")
            return {"status": "error", "error": str(e)}

    # üÜï NOUVELLES M√âTHODES POUR GESTION M√âMOIRE
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """üÜï Retourne les statistiques m√©moire et de performance"""
        try:
            return {
                "system_memory_percent": get_memory_usage_percent(),
                "collection_stats": self.collection_stats.copy(),
                "updater_config": UPDATER_CONFIG.copy(),
                "last_update": self.last_update.isoformat() if self.last_update else None,
                "update_in_progress": self.update_in_progress
            }
        except Exception as e:
            return {"error": str(e)}

    def toggle_parallel_collection(self, enable: bool = None) -> Dict[str, Any]:
        """üÜï Active/d√©sactive la collecte parall√®le"""
        try:
            if enable is None:
                enable = not UPDATER_CONFIG["ENABLE_PARALLEL_COLLECTION"]
            
            UPDATER_CONFIG["ENABLE_PARALLEL_COLLECTION"] = enable
            
            logger.info(f"üîß Collecte parall√®le: {'activ√©e' if enable else 'd√©sactiv√©e'}")
            
            return {
                "status": "success",
                "parallel_collection_enabled": enable,
                "recommendation": "Mode s√©quentiel recommand√© pour √©conomie m√©moire",
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}


# ==================== SINGLETON GLOBAL (CONSERV√â) ====================

_stats_updater_instance = None

def get_stats_updater() -> StatisticsUpdater:
    """CONSERV√â: R√©cup√®re l'instance singleton du collecteur"""
    global _stats_updater_instance
    if _stats_updater_instance is None:
        _stats_updater_instance = StatisticsUpdater()
    return _stats_updater_instance

# ==================== FONCTIONS UTILITAIRES (CONSERV√âES) ====================

async def run_update_cycle():
    """CONSERV√â: Fonction helper pour le scheduler"""
    updater = get_stats_updater()
    return await updater.update_all_statistics()

async def force_update_all():
    """CONSERV√â: Force une mise √† jour imm√©diate (pour admin)"""
    updater = get_stats_updater()
    return await updater.update_all_statistics()

def refresh_feedback_columns():
    """CONSERV√â: Force la re-d√©tection des colonnes feedback"""
    updater = get_stats_updater()
    return updater.refresh_feedback_detection()

# üÜï NOUVELLES FONCTIONS UTILITAIRES MEMORY-SAFE

def get_updater_memory_stats():
    """üÜï Statistiques m√©moire globales du updater"""
    try:
        updater = get_stats_updater()
        return updater.get_memory_stats()
    except Exception as e:
        return {"error": str(e), "system_memory_percent": get_memory_usage_percent()}

def toggle_heavy_analytics(skip: bool = None):
    """üÜï Active/d√©sactive les analytics lourdes"""
    if skip is None:
        skip = not UPDATER_CONFIG["SKIP_HEAVY_ANALYTICS"]
    
    UPDATER_CONFIG["SKIP_HEAVY_ANALYTICS"] = skip
    logger.info(f"üîß Heavy analytics: {'d√©sactiv√©es' if skip else 'activ√©es'}")
    
    return {
        "status": "success",
        "heavy_analytics_skipped": skip,
        "memory_impact": "R√©duction ~30% RAM si d√©sactiv√©es"
    }