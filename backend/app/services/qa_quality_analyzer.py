"""
Service d'analyse de qualit√© Q&A avec OpenAI
Version: 1.4.1
Last modified: 2025-10-26
"""
"""
Service d'analyse de qualit√© Q&A avec OpenAI
D√©tecte automatiquement les r√©ponses probl√©matiques
"""

import json
import logging
import os
from typing import Dict, Any, Optional, List
from openai import OpenAI
from datetime import datetime

logger = logging.getLogger(__name__)

# Configuration
DEFAULT_MODEL = "gpt-3.5-turbo"  # √âconomique pour l'analyse
ANALYSIS_PROMPT_VERSION = "v1.0"

# OpenAI client (lazy initialization)
_client = None


def get_openai_client():
    """Lazy initialization of OpenAI client"""
    global _client
    if _client is None:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable not set")
        _client = OpenAI(api_key=api_key)
    return _client

# Seuils de d√©tection
QUALITY_THRESHOLD = 5.0  # Score < 5 = probl√©matique
CONFIDENCE_THRESHOLD = 0.7  # Confiance minimale pour marquer comme probl√©matique


class QAQualityAnalyzer:
    """Analyseur de qualit√© Q&A utilisant OpenAI"""

    def __init__(self, model: str = DEFAULT_MODEL):
        self.model = model
        self.prompt_version = ANALYSIS_PROMPT_VERSION

    def build_analysis_prompt(
        self,
        question: str,
        response: str,
        response_source: Optional[str] = None,
        response_confidence: Optional[float] = None,
        context_docs: Optional[List[str]] = None
    ) -> str:
        """Construit le prompt d'analyse pour OpenAI"""

        prompt = f"""Tu es un expert en aviculture charg√© d'√©valuer la qualit√© des r√©ponses fournies par un syst√®me d'IA.

**QUESTION DE L'UTILISATEUR:**
{question}

**R√âPONSE FOURNIE:**
{response}

**M√âTADONN√âES:**
- Source de la r√©ponse: {response_source or 'inconnue'}
- Confiance du syst√®me: {response_confidence or 'non sp√©cifi√©e'}
"""

        if context_docs:
            prompt += f"\n**DOCUMENTS DE CONTEXTE UTILIS√âS:**\n"
            for i, doc in enumerate(context_docs[:3], 1):
                prompt += f"{i}. {doc[:200]}...\n"

        prompt += """

**T√ÇCHE D'√âVALUATION:**

Analyse cette r√©ponse selon les crit√®res suivants:

1. **Exactitude technique** (0-10): La r√©ponse est-elle techniquement correcte en aviculture?
2. **Pertinence** (0-10): La r√©ponse r√©pond-elle directement √† la question pos√©e?
3. **Compl√©tude** (0-10): La r√©ponse est-elle suffisamment d√©taill√©e et compl√®te?
4. **Coh√©rence** (0-10): La r√©ponse est-elle coh√©rente et sans contradictions?

**D√âTECTION DE PROBL√àMES:**

Identifie si la r√©ponse pr√©sente l'un de ces probl√®mes graves:

- ‚ùå **Informations incorrectes**: Erreurs factuelles, donn√©es obsol√®tes, conseils dangereux
- ‚ùå **Hors sujet**: R√©ponse qui ne correspond pas √† la question
- ‚ùå **R√©ponse g√©n√©rique**: R√©ponse vague sans d√©tails sp√©cifiques √† l'aviculture
- ‚ùå **Informations manquantes**: Omet des √©l√©ments critiques pour la s√©curit√©/sant√©
- ‚ùå **Contradictions**: Contient des affirmations contradictoires
- ‚ùå **Hallucination**: Invente des faits, statistiques ou recommandations non v√©rifiables

**FORMAT DE R√âPONSE (JSON strict):**

{
  "scores": {
    "accuracy": 0-10,
    "relevance": 0-10,
    "completeness": 0-10,
    "coherence": 0-10
  },
  "overall_quality_score": 0-10,
  "is_problematic": true/false,
  "problem_category": "incorrect|incomplete|off_topic|generic|contradictory|hallucination|none",
  "problems": ["liste des probl√®mes sp√©cifiques d√©tect√©s"],
  "recommendation": "recommandation pour am√©liorer la r√©ponse ou action √† prendre",
  "confidence": 0.0-1.0,
  "reasoning": "explication br√®ve de l'√©valuation"
}

**CRIT√àRES POUR is_problematic=true:**
- overall_quality_score < 5
- OU pr√©sence d'informations incorrectes/dangereuses
- OU r√©ponse compl√®tement hors sujet
- OU hallucination d√©tect√©e

R√©ponds UNIQUEMENT avec le JSON, sans texte additionnel.
"""

        return prompt

    async def analyze_qa(
        self,
        question: str,
        response: str,
        response_source: Optional[str] = None,
        response_confidence: Optional[float] = None,
        context_docs: Optional[List[str]] = None,
        trigger: str = "manual"
    ) -> Dict[str, Any]:
        """
        Analyse une paire Q&A et retourne les r√©sultats

        Args:
            question: Question de l'utilisateur
            response: R√©ponse fournie par le syst√®me
            response_source: Source de la r√©ponse (rag, openai_fallback, etc.)
            response_confidence: Score de confiance du syst√®me (0-1)
            context_docs: Documents de contexte utilis√©s (optionnel)
            trigger: Comment l'analyse a √©t√© d√©clench√©e (manual, batch, realtime, etc.)

        Returns:
            Dict contenant les r√©sultats d'analyse
        """
        try:
            prompt = self.build_analysis_prompt(
                question=question,
                response=response,
                response_source=response_source,
                response_confidence=response_confidence,
                context_docs=context_docs
            )

            logger.info(f"üîç [QA_QUALITY] Analysing Q&A with {self.model}")

            # Appel √† OpenAI
            client = get_openai_client()
            completion = client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "Tu es un expert en aviculture et en √©valuation de la qualit√© des r√©ponses. Tu r√©ponds toujours en JSON strict."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.1,  # Peu de cr√©ativit√© pour l'analyse
                max_tokens=800,
                response_format={"type": "json_object"}  # Force JSON
            )

            # Parser la r√©ponse
            analysis_text = completion.choices[0].message.content
            analysis = json.loads(analysis_text)

            # Valider et enrichir les r√©sultats
            overall_score = analysis.get("overall_quality_score", 0)
            is_problematic = analysis.get("is_problematic", False)
            confidence = analysis.get("confidence", 0.5)

            # Application des seuils
            if overall_score < QUALITY_THRESHOLD:
                is_problematic = True

            # Si confiance basse de l'analyseur, √™tre prudent
            if confidence < CONFIDENCE_THRESHOLD and is_problematic:
                logger.warning(f"‚ö†Ô∏è [QA_QUALITY] Low analysis confidence: {confidence}")

            result = {
                "quality_score": round(overall_score, 1),
                "is_problematic": is_problematic,
                "problem_category": analysis.get("problem_category", "none"),
                "problems": analysis.get("problems", []),
                "recommendation": analysis.get("recommendation", ""),
                "analysis_confidence": round(confidence, 2),
                "scores_detail": analysis.get("scores", {}),
                "reasoning": analysis.get("reasoning", ""),
                "analysis_trigger": trigger,
                "analysis_model": self.model,
                "analysis_prompt_version": self.prompt_version,
                "analyzed_at": datetime.now().isoformat(),
                "tokens_used": completion.usage.total_tokens,
                "cost_estimate": self._estimate_cost(completion.usage.total_tokens)
            }

            logger.info(
                f"‚úÖ [QA_QUALITY] Analysis complete: "
                f"score={overall_score}, problematic={is_problematic}, "
                f"category={result['problem_category']}"
            )

            return result

        except json.JSONDecodeError as e:
            logger.error(f"‚ùå [QA_QUALITY] JSON parsing error: {e}")
            return self._create_error_result("json_parse_error", str(e))

        except Exception as e:
            logger.error(f"‚ùå [QA_QUALITY] Analysis error: {e}", exc_info=True)
            return self._create_error_result("analysis_error", str(e))

    def _estimate_cost(self, tokens: int) -> float:
        """Estime le co√ªt de l'analyse en USD"""
        # Prix GPT-3.5-turbo: $0.0005/1K input + $0.0015/1K output
        # Approximation: ~$0.001 per 1K tokens
        return round((tokens / 1000) * 0.001, 4)

    def _create_error_result(self, error_type: str, error_message: str) -> Dict[str, Any]:
        """Cr√©e un r√©sultat d'erreur standardis√©"""
        return {
            "quality_score": None,
            "is_problematic": False,
            "problem_category": "analysis_error",
            "problems": [f"{error_type}: {error_message}"],
            "recommendation": "Analyse manuelle requise",
            "analysis_confidence": 0.0,
            "scores_detail": {},
            "reasoning": f"Erreur lors de l'analyse: {error_message}",
            "analysis_trigger": "error",
            "analysis_model": self.model,
            "analysis_prompt_version": self.prompt_version,
            "analyzed_at": datetime.now().isoformat(),
            "tokens_used": 0,
            "cost_estimate": 0.0,
            "error": True
        }

    def should_analyze_realtime(
        self,
        response_source: Optional[str] = None,
        response_confidence: Optional[float] = None,
        feedback: Optional[int] = None
    ) -> bool:
        """
        D√©termine si une Q&A doit √™tre analys√©e en temps r√©el (Approche C - Hybride)

        Crit√®res pour analyse temps r√©el:
        - Feedback n√©gatif de l'utilisateur
        - Confidence tr√®s basse (< 0.3)
        - Source openai_fallback avec confidence < 0.5
        - Source inconnue

        Returns:
            True si analyse temps r√©el recommand√©e
        """
        # Feedback n√©gatif = analyse imm√©diate
        if feedback is not None and feedback < 0:
            logger.info("üö® [QA_QUALITY] Realtime analysis triggered: negative feedback")
            return True

        # Confidence tr√®s basse
        if response_confidence is not None and response_confidence < 0.3:
            logger.info("üö® [QA_QUALITY] Realtime analysis triggered: low confidence")
            return True

        # Fallback OpenAI avec confidence moyenne-basse
        if response_source == "openai_fallback" and (response_confidence or 0) < 0.5:
            logger.info("üö® [QA_QUALITY] Realtime analysis triggered: fallback with low confidence")
            return True

        # Source inconnue ou manquante
        if not response_source or response_source == "unknown":
            logger.info("üö® [QA_QUALITY] Realtime analysis triggered: unknown source")
            return True

        return False


# Instance globale
qa_analyzer = QAQualityAnalyzer()
