"""
RAG Retriever - Species-aware, table-first, multi-index, robust embedding support

AmÃ©liorations clÃ©s vs. version prÃ©cÃ©dente :
- Multi-index par espÃ¨ce (broiler/layer/global) avec dÃ©tection auto et fallback
- PrioritÃ© table-first (re-ranking quand la requÃªte contient des chiffres/unitÃ©s)
- Cache des index FAISS par espÃ¨ce (Ã©vite les rechargements)
- ModÃ¨le SentenceTransformer paresseux et rÃ©utilisable
- Conserve : multi-mÃ©thodes d'embeddings, normalisation, robustesse FAISS, synthÃ¨se d'answer, debug

ENV pris en charge (si prÃ©sents) :
- RAG_INDEX_DIR (racine contenant /global, /broiler, /layer)
- RAG_INDEX_GLOBAL, RAG_INDEX_BROILER, RAG_INDEX_LAYER (chemins directs)
- OPENAI_API_KEY (si mÃ©thode OpenAI utilisÃ©e)
"""

from __future__ import annotations

import os
import re
import pickle
import logging
import threading
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

import numpy as np

logger = logging.getLogger(__name__)

# -----------------------------
# Helpers (espÃ¨ce & tables)
# -----------------------------

_SPECIES = ("broiler", "layer", "global")


def _detect_species_from_query(q: str) -> Optional[str]:
    ql = (q or "").lower()
    # Indices "layer"
    if any(w in ql for w in ["layer", "pondeuse", "ponte", "Å“uf", "oeuf", "lohmann", "hy-line", "hyline", "w-36", "w36", "w-80", "w80"]):
        return "layer"
    # Indices "broiler"
    if any(w in ql for w in ["broiler", "poulet de chair", "ross 308", "ross308", "cobb 500", "cobb500", "croissance", "poids", "fcr"]):
        # garde-fou : si indices layer aussi â†’ incertain
        if any(w in ql for w in ["layer", "pondeuse", "ponte", "lohmann", "hy-line", "hyline"]):
            return None
        return "broiler"
    return None


def _looks_like_table(text: str, md: Dict[str, Any]) -> bool:
    # mÃ©tadonnÃ©e explicite
    if isinstance(md, dict) and (md.get("chunk_type") == "table" or md.get("table_type")):
        return True
    t = text or ""
    # indices rapides (markdown pipes, CSV, colonnes Ã  espaces)
    if t.count("|") >= 3:
        return True
    if t.count(",") >= 5 and "\n" in t:
        return True
    if re.search(r"\S+\s{2,}\S+\s{2,}\S+", t):
        return True
    return False


def _query_has_numbers_or_units(q: str) -> bool:
    ql = (q or "").lower()
    return any(ch.isdigit() for ch in ql) or any(u in ql for u in ["kg", "g", "fcr", "%", "Â°c", "ppm", "mÂ³", "m3"])


# =====================================================================
# Retriever
# =====================================================================

class RAGRetriever:
    """
    RAG retriever avec :
    - MÃ©thodes d'embedding multiples (SentenceTransformers / OpenAI / TF-IDF)
    - Multi-index par espÃ¨ce (broiler/layer/global) avec auto-dÃ©tection
    - Table-first re-ranking
    - Chargements FAISS robustes + mÃ©tadonnÃ©es normalisÃ©es
    """

    # Cache modÃ¨le SentenceTransformer partagÃ© (par worker)
    _st_model = None
    _st_lock = threading.Lock()

    def __init__(self, openai_api_key: Optional[str] = None) -> None:
        self.openai_api_key = openai_api_key or os.environ.get("OPENAI_API_KEY")

        # Ã‰tats par espÃ¨ce
        self.index_by_species: Dict[str, Any] = {}
        self.documents_by_species: Dict[str, List[Dict[str, Any]]] = {}
        self.method_by_species: Dict[str, str] = {}
        self.emb_dim_by_species: Dict[str, Optional[int]] = {}
        self.is_loaded_by_species: Dict[str, bool] = {s: False for s in _SPECIES}

        # Chargement eager de "global" (optionnel)
        self._load_index_for_species("global")

    # -----------------------------
    # RÃ©solution chemins index
    # -----------------------------

    def _get_rag_index_path(self, species: str) -> Path:
        """
        RÃ©sout le chemin d'index pour une espÃ¨ce donnÃ©e en respectant (par ordre) :
        - RAG_INDEX_<SPECIES>
        - RAG_INDEX_DIR/<species>
        - ./backend/rag_index/<species>  # ðŸ”§ CORRECTION AJOUTÃ‰E
        - ./rag_index/<species>
        - ./rag_index (fallback legacy)
        """
        sp = (species or "global").lower()
        env_key = f"RAG_INDEX_{sp.upper()}"
        if os.environ.get(env_key):
            return Path(os.environ[env_key])

        root = os.environ.get("RAG_INDEX_DIR")
        if root:
            p = Path(root) / sp
            if p.exists():
                return p

        # ðŸ”§ CORRECTION : Ajouter le chemin backend/rag_index en prioritÃ©
        p_backend = Path.cwd() / "backend" / "rag_index" / sp
        if p_backend.exists():
            logger.info("âœ… Index trouvÃ© dans backend/rag_index pour %s: %s", sp, p_backend)
            return p_backend

        p1 = Path.cwd() / "rag_index" / sp
        if p1.exists():
            return p1
        p2 = Path(__file__).parent.parent / "rag_index" / sp
        if p2.exists():
            return p2

        # ðŸ”§ CORRECTION : Fallback aussi vers backend/rag_index
        p_backend_fallback = Path.cwd() / "backend" / "rag_index"
        if p_backend_fallback.exists():
            logger.info("âœ… Fallback vers backend/rag_index pour %s: %s", sp, p_backend_fallback)
            return p_backend_fallback

        # fallback legacy (un seul dossier)
        return Path.cwd() / "rag_index"

    # -----------------------------
    # Normalisation mÃ©thode embeddings
    # -----------------------------

    def _normalize_embedding_method(self, method: str) -> str:
        if not method or not isinstance(method, str):
            return "SentenceTransformers"
        m = method.lower().strip()
        mapping = {
            # SentenceTransformers variations
            "sentence_transformers": "SentenceTransformers",
            "sentence-transformers": "SentenceTransformers",
            "sentencetransformers": "SentenceTransformers",
            "sentence transformers": "SentenceTransformers",
            "all-minilm-l6-v2": "SentenceTransformers",
            "huggingface": "SentenceTransformers",
            "transformer": "SentenceTransformers",
            "bert": "SentenceTransformers",
            # OpenAI variations
            "openai": "OpenAI",
            "openaiembeddings": "OpenAI",
            "openai_embeddings": "OpenAI",
            "text-embedding-ada-002": "OpenAI",
            "ada-002": "OpenAI",
            # TF-IDF variations
            "tfidf": "TF-IDF",
            "tf-idf": "TF-IDF",
            "tf_idf": "TF-IDF",
        }
        out = mapping.get(m, method)
        if out not in {"SentenceTransformers", "OpenAI", "TF-IDF"}:
            logger.warning("Unknown embedding method '%s' â†’ fallback SentenceTransformers", method)
            return "SentenceTransformers"
        return out

    # -----------------------------
    # Chargement index (par espÃ¨ce)
    # -----------------------------

    def _load_index_for_species(self, species: str) -> bool:
        """Charge FAISS + documents pour une espÃ¨ce. Idempotent et mis en cache."""
        sp = (species or "global").lower()
        if sp not in _SPECIES:
            sp = "global"
        if self.is_loaded_by_species.get(sp):
            return True

        try:
            import faiss  # noqa: F401
        except ImportError:
            logger.error("FAISS non disponible â€” impossible de charger l'index (%s)", sp)
            return False

        path = self._get_rag_index_path(sp)
        faiss_file = path / "index.faiss"
        pkl_file = path / "index.pkl"

        if not faiss_file.exists() or not pkl_file.exists():
            logger.warning("Fichiers d'index introuvables pour %s dans %s", sp, path)
            return False

        try:
            idx = faiss.read_index(str(faiss_file))
            with open(pkl_file, "rb") as f:
                data = pickle.load(f)

            raw_method = data.get("method", data.get("embedding_method", "SentenceTransformers"))
            method = self._normalize_embedding_method(raw_method)
            docs = data.get("documents", [])
            embeddings = data.get("embeddings", None)
            emb_dim = (len(embeddings[0]) if isinstance(embeddings, list) and embeddings else None)

            # MÃ©moriser
            self.index_by_species[sp] = idx
            self.documents_by_species[sp] = docs
            self.method_by_species[sp] = method
            self.emb_dim_by_species[sp] = emb_dim
            self.is_loaded_by_species[sp] = True

            # Sauvegarder normalisation mÃ©thode si diffÃ©rente
            if raw_method != method:
                try:
                    data["method"] = method
                    data["embedding_method"] = method
                    backup = pkl_file.with_suffix(".pkl.backup")
                    if pkl_file.exists() and not backup.exists():
                        import shutil
                        shutil.copy2(pkl_file, backup)
                    with open(pkl_file, "wb") as wf:
                        pickle.dump(data, wf)
                    logger.info("MÃ©tadonnÃ©es corrigÃ©es (mÃ©thode embeddings normalisÃ©e) pour %s", sp)
                except Exception as e:
                    logger.warning("Impossible d'enregistrer les mÃ©tadonnÃ©es corrigÃ©es (%s): %s", sp, e)

            logger.info("âœ… Index %s chargÃ© | ntotal=%s | docs=%d | method=%s",
                        sp, getattr(idx, "ntotal", "n/a"), len(docs), method)
            return True
        except Exception as e:
            logger.error("Erreur de chargement index %s: %s", sp, e)
            return False

    # -----------------------------
    # DisponibilitÃ©
    # -----------------------------

    def is_available(self) -> bool:
        """Vrai si au moins un index espÃ¨ce est chargÃ© et non vide."""
        for sp in _SPECIES:
            if self.is_loaded_by_species.get(sp) and self.index_by_species.get(sp) is not None:
                if getattr(self.index_by_species[sp], "ntotal", 0) > 0:
                    return True
        return False

    # -----------------------------
    # Embeddings
    # -----------------------------

    def _ensure_st_model(self):
        """Singleton SentenceTransformer (lazy)."""
        if self._st_model is not None:
            return self._st_model
        with self._st_lock:
            if self._st_model is None:
                from sentence_transformers import SentenceTransformer
                self._st_model = SentenceTransformer("all-MiniLM-L6-v2")
        return self._st_model

    def _create_openai_embedding(self, query: str) -> Optional[np.ndarray]:
        if not self.openai_api_key:
            logger.error("OPENAI_API_KEY manquant pour embeddings OpenAI")
            return None
        try:
            import openai
            client = openai.OpenAI(api_key=self.openai_api_key)
            resp = client.embeddings.create(input=query, model="text-embedding-ada-002")
            vec = np.array(resp.data[0].embedding, dtype=np.float32)
            return vec
        except Exception as e:
            logger.error("Echec embedding OpenAI: %s", e)
            return None

    def _create_sentence_transformer_embedding(self, query: str) -> Optional[np.ndarray]:
        try:
            model = self._ensure_st_model()
            emb = model.encode([query], normalize_embeddings=True)
            return np.array(emb[0], dtype=np.float32)
        except Exception as e:
            logger.error("Echec embedding SentenceTransformer: %s", e)
            return None

    def _create_tfidf_embedding(self, query: str, species: str) -> Optional[np.ndarray]:
        dim = self.emb_dim_by_species.get(species) or 0
        if not dim:
            return None
        words = (query or "").lower().split()
        vec = np.zeros(dim, dtype=np.float32)
        for i, w in enumerate(words[:dim]):
            vec[i] = 0.1 + (hash(w) % 100) / 1000.0
        return vec

    def _create_query_embedding(self, query: str, method: str, species: str) -> Optional[np.ndarray]:
        try:
            if method == "OpenAI":
                return self._create_openai_embedding(query)
            if method == "SentenceTransformers":
                return self._create_sentence_transformer_embedding(query)
            if method == "TF-IDF":
                return self._create_tfidf_embedding(query, species)
            # fallback cascade
            emb = self._create_sentence_transformer_embedding(query)
            if emb is not None:
                return emb
            if self.openai_api_key:
                emb = self._create_openai_embedding(query)
                if emb is not None:
                    return emb
            return self._create_tfidf_embedding(query, species)
        except Exception as e:
            logger.error("Echec crÃ©ation embedding requÃªte: %s", e)
            return None

    # -----------------------------
    # FAISS search
    # -----------------------------

    def _search_index(self, species: str, query_embedding: np.ndarray, k: int) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        try:
            import faiss
            idx = self.index_by_species.get(species)
            if idx is None:
                return None, None

            qe = query_embedding
            if qe.ndim == 1:
                qe = qe.reshape(1, -1)
            # normalisation L2 sauf TF-IDF
            if self.method_by_species.get(species) != "TF-IDF":
                faiss.normalize_L2(qe)
            distances, indices = idx.search(qe.astype("float32"), k)
            return distances, indices
        except Exception as e:
            logger.error("Echec recherche FAISS (%s): %s", species, e)
            return None, None

    def _process_search_results(self, species: str, distances: np.ndarray, indices: np.ndarray) -> List[Tuple[Dict[str, Any], float]]:
        results: List[Tuple[Dict[str, Any], float]] = []
        docs = self.documents_by_species.get(species, [])
        try:
            for i, idx in enumerate(indices[0]):
                if 0 <= idx < len(docs):
                    doc = docs[idx]
                    score = float(distances[0][i])
                    if isinstance(doc, dict) and "content" in doc:
                        results.append((doc, score))
            return results
        except Exception as e:
            logger.error("Echec traitement rÃ©sultats: %s", e)
            return []

    # -----------------------------
    # Table-first re-ranking
    # -----------------------------

    @staticmethod
    def _score_from_distance(distance: float) -> float:
        if distance <= 0:
            return 1.0
        return float(max(0.0, min(1.0, np.exp(-distance * 1.5))))

    @staticmethod
    def _token_overlap_boost(q: str, t: str) -> float:
        qw = set((q or "").lower().split())
        if not qw:
            return 0.0
        tw = set((t or "").lower().split())
        overlap = len(qw & tw) / max(1, len(qw))
        return min(0.1, overlap * 0.1)  # petit bonus contrÃ´lÃ©

    def _table_first_rerank(self, query: str, pairs: List[Tuple[Dict[str, Any], float]]) -> List[Tuple[Dict[str, Any], float]]:
        if not pairs or not _query_has_numbers_or_units(query):
            return pairs
        boosted: List[Tuple[Dict[str, Any], float, float]] = []
        for doc, raw in pairs:
            text = doc.get("content", "")
            md = doc.get("metadata", {}) or {}
            base = self._score_from_distance(raw)
            bonus = 0.0
            if _looks_like_table(text, md):
                bonus += 0.15
            bonus += self._token_overlap_boost(query, text)
            boosted.append((doc, raw, min(1.0, base + bonus)))
        boosted.sort(key=lambda r: r[2], reverse=True)
        return [(d, s) for (d, s, _) in boosted]

    # -----------------------------
    # API publique
    # -----------------------------

    def get_contextual_diagnosis(self, query: str, k: int = 5) -> Optional[Dict[str, Any]]:
        """
        Recherche species-aware + table-first.
        Retourne une rÃ©ponse synthÃ©tisÃ©e + documents sources.
        """
        species_hint = _detect_species_from_query(query) or "global"
        tried: List[str] = []

        # ordre d'essai : espÃ¨ce dÃ©tectÃ©e â†’ autres â†’ fallback global
        candidates = [species_hint] + [s for s in _SPECIES if s != species_hint]
        for sp in candidates:
            tried.append(sp)
            if not self._load_index_for_species(sp):
                continue

            method = self.method_by_species.get(sp, "SentenceTransformers")
            emb = self._create_query_embedding(query, method, sp)
            if emb is None:
                continue

            # Ã©largir la recherche brute pour permettre le re-ranking
            scores, indices = self._search_index(sp, emb, max(k * 2, 10))
            if scores is None or indices is None:
                continue

            pairs = self._process_search_results(sp, scores, indices)
            if not pairs:
                continue

            # table-first si pertinent
            pairs = self._table_first_rerank(query, pairs)
            pairs = pairs[:k]

            answer = self._synthesize_answer(query, pairs)
            source_documents = [doc for doc, _ in pairs]

            return {
                "answer": answer,
                "source_documents": source_documents,
                "search_type": "vector",
                "total_results": len(pairs),
                "embedding_method": method,
                "species_index_used": sp,
                "tried": tried,
            }

        logger.warning("Aucun rÃ©sultat valide sur les espÃ¨ces testÃ©es: %s", "â†’".join(tried))
        return None

    def _synthesize_answer(self, query: str, results: List[Tuple[Dict[str, Any], float]]) -> str:
        if not results:
            return "Aucune information pertinente trouvÃ©e dans la base de connaissances."

        try:
            parts: List[str] = []
            for doc, _score in results[:3]:
                content = doc.get("content", "") or ""
                src = doc.get("source", doc.get("file_path", "source inconnue")) or "source inconnue"
                # nom de fichier lisible
                if isinstance(src, str) and ("/" in src or "\\" in src):
                    src = src.split("/")[-1].split("\\")[-1]
                # tronquer le contenu pour l'aperÃ§u
                if len(content) > 500:
                    content = content[:500] + "..."
                parts.append(f"Source: {src}\n{content}")

            header = "BasÃ© sur la base de connaissances :"  # court et clair
            return header + "\n\n" + "\n\n".join(parts)

        except Exception as e:
            logger.error("Echec synthÃ¨se rÃ©ponse: %s", e)
            return f"{len(results)} documents pertinents trouvÃ©s, mais la synthÃ¨se a Ã©chouÃ©."

    # Interface compacte de compatibilitÃ© (comme avant)
    def retrieve(self, query: str, **kwargs) -> List[Dict[str, Any]]:
        result = self.get_contextual_diagnosis(query, k=kwargs.get("k", 5))
        if result and result.get("source_documents"):
            return result["source_documents"]
        return []

    # Debug Ã©tendu
    def get_debug_info(self) -> Dict[str, Any]:
        info: Dict[str, Any] = {
            "is_available": self.is_available(),
            "loaded_species": {sp: self.is_loaded_by_species.get(sp, False) for sp in _SPECIES},
            "faiss_ntotal": {sp: getattr(self.index_by_species.get(sp), "ntotal", 0) if self.index_by_species.get(sp) else 0 for sp in _SPECIES},
            "documents_count": {sp: len(self.documents_by_species.get(sp, [])) for sp in _SPECIES},
            "embedding_method": {sp: self.method_by_species.get(sp) for sp in _SPECIES},
            "embedding_dimension": {sp: self.emb_dim_by_species.get(sp) for sp in _SPECIES},
        }
        # chemins rÃ©solus (utile en prod)
        try:
            info["index_paths"] = {sp: str(self._get_rag_index_path(sp)) for sp in _SPECIES}
        except Exception:
            pass
        return info


# Compat alias & factory
ContextualRetriever = RAGRetriever


def create_rag_retriever(openai_api_key: Optional[str] = None) -> RAGRetriever:
    return RAGRetriever(openai_api_key=openai_api_key)